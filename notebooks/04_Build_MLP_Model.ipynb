{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d437d79-cc48-443b-9d1e-cc5164317b54",
   "metadata": {},
   "source": [
    "# Initial Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9736f-e9a3-4924-b83a-d57cd3f02a8a",
   "metadata": {},
   "source": [
    "## Setup Environment and Project Path Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97da9454-843a-4452-95cb-2247c2699597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch threads: 16\n",
      "PyTorch interop threads: 16\n",
      "Project root added to sys.path: C:\\Users\\Acer\\Desktop\\Projects for Data Science\\Drug Gi50 Value Prediction\n"
     ]
    }
   ],
   "source": [
    "# General CPU Usage Optimization\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "os.environ['MKL_NUM_THREADS'] = '16'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '16'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
    "\n",
    "# PyTorch-specific CPU Usage Optimization\n",
    "import torch\n",
    "try:\n",
    "    torch.set_num_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_threads.\\n{e}\")\n",
    "\n",
    "try:\n",
    "    torch.set_num_interop_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_interop_threads.\\n{e}\")\n",
    "\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"PyTorch interop threads: {torch.get_num_interop_threads()}\")\n",
    "\n",
    "# Configure Project Path for Module Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root directory\n",
    "project_root = Path(current_dir).parent.resolve()\n",
    "\n",
    "# Add the project root to sys.path if it's not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "\n",
    "# General Utility for Timestamps\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493e540-40b2-4036-bf51-994fd32cba1b",
   "metadata": {},
   "source": [
    "## Import Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1798eed-a2ab-44f7-8b3c-560494dfd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch Core for Neural Networks\n",
    "import torch.nn as nn  # Neural network modules like Linear, ReLU, MSELoss\n",
    "import torch.nn.functional as F  # Functional interface for activations, e.g. F.ReLU\n",
    "import torch.optim as optim  # Optimization functions like Adam, AdamW, etc.\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "from torch.utils.data import TensorDataset, DataLoader  # Feed data to the model in batches\n",
    "\n",
    "# MLP Model Class\n",
    "from src.models.mlp_models import MLP\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mixed Precision Training (for GPU-accelerated training)\n",
    "# Speeds up training by using float16 where possible\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Git commit ID for final model filename (for reproducibility)\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ee6e6-8244-4dac-9daa-460d27697765",
   "metadata": {},
   "source": [
    "## Import Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c5b793-09c2-445a-b096-026012b631e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bars\n",
    "tqdm_notebook_available = False\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas() # Enable tqdm for pandas apply\n",
    "    tqdm_notebook_available = True\n",
    "except ImportError:\n",
    "    print(\"tqdm.notebook not found. Install with 'pip install tqdm'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91343e-946d-4d69-b1e1-efec438ea09e",
   "metadata": {},
   "source": [
    "## Define Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01abbc85-cd0b-4899-bc1c-f6eae610214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560b36e-9624-4ce5-9d4b-3d1aec960aba",
   "metadata": {},
   "source": [
    "## Set Final Model Save Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7b983a-2456-4e0e-aad0-3ee6522afada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best final MLP model will be saved in: ..\\models\\mlp\n"
     ]
    }
   ],
   "source": [
    "mlp_models_base_dir = Path(\"../models/mlp\")\n",
    "mlp_models_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"The best final MLP model will be saved in: {mlp_models_base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11429021-e5ba-47f4-bf29-1265220cee2f",
   "metadata": {},
   "source": [
    "# Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea93e50c-1ae2-465b-8fd2-e6bd52b9c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data splits from ..\\data\\splits...\n",
      "Data splits loaded successfully.\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1)\n",
      "y_val shape: (2812, 1)\n",
      "y_test shape: (2812, 1)\n",
      "\n",
      "First 5 rows of X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of y_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits_dir = Path(\"../data/splits\")\n",
    "print(f\"\\nLoading data splits from {splits_dir}...\")\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet(splits_dir / \"X_train.parquet\")\n",
    "    X_val = pd.read_parquet(splits_dir / \"X_val.parquet\")\n",
    "    X_test = pd.read_parquet(splits_dir / \"X_test.parquet\")\n",
    "    \n",
    "    y_train = pd.read_parquet(splits_dir / \"y_train.parquet\")\n",
    "    y_val = pd.read_parquet(splits_dir / \"y_val.parquet\")\n",
    "    y_test = pd.read_parquet(splits_dir / \"y_test.parquet\")\n",
    "    print(\"Data splits loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more split files not found in '{splits_dir}'.\")\n",
    "    print(\"Please ensure you have run '02_Split_Features.ipynb' to generate and save the splits.\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"\\nFirst 5 rows of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of y_train:\")\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdffce-3a7f-45ae-a8af-5a3adfa3fbbe",
   "metadata": {},
   "source": [
    "# Prepare Data for MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f14d51-b772-4ac1-b9d9-eff89e29b030",
   "metadata": {},
   "source": [
    "## Convert Data To Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "720e2ec5-e2c8-42b1-b6c3-0a95f6df39d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing X for MLP training (dropping identifiers)...\n",
      "X_train_mlp shape (numerical features only): (13119, 2266)\n",
      "X_val_mlp shape (numerical features only): (2812, 2266)\n",
      "X_test_mlp shape (numerical features only): (2812, 2266)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>312.243</td>\n",
       "      <td>328.121178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>334.222</td>\n",
       "      <td>353.126323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>418.303</td>\n",
       "      <td>447.215806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>360.102</td>\n",
       "      <td>374.026604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>453.727</td>\n",
       "      <td>472.111375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  HeavyAtomMolWt  ExactMolWt  \\\n",
       "0        0.494176  0.476742  12.560000  328.371         312.243  328.121178   \n",
       "1        0.459195  0.604738  12.923077  353.374         334.222  353.126323   \n",
       "2       -0.173158  0.359463  15.909091  447.535         418.303  447.215806   \n",
       "3       -0.686457  0.479732  11.217391  375.222         360.102  374.026604   \n",
       "4       -3.116139  0.437556  16.121212  472.879         453.727  472.111375   \n",
       "\n",
       "   ...  morgan_fp_2038  morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  \\\n",
       "0  ...               0               0               0               0   \n",
       "1  ...               0               0               0               0   \n",
       "2  ...               0               0               0               0   \n",
       "3  ...               0               0               0               0   \n",
       "4  ...               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2042  morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2046  morgan_fp_2047  \n",
       "0               0               0  \n",
       "1               0               0  \n",
       "2               0               0  \n",
       "3               0               0  \n",
       "4               0               0  \n",
       "\n",
       "[5 rows x 2266 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data converted to NumPy arrays.\n",
      "X_train_np shape: (13119, 2266), y_train_np shape: (13119, 1)\n",
      "X_val_np shape: (2812, 2266), y_val_np shape: (2812, 1)\n",
      "X_test_np shape: (2812, 2266), y_test_np shape: (2812, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreparing X for MLP training (dropping identifiers)...\")\n",
    "X_train_mlp = X_train.drop(columns=['molregno', 'canonical_smiles'], errors='ignore')\n",
    "X_val_mlp = X_val.drop(columns=['molregno', 'canonical_smiles'], errors='ignore')\n",
    "X_test_mlp = X_test.drop(columns=['molregno', 'canonical_smiles'], errors='ignore')\n",
    "\n",
    "print(f\"X_train_mlp shape (numerical features only): {X_train_mlp.shape}\")\n",
    "print(f\"X_val_mlp shape (numerical features only): {X_val_mlp.shape}\")\n",
    "print(f\"X_test_mlp shape (numerical features only): {X_test_mlp.shape}\")\n",
    "\n",
    "display(X_train_mlp.head())\n",
    "display(y_train.head())\n",
    "\n",
    "X_train_np = X_train_mlp.values\n",
    "y_train_np = y_train.values.reshape(-1, 1)\n",
    "\n",
    "X_val_np = X_val_mlp.values\n",
    "y_val_np = y_val.values.reshape(-1, 1)\n",
    "\n",
    "X_test_np = X_test_mlp.values\n",
    "y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "print(\"Data converted to NumPy arrays.\")\n",
    "print(f\"X_train_np shape: {X_train_np.shape}, y_train_np shape: {y_train_np.shape}\")\n",
    "print(f\"X_val_np shape: {X_val_np.shape}, y_val_np shape: {y_val_np.shape}\")\n",
    "print(f\"X_test_np shape: {X_test_np.shape}, y_test_np shape: {y_test_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ec5c7-198c-4503-89a5-07b79f3948fa",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402bb90e-f384-4949-b342-02075fe6453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaling features using StandardScaler for MLP...\n",
      "Features scaled successfully.\n",
      "X_train_scaled_np shape: (13119, 2266)\n",
      "X_val_scaled_np shape: (2812, 2266)\n",
      "X_test_scaled_np shape: (2812, 2266)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScaling features using StandardScaler for MLP...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ONLY fit_transform on TRAINING data\n",
    "X_train_scaled_np = scaler.fit_transform(X_train_np)\n",
    "X_val_scaled_np = scaler.transform(X_val_np)\n",
    "X_test_scaled_np = scaler.transform(X_test_np)\n",
    "\n",
    "print(\"Features scaled successfully.\")\n",
    "print(f\"X_train_scaled_np shape: {X_train_scaled_np.shape}\")\n",
    "print(f\"X_val_scaled_np shape: {X_val_scaled_np.shape}\")\n",
    "print(f\"X_test_scaled_np shape: {X_test_scaled_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf278e-3b42-4f9f-af1d-df0df2961421",
   "metadata": {},
   "source": [
    "## Convert Data to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e47270d-848a-4a67-bd8b-a8252b8b0ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting Scaled NumPy Arrays to PyTorch Tensors ---\n",
      "Data converted to PyTorch Tensors and moved to device.\n",
      "X_train_tensor device: cuda:0, shape: torch.Size([13119, 2266])\n",
      "y_train_tensor device: cuda:0, shape: torch.Size([13119, 1])\n",
      "X_val_tensor device: cuda:0, shape: torch.Size([2812, 2266])\n",
      "y_val_tensor device: cuda:0, shape: torch.Size([2812, 1])\n",
      "X_test_tensor device: cuda:0, shape: torch.Size([2812, 2266])\n",
      "y_test_tensor device: cuda:0, shape: torch.Size([2812, 1])\n"
     ]
    }
   ],
   "source": [
    "if 'device' not in locals(): # Check if device variable is already set\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device not previously set, now using: {device}\")\n",
    "\n",
    "print(\"\\n--- Converting Scaled NumPy Arrays to PyTorch Tensors ---\")\n",
    "X_train_tensor = torch.from_numpy(X_train_scaled_np).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train_np).float().to(device)\n",
    "X_val_tensor = torch.from_numpy(X_val_scaled_np).float().to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val_np).float().to(device)\n",
    "X_test_tensor = torch.from_numpy(X_test_scaled_np).float().to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test_np).float().to(device)\n",
    "\n",
    "print(\"Data converted to PyTorch Tensors and moved to device.\")\n",
    "print(f\"X_train_tensor device: {X_train_tensor.device}, shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train_tensor device: {y_train_tensor.device}, shape: {y_train_tensor.shape}\")\n",
    "print(f\"X_val_tensor device: {X_val_tensor.device}, shape: {X_val_tensor.shape}\")\n",
    "print(f\"y_val_tensor device: {y_val_tensor.device}, shape: {y_val_tensor.shape}\")\n",
    "print(f\"X_test_tensor device: {X_test_tensor.device}, shape: {X_test_tensor.shape}\")\n",
    "print(f\"y_test_tensor device: {y_test_tensor.device}, shape: {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6b14a-000f-4538-9e38-818dcfbf7c19",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e3a9c-d5f7-4357-a366-d5f5603b9244",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30857a8-2061-45ca-817d-b42815e33c62",
   "metadata": {},
   "source": [
    "### Create TensorDatasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4298e4f0-f26b-4851-a09d-55569821276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDatasets and DataLoaders created with temporary batch size: 128\n",
      "Number of training batches: 103\n",
      "Number of validation batches: 22\n"
     ]
    }
   ],
   "source": [
    "# Temporary batch size, used as an Optuna hyperparam later\n",
    "temp_batch_size = 128\n",
    "\n",
    "# Create TensorDataset objects from PyTorch Tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create DataLoader objects using the TensorDataset objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=temp_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=temp_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"TensorDatasets and DataLoaders created with temporary batch size: {temp_batch_size}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d243231-030c-45ef-9825-f13095dfa3ce",
   "metadata": {},
   "source": [
    "### Define Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4bff22-8189-4706-b87f-e3240469b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 1024, log=True) # Number of neurons in hidden layer\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True) # Learning rate for optimizer\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256]) # Batch size for DataLoaders\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 150, 600) # Number of training epochs\n",
    "\n",
    "    # Initialize model\n",
    "    # input_size is number of features in X_train_tensor\n",
    "    input_size = X_train_tensor.shape[1]\n",
    "    output_size = 1  # For regression (pGI50)\n",
    "\n",
    "    model = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Loss function and Optimizer\n",
    "    criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # DataLoaders for batching within the trial\n",
    "    # Re-create DataLoaders here because batch_size is a hyperparameter\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "     # Training Loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients before backpropagation\n",
    "            outputs = model(batch_x)  # Forward pass\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)  # Calculate loss\n",
    "            loss.backward()  # Backward pass: compute gradients\n",
    "            optimizer.step() # Apply gradients\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for batch_x_val, batch_y_val in val_loader:\n",
    "                val_outputs = model(batch_x_val)\n",
    "                val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(batch_y_val.cpu().numpy().flatten())\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "        # Optuna Pruning: Report current validation RMSE to Optuna\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Manual Early Stopping Check\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_counter = 0  # Reset patience if improvement is found\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                # print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}\")\n",
    "                break\n",
    "\n",
    "    # Final evaluation on validation set after training (or early stopping)\n",
    "    model.eval()\n",
    "    final_val_predictions = []\n",
    "    final_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x_val, batch_y_val in val_loader:\n",
    "            val_outputs = model(batch_x_val)\n",
    "            final_val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            final_val_targets.extend(batch_y_val.cpu().numpy().flatten())\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(final_val_targets, final_val_predictions))\n",
    "    final_r2 = r2_score(final_val_targets, final_val_predictions)\n",
    "\n",
    "    # Store R2 score as well in the study\n",
    "    trial.set_user_attr(\"final_r2_score\", float(final_r2))\n",
    "\n",
    "    return final_rmse # Optuna minimizes this value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5a47f-acb8-4f2b-a23c-839676f6d526",
   "metadata": {},
   "source": [
    "### Run Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deb4ef0e-f10d-4faf-be70-8ac605df22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for MLP will be stored at: sqlite:///..\\studies\\mlp_study\\mlp_optuna_study.db\n",
      "Loaded existing study 'mlp_regression_pGI50' from sqlite:///..\\studies\\mlp_study\\mlp_optuna_study.db. Resuming optimization.\n",
      "\n",
      "Starting Optuna optimization for MLP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ae68b7d74c4e718075f5a66673814e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 20:44:37,349] Trial 102 finished with value: 0.6443317158507817 and parameters: {'hidden_size': 877, 'lr': 0.0002068072782275973, 'batch_size': 128, 'n_epochs': 240}. Best is trial 102 with value: 0.6443317158507817.\n",
      "[I 2025-07-15 20:48:08,528] Trial 103 finished with value: 0.6487836594518843 and parameters: {'hidden_size': 788, 'lr': 0.00017600995241280688, 'batch_size': 128, 'n_epochs': 189}. Best is trial 102 with value: 0.6443317158507817.\n",
      "[I 2025-07-15 20:48:09,936] Trial 104 pruned. \n",
      "[I 2025-07-15 20:51:26,380] Trial 105 finished with value: 0.6432357152089493 and parameters: {'hidden_size': 794, 'lr': 0.00018179788831323602, 'batch_size': 128, 'n_epochs': 173}. Best is trial 105 with value: 0.6432357152089493.\n",
      "[I 2025-07-15 20:53:37,075] Trial 106 finished with value: 0.6432342027977853 and parameters: {'hidden_size': 728, 'lr': 0.0001686614362357667, 'batch_size': 128, 'n_epochs': 189}. Best is trial 106 with value: 0.6432342027977853.\n",
      "[I 2025-07-15 20:53:38,452] Trial 107 pruned. \n",
      "[I 2025-07-15 20:54:41,001] Trial 108 pruned. \n",
      "[I 2025-07-15 20:54:42,465] Trial 109 pruned. \n",
      "[I 2025-07-15 20:54:43,619] Trial 110 pruned. \n",
      "[I 2025-07-15 20:54:45,032] Trial 111 pruned. \n",
      "[I 2025-07-15 20:56:26,152] Trial 112 pruned. \n",
      "[I 2025-07-15 20:58:53,198] Trial 113 finished with value: 0.648537869135224 and parameters: {'hidden_size': 846, 'lr': 0.0001675386641315324, 'batch_size': 128, 'n_epochs': 228}. Best is trial 106 with value: 0.6432342027977853.\n",
      "[I 2025-07-15 21:02:31,522] Trial 114 finished with value: 0.641034456296339 and parameters: {'hidden_size': 840, 'lr': 0.00016428551058966716, 'batch_size': 128, 'n_epochs': 223}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:05:46,624] Trial 115 finished with value: 0.6473055463183819 and parameters: {'hidden_size': 753, 'lr': 0.0001676557803210134, 'batch_size': 128, 'n_epochs': 214}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:06:49,091] Trial 116 pruned. \n",
      "[I 2025-07-15 21:06:50,435] Trial 117 pruned. \n",
      "[I 2025-07-15 21:08:59,237] Trial 118 finished with value: 0.6489644763451264 and parameters: {'hidden_size': 932, 'lr': 0.0001883044325544959, 'batch_size': 128, 'n_epochs': 226}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:10:00,506] Trial 119 pruned. \n",
      "[I 2025-07-15 21:11:18,416] Trial 120 pruned. \n",
      "[I 2025-07-15 21:14:47,433] Trial 121 finished with value: 0.6441766021598977 and parameters: {'hidden_size': 921, 'lr': 0.00021589873767333465, 'batch_size': 128, 'n_epochs': 203}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:16:57,748] Trial 122 finished with value: 0.6495784489613806 and parameters: {'hidden_size': 933, 'lr': 0.00020915823570471926, 'batch_size': 128, 'n_epochs': 171}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:19:07,520] Trial 123 finished with value: 0.6446603844288703 and parameters: {'hidden_size': 916, 'lr': 0.0002056203994685657, 'batch_size': 128, 'n_epochs': 169}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:20:58,105] Trial 124 finished with value: 0.647507509139333 and parameters: {'hidden_size': 851, 'lr': 0.000210489786941832, 'batch_size': 128, 'n_epochs': 172}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:22:09,455] Trial 125 pruned. \n",
      "[I 2025-07-15 21:25:23,580] Trial 126 finished with value: 0.6456169771100119 and parameters: {'hidden_size': 844, 'lr': 0.00017441945235646298, 'batch_size': 128, 'n_epochs': 166}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:26:25,909] Trial 127 pruned. \n",
      "[I 2025-07-15 21:27:30,403] Trial 128 pruned. \n",
      "[I 2025-07-15 21:30:35,822] Trial 129 finished with value: 0.6442365710321762 and parameters: {'hidden_size': 836, 'lr': 0.00014368737087768662, 'batch_size': 128, 'n_epochs': 157}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:31:40,018] Trial 130 pruned. \n",
      "[I 2025-07-15 21:31:41,321] Trial 131 pruned. \n",
      "[I 2025-07-15 21:34:34,350] Trial 132 finished with value: 0.6459287667748076 and parameters: {'hidden_size': 831, 'lr': 0.00019105967832456033, 'batch_size': 128, 'n_epochs': 170}. Best is trial 114 with value: 0.641034456296339.\n",
      "[I 2025-07-15 21:38:03,482] Trial 133 finished with value: 0.6386120885235618 and parameters: {'hidden_size': 801, 'lr': 0.00016038661160511867, 'batch_size': 128, 'n_epochs': 195}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:39:43,180] Trial 134 pruned. \n",
      "[I 2025-07-15 21:40:56,072] Trial 135 pruned. \n",
      "[I 2025-07-15 21:40:57,457] Trial 136 pruned. \n",
      "[I 2025-07-15 21:42:06,144] Trial 137 pruned. \n",
      "[I 2025-07-15 21:45:12,721] Trial 138 finished with value: 0.6447950085797246 and parameters: {'hidden_size': 787, 'lr': 0.00015396136250105862, 'batch_size': 128, 'n_epochs': 189}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:47:30,207] Trial 139 finished with value: 0.6425849033111187 and parameters: {'hidden_size': 884, 'lr': 0.000158120098671704, 'batch_size': 128, 'n_epochs': 175}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:47:31,730] Trial 140 pruned. \n",
      "[I 2025-07-15 21:48:42,924] Trial 141 pruned. \n",
      "[I 2025-07-15 21:50:47,948] Trial 142 finished with value: 0.6453836754835013 and parameters: {'hidden_size': 858, 'lr': 0.00019643378225898885, 'batch_size': 128, 'n_epochs': 198}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:54:04,736] Trial 143 finished with value: 0.639776851844222 and parameters: {'hidden_size': 805, 'lr': 0.0001992167393108485, 'batch_size': 128, 'n_epochs': 199}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:56:48,112] Trial 144 finished with value: 0.6421867242934534 and parameters: {'hidden_size': 765, 'lr': 0.00019023233511012178, 'batch_size': 128, 'n_epochs': 199}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:59:25,323] Trial 145 finished with value: 0.64225844316349 and parameters: {'hidden_size': 808, 'lr': 0.0001972678458408806, 'batch_size': 128, 'n_epochs': 196}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 21:59:26,740] Trial 146 pruned. \n",
      "[I 2025-07-15 22:02:44,509] Trial 147 finished with value: 0.6466180692486679 and parameters: {'hidden_size': 891, 'lr': 0.0001986465929913698, 'batch_size': 128, 'n_epochs': 189}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 22:03:43,458] Trial 148 pruned. \n",
      "[I 2025-07-15 22:03:44,815] Trial 149 pruned. \n",
      "[I 2025-07-15 22:07:00,009] Trial 150 finished with value: 0.6453249486377995 and parameters: {'hidden_size': 816, 'lr': 0.00015634798021813223, 'batch_size': 128, 'n_epochs': 195}. Best is trial 133 with value: 0.6386120885235618.\n",
      "[I 2025-07-15 22:07:01,351] Trial 151 pruned. \n",
      "\n",
      "Optuna optimization finished for MLP.\n",
      "\n",
      "--- Best Trial Results for MLP ---\n",
      "Best trial number: 133\n",
      "Best RMSE (Validation): 0.6386\n",
      "Best hyperparameters:\n",
      "  hidden_size: 801\n",
      "  lr: 0.00016038661160511867\n",
      "  batch_size: 128\n",
      "  n_epochs: 195\n",
      "Best R2 Score (Validation): 0.5839\n"
     ]
    }
   ],
   "source": [
    "study_dir = Path(\"../studies/mlp_study\")\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "study_db_path = f\"sqlite:///{study_dir / 'mlp_optuna_study.db'}\"\n",
    "study_name = \"mlp_regression_pGI50\"\n",
    "print(f\"Optuna study for MLP will be stored at: {study_db_path}\")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=10,  # Run at least these many trials completely before starting to prune\n",
    "    n_warmup_steps=20,    # Don't prune trials until they've completed these many epochs\n",
    "    interval_steps=10     # Check for pruning every these many epochs\n",
    ")\n",
    "\n",
    "# Check if a study with the same name already exists in the database\n",
    "# If it does, load it to resume the optimization.\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(f\"Loaded existing study '{study_name}' from {study_db_path}. Resuming optimization.\")\n",
    "except KeyError:\n",
    "    # If the study does not exist, create a new one\n",
    "    print(f\"Creating new study '{study_name}' at {study_db_path}.\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        storage=study_db_path,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "print(\"\\nStarting Optuna optimization for MLP...\")\n",
    "study.optimize(objective,\n",
    "                   n_trials=50,\n",
    "                   timeout=7200,\n",
    "                   show_progress_bar=True)\n",
    "print(\"\\nOptuna optimization finished for MLP.\")\n",
    "\n",
    "# Print best trial results\n",
    "print(\"\\n--- Best Trial Results for MLP ---\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best RMSE (Validation): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if \"final_r2_score\" in study.best_trial.user_attrs:\n",
    "    print(f\"Best R2 Score (Validation): {study.best_trial.user_attrs['final_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b70318-884b-4f20-b1fd-62b22fa1b25b",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b991a-66d2-4293-8c8c-03003b975e11",
   "metadata": {},
   "source": [
    "### Reinitialize Everything with Best Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00252baf-050d-48a5-92d1-aa337cfec049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters from Optuna: {'hidden_size': 801, 'lr': 0.00016038661160511867, 'batch_size': 128, 'n_epochs': 195}\n",
      "Final model, criterion, optimizer, and DataLoaders initialized with best parameters.\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_hidden_size = best_params[\"hidden_size\"]\n",
    "best_learning_rate = best_params[\"lr\"]\n",
    "best_batch_size = best_params[\"batch_size\"]\n",
    "best_n_epochs = best_params[\"n_epochs\"]\n",
    "\n",
    "print(f\"Best hyperparameters from Optuna: {best_params}\")\n",
    "\n",
    "# Re-initialize the model with best hyperparameters\n",
    "input_size = X_train_tensor.shape[1]\n",
    "output_size = 1\n",
    "final_mlp_model = MLP(input_size, best_hidden_size, output_size).to(device)\n",
    "\n",
    "# Re-initialize criterion and optimizer\n",
    "final_criterion = nn.MSELoss()\n",
    "final_optimizer = optim.Adam(final_mlp_model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Re-create DataLoaders with the best batch size (Training + Validation data COMBINED)\n",
    "X_train_val_tensor = torch.cat((X_train_tensor, X_val_tensor), dim=0)\n",
    "y_train_val_tensor = torch.cat((y_train_tensor, y_val_tensor), dim=0)\n",
    "\n",
    "final_train_val_dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
    "final_train_val_loader = DataLoader(final_train_val_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "# Create the FINAL TEST DataLoader\n",
    "final_test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "final_test_loader = DataLoader(final_test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Final model, criterion, optimizer, and DataLoaders initialized with best parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3824aa-b3a6-4209-b467-750f679a0fef",
   "metadata": {},
   "source": [
    "### Get Current Git Commit ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c918ad5-506d-48ae-bff3-8d985261c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_commit_hash():\n",
    "    try:\n",
    "        # Get the short commit hash\n",
    "        commit_hash = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('ascii')\n",
    "        return commit_hash\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"unknown_commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eaf2e7f-ec98-4bc4-b242-ba2dca3c3760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit ID: 60d86ce\n"
     ]
    }
   ],
   "source": [
    "# Optionally, see the current commit ID\n",
    "current_commit = get_git_commit_hash()\n",
    "print(f\"Current Git Commit ID: {current_commit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c629f2-d49e-4e64-be1f-46215e9f7253",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4925285b-29f4-4f45-856f-066ce800d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining final MLP model for 195 epochs with best parameters...\n",
      "Associated Git Commit ID for saved model: 60d86ce\n",
      "Epoch 1/195, Train Loss: 10.9005, Eval RMSE on combined data: 0.9733\n",
      "--- New best final model saved at epoch 1 with RMSE: 0.9733 ---\n",
      "Epoch 2/195, Train Loss: 0.7031, Eval RMSE on combined data: 0.6753\n",
      "--- New best final model saved at epoch 2 with RMSE: 0.6753 ---\n",
      "Epoch 3/195, Train Loss: 0.4381, Eval RMSE on combined data: 0.5675\n",
      "--- New best final model saved at epoch 3 with RMSE: 0.5675 ---\n",
      "Epoch 4/195, Train Loss: 0.3292, Eval RMSE on combined data: 0.5020\n",
      "--- New best final model saved at epoch 4 with RMSE: 0.5020 ---\n",
      "Epoch 5/195, Train Loss: 0.2661, Eval RMSE on combined data: 0.4540\n",
      "--- New best final model saved at epoch 5 with RMSE: 0.4540 ---\n",
      "Epoch 6/195, Train Loss: 0.2241, Eval RMSE on combined data: 0.4215\n",
      "--- New best final model saved at epoch 6 with RMSE: 0.4215 ---\n",
      "Epoch 7/195, Train Loss: 0.1907, Eval RMSE on combined data: 0.3879\n",
      "--- New best final model saved at epoch 7 with RMSE: 0.3879 ---\n",
      "Epoch 8/195, Train Loss: 0.1667, Eval RMSE on combined data: 0.3626\n",
      "--- New best final model saved at epoch 8 with RMSE: 0.3626 ---\n",
      "Epoch 9/195, Train Loss: 0.1462, Eval RMSE on combined data: 0.3433\n",
      "--- New best final model saved at epoch 9 with RMSE: 0.3433 ---\n",
      "Epoch 10/195, Train Loss: 0.1316, Eval RMSE on combined data: 0.3225\n",
      "--- New best final model saved at epoch 10 with RMSE: 0.3225 ---\n",
      "Epoch 11/195, Train Loss: 0.1158, Eval RMSE on combined data: 0.3107\n",
      "--- New best final model saved at epoch 11 with RMSE: 0.3107 ---\n",
      "Epoch 12/195, Train Loss: 0.1075, Eval RMSE on combined data: 0.2952\n",
      "--- New best final model saved at epoch 12 with RMSE: 0.2952 ---\n",
      "Epoch 13/195, Train Loss: 0.1007, Eval RMSE on combined data: 0.2790\n",
      "--- New best final model saved at epoch 13 with RMSE: 0.2790 ---\n",
      "Epoch 14/195, Train Loss: 0.0941, Eval RMSE on combined data: 0.2748\n",
      "--- New best final model saved at epoch 14 with RMSE: 0.2748 ---\n",
      "Epoch 15/195, Train Loss: 0.0860, Eval RMSE on combined data: 0.2777\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2748\n",
      "Epoch 16/195, Train Loss: 0.0828, Eval RMSE on combined data: 0.2641\n",
      "--- New best final model saved at epoch 16 with RMSE: 0.2641 ---\n",
      "Epoch 17/195, Train Loss: 0.0778, Eval RMSE on combined data: 0.2573\n",
      "--- New best final model saved at epoch 17 with RMSE: 0.2573 ---\n",
      "Epoch 18/195, Train Loss: 0.0782, Eval RMSE on combined data: 0.2590\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2573\n",
      "Epoch 19/195, Train Loss: 0.0827, Eval RMSE on combined data: 0.2535\n",
      "--- New best final model saved at epoch 19 with RMSE: 0.2535 ---\n",
      "Epoch 20/195, Train Loss: 0.0805, Eval RMSE on combined data: 0.2583\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2535\n",
      "Epoch 21/195, Train Loss: 0.0750, Eval RMSE on combined data: 0.2628\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2535\n",
      "Epoch 22/195, Train Loss: 0.0766, Eval RMSE on combined data: 0.2497\n",
      "--- New best final model saved at epoch 22 with RMSE: 0.2497 ---\n",
      "Epoch 23/195, Train Loss: 0.0762, Eval RMSE on combined data: 0.2480\n",
      "--- New best final model saved at epoch 23 with RMSE: 0.2480 ---\n",
      "Epoch 24/195, Train Loss: 0.0646, Eval RMSE on combined data: 0.2397\n",
      "--- New best final model saved at epoch 24 with RMSE: 0.2397 ---\n",
      "Epoch 25/195, Train Loss: 0.0664, Eval RMSE on combined data: 0.2385\n",
      "--- New best final model saved at epoch 25 with RMSE: 0.2385 ---\n",
      "Epoch 26/195, Train Loss: 0.0730, Eval RMSE on combined data: 0.2661\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2385\n",
      "Epoch 27/195, Train Loss: 0.0719, Eval RMSE on combined data: 0.2363\n",
      "--- New best final model saved at epoch 27 with RMSE: 0.2363 ---\n",
      "Epoch 28/195, Train Loss: 0.0624, Eval RMSE on combined data: 0.2563\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2363\n",
      "Epoch 29/195, Train Loss: 0.0663, Eval RMSE on combined data: 0.2540\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2363\n",
      "Epoch 30/195, Train Loss: 0.0630, Eval RMSE on combined data: 0.2392\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2363\n",
      "Epoch 31/195, Train Loss: 0.0597, Eval RMSE on combined data: 0.2353\n",
      "--- New best final model saved at epoch 31 with RMSE: 0.2353 ---\n",
      "Epoch 32/195, Train Loss: 0.0607, Eval RMSE on combined data: 0.2470\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2353\n",
      "Epoch 33/195, Train Loss: 0.0570, Eval RMSE on combined data: 0.2174\n",
      "--- New best final model saved at epoch 33 with RMSE: 0.2174 ---\n",
      "Epoch 34/195, Train Loss: 0.0559, Eval RMSE on combined data: 0.2184\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2174\n",
      "Epoch 35/195, Train Loss: 0.0541, Eval RMSE on combined data: 0.2283\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2174\n",
      "Epoch 36/195, Train Loss: 0.0589, Eval RMSE on combined data: 0.2310\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2174\n",
      "Epoch 37/195, Train Loss: 0.0572, Eval RMSE on combined data: 0.2193\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.2174\n",
      "Epoch 38/195, Train Loss: 0.0558, Eval RMSE on combined data: 0.2217\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.2174\n",
      "Epoch 39/195, Train Loss: 0.0526, Eval RMSE on combined data: 0.2123\n",
      "--- New best final model saved at epoch 39 with RMSE: 0.2123 ---\n",
      "Epoch 40/195, Train Loss: 0.0527, Eval RMSE on combined data: 0.2218\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2123\n",
      "Epoch 41/195, Train Loss: 0.0505, Eval RMSE on combined data: 0.2231\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2123\n",
      "Epoch 42/195, Train Loss: 0.0504, Eval RMSE on combined data: 0.2101\n",
      "--- New best final model saved at epoch 42 with RMSE: 0.2101 ---\n",
      "Epoch 43/195, Train Loss: 0.0513, Eval RMSE on combined data: 0.2131\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2101\n",
      "Epoch 44/195, Train Loss: 0.0499, Eval RMSE on combined data: 0.2001\n",
      "--- New best final model saved at epoch 44 with RMSE: 0.2001 ---\n",
      "Epoch 45/195, Train Loss: 0.0469, Eval RMSE on combined data: 0.2145\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2001\n",
      "Epoch 46/195, Train Loss: 0.0462, Eval RMSE on combined data: 0.1968\n",
      "--- New best final model saved at epoch 46 with RMSE: 0.1968 ---\n",
      "Epoch 47/195, Train Loss: 0.0443, Eval RMSE on combined data: 0.1964\n",
      "--- New best final model saved at epoch 47 with RMSE: 0.1964 ---\n",
      "Epoch 48/195, Train Loss: 0.0468, Eval RMSE on combined data: 0.2055\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1964\n",
      "Epoch 49/195, Train Loss: 0.0453, Eval RMSE on combined data: 0.2030\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1964\n",
      "Epoch 50/195, Train Loss: 0.0452, Eval RMSE on combined data: 0.2141\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1964\n",
      "Epoch 51/195, Train Loss: 0.0483, Eval RMSE on combined data: 0.2010\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1964\n",
      "Epoch 52/195, Train Loss: 0.0457, Eval RMSE on combined data: 0.2042\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1964\n",
      "Epoch 53/195, Train Loss: 0.0445, Eval RMSE on combined data: 0.1938\n",
      "--- New best final model saved at epoch 53 with RMSE: 0.1938 ---\n",
      "Epoch 54/195, Train Loss: 0.0439, Eval RMSE on combined data: 0.1947\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1938\n",
      "Epoch 55/195, Train Loss: 0.0453, Eval RMSE on combined data: 0.2089\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1938\n",
      "Epoch 56/195, Train Loss: 0.0417, Eval RMSE on combined data: 0.1960\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1938\n",
      "Epoch 57/195, Train Loss: 0.0426, Eval RMSE on combined data: 0.1906\n",
      "--- New best final model saved at epoch 57 with RMSE: 0.1906 ---\n",
      "Epoch 58/195, Train Loss: 0.0422, Eval RMSE on combined data: 0.1969\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1906\n",
      "Epoch 59/195, Train Loss: 0.0414, Eval RMSE on combined data: 0.1946\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1906\n",
      "Epoch 60/195, Train Loss: 0.0410, Eval RMSE on combined data: 0.1890\n",
      "--- New best final model saved at epoch 60 with RMSE: 0.1890 ---\n",
      "Epoch 61/195, Train Loss: 0.0399, Eval RMSE on combined data: 0.1846\n",
      "--- New best final model saved at epoch 61 with RMSE: 0.1846 ---\n",
      "Epoch 62/195, Train Loss: 0.0399, Eval RMSE on combined data: 0.1826\n",
      "--- New best final model saved at epoch 62 with RMSE: 0.1826 ---\n",
      "Epoch 63/195, Train Loss: 0.0385, Eval RMSE on combined data: 0.1839\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1826\n",
      "Epoch 64/195, Train Loss: 0.0383, Eval RMSE on combined data: 0.1819\n",
      "--- New best final model saved at epoch 64 with RMSE: 0.1819 ---\n",
      "Epoch 65/195, Train Loss: 0.0388, Eval RMSE on combined data: 0.1902\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1819\n",
      "Epoch 66/195, Train Loss: 0.0396, Eval RMSE on combined data: 0.1892\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1819\n",
      "Epoch 67/195, Train Loss: 0.0417, Eval RMSE on combined data: 0.1903\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1819\n",
      "Epoch 68/195, Train Loss: 0.0389, Eval RMSE on combined data: 0.1807\n",
      "--- New best final model saved at epoch 68 with RMSE: 0.1807 ---\n",
      "Epoch 69/195, Train Loss: 0.0377, Eval RMSE on combined data: 0.1754\n",
      "--- New best final model saved at epoch 69 with RMSE: 0.1754 ---\n",
      "Epoch 70/195, Train Loss: 0.0366, Eval RMSE on combined data: 0.1816\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 71/195, Train Loss: 0.0372, Eval RMSE on combined data: 0.1863\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 72/195, Train Loss: 0.0410, Eval RMSE on combined data: 0.1856\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 73/195, Train Loss: 0.0389, Eval RMSE on combined data: 0.1977\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 74/195, Train Loss: 0.0362, Eval RMSE on combined data: 0.1760\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 75/195, Train Loss: 0.0362, Eval RMSE on combined data: 0.1763\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 76/195, Train Loss: 0.0367, Eval RMSE on combined data: 0.1754\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 77/195, Train Loss: 0.0357, Eval RMSE on combined data: 0.1891\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 78/195, Train Loss: 0.0360, Eval RMSE on combined data: 0.1770\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1754\n",
      "Epoch 79/195, Train Loss: 0.0351, Eval RMSE on combined data: 0.1695\n",
      "--- New best final model saved at epoch 79 with RMSE: 0.1695 ---\n",
      "Epoch 80/195, Train Loss: 0.0342, Eval RMSE on combined data: 0.1775\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1695\n",
      "Epoch 81/195, Train Loss: 0.0358, Eval RMSE on combined data: 0.1733\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1695\n",
      "Epoch 82/195, Train Loss: 0.0351, Eval RMSE on combined data: 0.1743\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1695\n",
      "Epoch 83/195, Train Loss: 0.0345, Eval RMSE on combined data: 0.1673\n",
      "--- New best final model saved at epoch 83 with RMSE: 0.1673 ---\n",
      "Epoch 84/195, Train Loss: 0.0336, Eval RMSE on combined data: 0.1737\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 85/195, Train Loss: 0.0345, Eval RMSE on combined data: 0.1752\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 86/195, Train Loss: 0.0339, Eval RMSE on combined data: 0.1737\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 87/195, Train Loss: 0.0328, Eval RMSE on combined data: 0.1724\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 88/195, Train Loss: 0.0321, Eval RMSE on combined data: 0.1693\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 89/195, Train Loss: 0.0342, Eval RMSE on combined data: 0.1690\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 90/195, Train Loss: 0.0324, Eval RMSE on combined data: 0.1713\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 91/195, Train Loss: 0.0320, Eval RMSE on combined data: 0.1694\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 92/195, Train Loss: 0.0327, Eval RMSE on combined data: 0.1717\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 93/195, Train Loss: 0.0335, Eval RMSE on combined data: 0.1678\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 94/195, Train Loss: 0.0334, Eval RMSE on combined data: 0.1682\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1673\n",
      "Epoch 95/195, Train Loss: 0.0329, Eval RMSE on combined data: 0.1633\n",
      "--- New best final model saved at epoch 95 with RMSE: 0.1633 ---\n",
      "Epoch 96/195, Train Loss: 0.0320, Eval RMSE on combined data: 0.1618\n",
      "--- New best final model saved at epoch 96 with RMSE: 0.1618 ---\n",
      "Epoch 97/195, Train Loss: 0.0315, Eval RMSE on combined data: 0.1679\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1618\n",
      "Epoch 98/195, Train Loss: 0.0323, Eval RMSE on combined data: 0.1725\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1618\n",
      "Epoch 99/195, Train Loss: 0.0307, Eval RMSE on combined data: 0.1595\n",
      "--- New best final model saved at epoch 99 with RMSE: 0.1595 ---\n",
      "Epoch 100/195, Train Loss: 0.0296, Eval RMSE on combined data: 0.1670\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1595\n",
      "Epoch 101/195, Train Loss: 0.0307, Eval RMSE on combined data: 0.1642\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1595\n",
      "Epoch 102/195, Train Loss: 0.0306, Eval RMSE on combined data: 0.1640\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1595\n",
      "Epoch 103/195, Train Loss: 0.0304, Eval RMSE on combined data: 0.1629\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1595\n",
      "Epoch 104/195, Train Loss: 0.0308, Eval RMSE on combined data: 0.1610\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1595\n",
      "Epoch 105/195, Train Loss: 0.0302, Eval RMSE on combined data: 0.1572\n",
      "--- New best final model saved at epoch 105 with RMSE: 0.1572 ---\n",
      "Epoch 106/195, Train Loss: 0.0299, Eval RMSE on combined data: 0.1632\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1572\n",
      "Epoch 107/195, Train Loss: 0.0309, Eval RMSE on combined data: 0.1645\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1572\n",
      "Epoch 108/195, Train Loss: 0.0301, Eval RMSE on combined data: 0.1571\n",
      "--- New best final model saved at epoch 108 with RMSE: 0.1571 ---\n",
      "Epoch 109/195, Train Loss: 0.0298, Eval RMSE on combined data: 0.1695\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 110/195, Train Loss: 0.0299, Eval RMSE on combined data: 0.1626\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 111/195, Train Loss: 0.0305, Eval RMSE on combined data: 0.1621\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 112/195, Train Loss: 0.0291, Eval RMSE on combined data: 0.1618\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 113/195, Train Loss: 0.0289, Eval RMSE on combined data: 0.1578\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 114/195, Train Loss: 0.0295, Eval RMSE on combined data: 0.1597\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 115/195, Train Loss: 0.0289, Eval RMSE on combined data: 0.1609\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1571\n",
      "Epoch 116/195, Train Loss: 0.0290, Eval RMSE on combined data: 0.1546\n",
      "--- New best final model saved at epoch 116 with RMSE: 0.1546 ---\n",
      "Epoch 117/195, Train Loss: 0.0280, Eval RMSE on combined data: 0.1575\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 118/195, Train Loss: 0.0284, Eval RMSE on combined data: 0.1643\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 119/195, Train Loss: 0.0283, Eval RMSE on combined data: 0.1609\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 120/195, Train Loss: 0.0275, Eval RMSE on combined data: 0.1617\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 121/195, Train Loss: 0.0279, Eval RMSE on combined data: 0.1603\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 122/195, Train Loss: 0.0280, Eval RMSE on combined data: 0.1563\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1546\n",
      "Epoch 123/195, Train Loss: 0.0273, Eval RMSE on combined data: 0.1509\n",
      "--- New best final model saved at epoch 123 with RMSE: 0.1509 ---\n",
      "Epoch 124/195, Train Loss: 0.0282, Eval RMSE on combined data: 0.1542\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 125/195, Train Loss: 0.0267, Eval RMSE on combined data: 0.1510\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 126/195, Train Loss: 0.0283, Eval RMSE on combined data: 0.1563\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 127/195, Train Loss: 0.0273, Eval RMSE on combined data: 0.1523\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 128/195, Train Loss: 0.0275, Eval RMSE on combined data: 0.1570\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 129/195, Train Loss: 0.0285, Eval RMSE on combined data: 0.1600\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1509\n",
      "Epoch 130/195, Train Loss: 0.0279, Eval RMSE on combined data: 0.1495\n",
      "--- New best final model saved at epoch 130 with RMSE: 0.1495 ---\n",
      "Epoch 131/195, Train Loss: 0.0273, Eval RMSE on combined data: 0.1548\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 132/195, Train Loss: 0.0271, Eval RMSE on combined data: 0.1549\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 133/195, Train Loss: 0.0281, Eval RMSE on combined data: 0.1585\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 134/195, Train Loss: 0.0266, Eval RMSE on combined data: 0.1522\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 135/195, Train Loss: 0.0259, Eval RMSE on combined data: 0.1517\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 136/195, Train Loss: 0.0267, Eval RMSE on combined data: 0.1512\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 137/195, Train Loss: 0.0259, Eval RMSE on combined data: 0.1504\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 138/195, Train Loss: 0.0259, Eval RMSE on combined data: 0.1549\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 139/195, Train Loss: 0.0260, Eval RMSE on combined data: 0.1496\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 140/195, Train Loss: 0.0258, Eval RMSE on combined data: 0.1567\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 141/195, Train Loss: 0.0262, Eval RMSE on combined data: 0.1506\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 142/195, Train Loss: 0.0266, Eval RMSE on combined data: 0.1514\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 143/195, Train Loss: 0.0273, Eval RMSE on combined data: 0.1556\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1495\n",
      "Epoch 144/195, Train Loss: 0.0262, Eval RMSE on combined data: 0.1477\n",
      "--- New best final model saved at epoch 144 with RMSE: 0.1477 ---\n",
      "Epoch 145/195, Train Loss: 0.0258, Eval RMSE on combined data: 0.1465\n",
      "--- New best final model saved at epoch 145 with RMSE: 0.1465 ---\n",
      "Epoch 146/195, Train Loss: 0.0258, Eval RMSE on combined data: 0.1507\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1465\n",
      "Epoch 147/195, Train Loss: 0.0252, Eval RMSE on combined data: 0.1477\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1465\n",
      "Epoch 148/195, Train Loss: 0.0247, Eval RMSE on combined data: 0.1467\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1465\n",
      "Epoch 149/195, Train Loss: 0.0248, Eval RMSE on combined data: 0.1456\n",
      "--- New best final model saved at epoch 149 with RMSE: 0.1456 ---\n",
      "Epoch 150/195, Train Loss: 0.0249, Eval RMSE on combined data: 0.1450\n",
      "--- New best final model saved at epoch 150 with RMSE: 0.1450 ---\n",
      "Epoch 151/195, Train Loss: 0.0239, Eval RMSE on combined data: 0.1425\n",
      "--- New best final model saved at epoch 151 with RMSE: 0.1425 ---\n",
      "Epoch 152/195, Train Loss: 0.0241, Eval RMSE on combined data: 0.1450\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 153/195, Train Loss: 0.0256, Eval RMSE on combined data: 0.1469\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 154/195, Train Loss: 0.0248, Eval RMSE on combined data: 0.1462\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 155/195, Train Loss: 0.0253, Eval RMSE on combined data: 0.1498\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 156/195, Train Loss: 0.0259, Eval RMSE on combined data: 0.1449\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 157/195, Train Loss: 0.0241, Eval RMSE on combined data: 0.1434\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 158/195, Train Loss: 0.0249, Eval RMSE on combined data: 0.1491\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 159/195, Train Loss: 0.0255, Eval RMSE on combined data: 0.1427\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1425\n",
      "Epoch 160/195, Train Loss: 0.0248, Eval RMSE on combined data: 0.1419\n",
      "--- New best final model saved at epoch 160 with RMSE: 0.1419 ---\n",
      "Epoch 161/195, Train Loss: 0.0248, Eval RMSE on combined data: 0.1481\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1419\n",
      "Epoch 162/195, Train Loss: 0.0242, Eval RMSE on combined data: 0.1439\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1419\n",
      "Epoch 163/195, Train Loss: 0.0237, Eval RMSE on combined data: 0.1475\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1419\n",
      "Epoch 164/195, Train Loss: 0.0247, Eval RMSE on combined data: 0.1430\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1419\n",
      "Epoch 165/195, Train Loss: 0.0250, Eval RMSE on combined data: 0.1450\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1419\n",
      "Epoch 166/195, Train Loss: 0.0238, Eval RMSE on combined data: 0.1413\n",
      "--- New best final model saved at epoch 166 with RMSE: 0.1413 ---\n",
      "Epoch 167/195, Train Loss: 0.0240, Eval RMSE on combined data: 0.1423\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1413\n",
      "Epoch 168/195, Train Loss: 0.0232, Eval RMSE on combined data: 0.1482\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1413\n",
      "Epoch 169/195, Train Loss: 0.0243, Eval RMSE on combined data: 0.1453\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1413\n",
      "Epoch 170/195, Train Loss: 0.0246, Eval RMSE on combined data: 0.1453\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1413\n",
      "Epoch 171/195, Train Loss: 0.0237, Eval RMSE on combined data: 0.1391\n",
      "--- New best final model saved at epoch 171 with RMSE: 0.1391 ---\n",
      "Epoch 172/195, Train Loss: 0.0237, Eval RMSE on combined data: 0.1448\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 173/195, Train Loss: 0.0239, Eval RMSE on combined data: 0.1465\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 174/195, Train Loss: 0.0240, Eval RMSE on combined data: 0.1406\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 175/195, Train Loss: 0.0241, Eval RMSE on combined data: 0.1456\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 176/195, Train Loss: 0.0237, Eval RMSE on combined data: 0.1464\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 177/195, Train Loss: 0.0238, Eval RMSE on combined data: 0.1433\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 178/195, Train Loss: 0.0240, Eval RMSE on combined data: 0.1421\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 179/195, Train Loss: 0.0230, Eval RMSE on combined data: 0.1408\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 180/195, Train Loss: 0.0234, Eval RMSE on combined data: 0.1437\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1391\n",
      "Epoch 181/195, Train Loss: 0.0234, Eval RMSE on combined data: 0.1384\n",
      "--- New best final model saved at epoch 181 with RMSE: 0.1384 ---\n",
      "Epoch 182/195, Train Loss: 0.0231, Eval RMSE on combined data: 0.1422\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1384\n",
      "Epoch 183/195, Train Loss: 0.0231, Eval RMSE on combined data: 0.1413\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1384\n",
      "Epoch 184/195, Train Loss: 0.0232, Eval RMSE on combined data: 0.1428\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1384\n",
      "Epoch 185/195, Train Loss: 0.0233, Eval RMSE on combined data: 0.1422\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1384\n",
      "Epoch 186/195, Train Loss: 0.0229, Eval RMSE on combined data: 0.1439\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1384\n",
      "Epoch 187/195, Train Loss: 0.0225, Eval RMSE on combined data: 0.1357\n",
      "--- New best final model saved at epoch 187 with RMSE: 0.1357 ---\n",
      "Epoch 188/195, Train Loss: 0.0220, Eval RMSE on combined data: 0.1410\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 189/195, Train Loss: 0.0232, Eval RMSE on combined data: 0.1390\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 190/195, Train Loss: 0.0230, Eval RMSE on combined data: 0.1413\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 191/195, Train Loss: 0.0219, Eval RMSE on combined data: 0.1458\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 192/195, Train Loss: 0.0230, Eval RMSE on combined data: 0.1359\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 193/195, Train Loss: 0.0221, Eval RMSE on combined data: 0.1399\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 194/195, Train Loss: 0.0225, Eval RMSE on combined data: 0.1430\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1357\n",
      "Epoch 195/195, Train Loss: 0.0227, Eval RMSE on combined data: 0.1394\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1357\n",
      "Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "best_final_val_rmse = float('inf')\n",
    "patience_counter_final = 0\n",
    "final_patience = 50\n",
    "\n",
    "current_commit_hash = get_git_commit_hash()\n",
    "model_filename = f\"final_best_mlp_model_{current_commit_hash}.pt\" # Pre-define filename\n",
    "\n",
    "print(f\"Retraining final MLP model for {best_n_epochs} epochs with best parameters...\")\n",
    "print(f\"Associated Git Commit ID for saved model: {current_commit_hash}\")\n",
    "\n",
    "for epoch in range(best_n_epochs):\n",
    "    # Training\n",
    "    final_mlp_model.train()\n",
    "    total_train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    for batch_x, batch_y in final_train_val_loader:\n",
    "        # Move data to device\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_mlp_model(batch_x)\n",
    "        loss = final_criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "\n",
    "    # Evaluation\n",
    "    final_mlp_model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x_eval, batch_y_eval in final_train_val_loader:\n",
    "            batch_x_eval = batch_x_eval.to(device)\n",
    "            batch_y_eval = batch_y_eval.to(device)\n",
    "            \n",
    "            val_outputs = final_mlp_model(batch_x_eval)\n",
    "            val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            val_targets.extend(batch_y_eval.cpu().numpy().flatten())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "    print(f\"Epoch {epoch+1}/{best_n_epochs}, Train Loss: {avg_train_loss:.4f}, Eval RMSE on combined data: {current_val_rmse:.4f}\")\n",
    "\n",
    "    # Dynamic Best Model Saving & Early Stopping\n",
    "    if current_val_rmse < best_final_val_rmse:\n",
    "        best_final_val_rmse = current_val_rmse\n",
    "        torch.save(final_mlp_model.state_dict(), mlp_models_base_dir / model_filename) # Save the model state\n",
    "        patience_counter_final = 0 # Reset patience counter if performance improved\n",
    "        print(f\"--- New best final model saved at epoch {epoch+1} with RMSE: {current_val_rmse:.4f} ---\")\n",
    "    else:\n",
    "        patience_counter_final += 1 # Increment patience counter if no improvement\n",
    "        print(f\"No improvement for {patience_counter_final} epochs. Best RMSE so far: {best_final_val_rmse:.4f}\")\n",
    "\n",
    "    if patience_counter_final >= final_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "print(\"Final model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75fe20e-5de7-46c7-ab2b-2ffb51e2768a",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c684fe3e-88a0-4b8f-b021-11604d562405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best saved model from 'final_best_mlp_model_60d86ce.pt' for final test evaluation...\n",
      "\n",
      "Starting final evaluation on test set...\n",
      "Final Model Test RMSE: 0.6418\n",
      "Final Model Test R2: 0.5703\n"
     ]
    }
   ],
   "source": [
    "# Load the best state dict model\n",
    "print(f\"Loading best saved model from '{model_filename}' for final test evaluation...\")\n",
    "path_to_saved_model = mlp_models_base_dir / model_filename\n",
    "loaded_model_state_dict = torch.load(path_to_saved_model)\n",
    "final_mlp_model.load_state_dict(loaded_model_state_dict)\n",
    "final_mlp_model.eval() # Set to evaluation mode for final test\n",
    "\n",
    "print(\"\\nStarting final evaluation on test set...\")\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch_x_test, batch_y_test in final_test_loader:\n",
    "        batch_x_test = batch_x_test.to(device)\n",
    "        batch_y_test = batch_y_test.to(device)\n",
    "\n",
    "        test_outputs = final_mlp_model(batch_x_test)\n",
    "        test_predictions.extend(test_outputs.cpu().numpy().flatten())\n",
    "        test_targets.extend(batch_y_test.cpu().numpy().flatten())\n",
    "\n",
    "final_test_rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))\n",
    "final_test_r2 = r2_score(test_targets, test_predictions)\n",
    "\n",
    "print(f\"Final Model Test RMSE: {final_test_rmse:.4f}\")\n",
    "print(f\"Final Model Test R2: {final_test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
