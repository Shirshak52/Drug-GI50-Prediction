{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bd2555-5c6e-4a73-be36-c637e121dbde",
   "metadata": {},
   "source": [
    "# Initial Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba522b06-3bb8-4944-88b5-ec728a3dd895",
   "metadata": {},
   "source": [
    "## Setup Environment and Project Path Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cfbd48-ceb7-4107-a3e7-720588151435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch threads: 16\n",
      "PyTorch interop threads: 16\n",
      "Project root added to sys.path: C:\\Users\\Acer\\Desktop\\Projects for Data Science\\Drug Gi50 Value Prediction\n"
     ]
    }
   ],
   "source": [
    "# General CPU Usage Optimization\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "os.environ['MKL_NUM_THREADS'] = '16'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '16'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
    "\n",
    "import time\n",
    "\n",
    "# PyTorch-specific CPU Usage Optimization\n",
    "import torch\n",
    "try:\n",
    "    torch.set_num_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_threads.\\n{e}\")\n",
    "\n",
    "try:\n",
    "    torch.set_num_interop_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_interop_threads.\\n{e}\")\n",
    "\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"PyTorch interop threads: {torch.get_num_interop_threads()}\")\n",
    "\n",
    "# Configure Project Path for Module Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root directory\n",
    "project_root = Path(current_dir).parent.resolve()\n",
    "\n",
    "# Add the project root to sys.path if it's not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "\n",
    "# General Utility for Timestamps\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e4ed-618b-45c9-ab02-79404947c941",
   "metadata": {},
   "source": [
    "## Import Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607b77a9-8ab7-4dde-b15c-cae9a6a0e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem  # For basic molecule handling\n",
    "from rdkit.Chem import AllChem  # For atom features like Gasteiger charges, and other utilities\n",
    "\n",
    "# PyTorch Core for Neural Networks\n",
    "import torch.nn as nn  # Neural network modules like Linear, ReLU, MSELoss\n",
    "import torch.nn.functional as F  # Functional interface for activations, e.g. F.ReLU\n",
    "import torch.optim as optim  # Optimization functions like Adam, AdamW, etc.\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "\n",
    "# PyTorch Geometric for Graph Neural Networks\n",
    "from torch_geometric.data import Data # The graph data object in PyG\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader # PyG DataLoader for graphs\n",
    "import torch_geometric.nn as pyg_nn # Common GNN layers (e.g., GCNConv, GraphSAGEConv)\n",
    "import torch_geometric.utils as pyg_utils # Utility functions for graph manipulation\n",
    "\n",
    "# GNN Model Class\n",
    "from src.models.gnn_models import GNN\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mixed Precision Training (for GPU-accelerated training)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Git commit ID for final model filename (for reproducibility)\n",
    "import subprocess\n",
    "\n",
    "# For graph visualization (optional)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73369f-d2e5-42ad-98e1-5d1cfba0d789",
   "metadata": {},
   "source": [
    "## Import Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ce2845-e6d6-4546-929f-568de3729445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm.notebook found and enabled for pandas.\n"
     ]
    }
   ],
   "source": [
    "# Progress bars\n",
    "tqdm_notebook_available = False\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas() # Enable tqdm for pandas apply\n",
    "    tqdm_notebook_available = True\n",
    "    print(\"tqdm.notebook found and enabled for pandas.\")\n",
    "except ImportError:\n",
    "    print(\"tqdm.notebook not found. Install with 'pip install tqdm'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afdaa5-b8f4-4c1c-9231-edf19fbc344c",
   "metadata": {},
   "source": [
    "## Define Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385334b2-5c91-4353-a650-b5e6c9b74825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939c1c3-4217-4c59-837e-0f59067088b6",
   "metadata": {},
   "source": [
    "## Set Final Model Save Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd70cd4-d396-4621-a342-7d6876c6f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best final GNN model will be saved in: ..\\models\\gnn\n"
     ]
    }
   ],
   "source": [
    "gnn_models_base_dir = Path(\"../models/gnn\")\n",
    "gnn_models_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"The best final GNN model will be saved in: {gnn_models_base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af314e-5d67-4625-a656-cfb57a00d6e4",
   "metadata": {},
   "source": [
    "# Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d4c581-37f9-42ef-a421-e67646669758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data splits from ..\\data\\splits...\n",
      "Data splits loaded successfully.\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1)\n",
      "y_val shape: (2812, 1)\n",
      "y_test shape: (2812, 1)\n",
      "\n",
      "First 5 rows of X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of y_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits_dir = Path(\"../data/splits\")\n",
    "print(f\"\\nLoading data splits from {splits_dir}...\")\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet(splits_dir / \"X_train.parquet\")\n",
    "    X_val = pd.read_parquet(splits_dir / \"X_val.parquet\")\n",
    "    X_test = pd.read_parquet(splits_dir / \"X_test.parquet\")\n",
    "    \n",
    "    y_train = pd.read_parquet(splits_dir / \"y_train.parquet\")\n",
    "    y_val = pd.read_parquet(splits_dir / \"y_val.parquet\")\n",
    "    y_test = pd.read_parquet(splits_dir / \"y_test.parquet\")\n",
    "    print(\"Data splits loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more split files not found in '{splits_dir}'.\")\n",
    "    print(\"Please ensure you have run '02_Split_Features.ipynb' to generate and save the splits.\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"\\nFirst 5 rows of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of y_train:\")\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c86cd-f9d1-4b3f-ac55-99cae504d03a",
   "metadata": {},
   "source": [
    "# Prepare Data for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf767878-0371-4b4e-acb6-63085a6ef138",
   "metadata": {},
   "source": [
    "## Extract Global Features of Each Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4eecff-86f8-4cdc-b34d-d67ac68c7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Excracting global features of each molecule ---\n",
      "Identified 2266 global feature columns for GNN.\n",
      "Global feature columns: ['num_activities', 'MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAmideBonds', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumAtomStereoCenters', 'NumBridgeheadAtoms', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'NumSpiroAtoms', 'NumUnspecifiedAtomStereoCenters', 'Phi', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea', 'morgan_fp_0', 'morgan_fp_1', 'morgan_fp_2', 'morgan_fp_3', 'morgan_fp_4', 'morgan_fp_5', 'morgan_fp_6', 'morgan_fp_7', 'morgan_fp_8', 'morgan_fp_9', 'morgan_fp_10', 'morgan_fp_11', 'morgan_fp_12', 'morgan_fp_13', 'morgan_fp_14', 'morgan_fp_15', 'morgan_fp_16', 'morgan_fp_17', 'morgan_fp_18', 'morgan_fp_19', 'morgan_fp_20', 'morgan_fp_21', 'morgan_fp_22', 'morgan_fp_23', 'morgan_fp_24', 'morgan_fp_25', 'morgan_fp_26', 'morgan_fp_27', 'morgan_fp_28', 'morgan_fp_29', 'morgan_fp_30', 'morgan_fp_31', 'morgan_fp_32', 'morgan_fp_33', 'morgan_fp_34', 'morgan_fp_35', 'morgan_fp_36', 'morgan_fp_37', 'morgan_fp_38', 'morgan_fp_39', 'morgan_fp_40', 'morgan_fp_41', 'morgan_fp_42', 'morgan_fp_43', 'morgan_fp_44', 'morgan_fp_45', 'morgan_fp_46', 'morgan_fp_47', 'morgan_fp_48', 'morgan_fp_49', 'morgan_fp_50', 'morgan_fp_51', 'morgan_fp_52', 'morgan_fp_53', 'morgan_fp_54', 'morgan_fp_55', 'morgan_fp_56', 'morgan_fp_57', 'morgan_fp_58', 'morgan_fp_59', 'morgan_fp_60', 'morgan_fp_61', 'morgan_fp_62', 'morgan_fp_63', 'morgan_fp_64', 'morgan_fp_65', 'morgan_fp_66', 'morgan_fp_67', 'morgan_fp_68', 'morgan_fp_69', 'morgan_fp_70', 'morgan_fp_71', 'morgan_fp_72', 'morgan_fp_73', 'morgan_fp_74', 'morgan_fp_75', 'morgan_fp_76', 'morgan_fp_77', 'morgan_fp_78', 'morgan_fp_79', 'morgan_fp_80', 'morgan_fp_81', 'morgan_fp_82', 'morgan_fp_83', 'morgan_fp_84', 'morgan_fp_85', 'morgan_fp_86', 'morgan_fp_87', 'morgan_fp_88', 'morgan_fp_89', 'morgan_fp_90', 'morgan_fp_91', 'morgan_fp_92', 'morgan_fp_93', 'morgan_fp_94', 'morgan_fp_95', 'morgan_fp_96', 'morgan_fp_97', 'morgan_fp_98', 'morgan_fp_99', 'morgan_fp_100', 'morgan_fp_101', 'morgan_fp_102', 'morgan_fp_103', 'morgan_fp_104', 'morgan_fp_105', 'morgan_fp_106', 'morgan_fp_107', 'morgan_fp_108', 'morgan_fp_109', 'morgan_fp_110', 'morgan_fp_111', 'morgan_fp_112', 'morgan_fp_113', 'morgan_fp_114', 'morgan_fp_115', 'morgan_fp_116', 'morgan_fp_117', 'morgan_fp_118', 'morgan_fp_119', 'morgan_fp_120', 'morgan_fp_121', 'morgan_fp_122', 'morgan_fp_123', 'morgan_fp_124', 'morgan_fp_125', 'morgan_fp_126', 'morgan_fp_127', 'morgan_fp_128', 'morgan_fp_129', 'morgan_fp_130', 'morgan_fp_131', 'morgan_fp_132', 'morgan_fp_133', 'morgan_fp_134', 'morgan_fp_135', 'morgan_fp_136', 'morgan_fp_137', 'morgan_fp_138', 'morgan_fp_139', 'morgan_fp_140', 'morgan_fp_141', 'morgan_fp_142', 'morgan_fp_143', 'morgan_fp_144', 'morgan_fp_145', 'morgan_fp_146', 'morgan_fp_147', 'morgan_fp_148', 'morgan_fp_149', 'morgan_fp_150', 'morgan_fp_151', 'morgan_fp_152', 'morgan_fp_153', 'morgan_fp_154', 'morgan_fp_155', 'morgan_fp_156', 'morgan_fp_157', 'morgan_fp_158', 'morgan_fp_159', 'morgan_fp_160', 'morgan_fp_161', 'morgan_fp_162', 'morgan_fp_163', 'morgan_fp_164', 'morgan_fp_165', 'morgan_fp_166', 'morgan_fp_167', 'morgan_fp_168', 'morgan_fp_169', 'morgan_fp_170', 'morgan_fp_171', 'morgan_fp_172', 'morgan_fp_173', 'morgan_fp_174', 'morgan_fp_175', 'morgan_fp_176', 'morgan_fp_177', 'morgan_fp_178', 'morgan_fp_179', 'morgan_fp_180', 'morgan_fp_181', 'morgan_fp_182', 'morgan_fp_183', 'morgan_fp_184', 'morgan_fp_185', 'morgan_fp_186', 'morgan_fp_187', 'morgan_fp_188', 'morgan_fp_189', 'morgan_fp_190', 'morgan_fp_191', 'morgan_fp_192', 'morgan_fp_193', 'morgan_fp_194', 'morgan_fp_195', 'morgan_fp_196', 'morgan_fp_197', 'morgan_fp_198', 'morgan_fp_199', 'morgan_fp_200', 'morgan_fp_201', 'morgan_fp_202', 'morgan_fp_203', 'morgan_fp_204', 'morgan_fp_205', 'morgan_fp_206', 'morgan_fp_207', 'morgan_fp_208', 'morgan_fp_209', 'morgan_fp_210', 'morgan_fp_211', 'morgan_fp_212', 'morgan_fp_213', 'morgan_fp_214', 'morgan_fp_215', 'morgan_fp_216', 'morgan_fp_217', 'morgan_fp_218', 'morgan_fp_219', 'morgan_fp_220', 'morgan_fp_221', 'morgan_fp_222', 'morgan_fp_223', 'morgan_fp_224', 'morgan_fp_225', 'morgan_fp_226', 'morgan_fp_227', 'morgan_fp_228', 'morgan_fp_229', 'morgan_fp_230', 'morgan_fp_231', 'morgan_fp_232', 'morgan_fp_233', 'morgan_fp_234', 'morgan_fp_235', 'morgan_fp_236', 'morgan_fp_237', 'morgan_fp_238', 'morgan_fp_239', 'morgan_fp_240', 'morgan_fp_241', 'morgan_fp_242', 'morgan_fp_243', 'morgan_fp_244', 'morgan_fp_245', 'morgan_fp_246', 'morgan_fp_247', 'morgan_fp_248', 'morgan_fp_249', 'morgan_fp_250', 'morgan_fp_251', 'morgan_fp_252', 'morgan_fp_253', 'morgan_fp_254', 'morgan_fp_255', 'morgan_fp_256', 'morgan_fp_257', 'morgan_fp_258', 'morgan_fp_259', 'morgan_fp_260', 'morgan_fp_261', 'morgan_fp_262', 'morgan_fp_263', 'morgan_fp_264', 'morgan_fp_265', 'morgan_fp_266', 'morgan_fp_267', 'morgan_fp_268', 'morgan_fp_269', 'morgan_fp_270', 'morgan_fp_271', 'morgan_fp_272', 'morgan_fp_273', 'morgan_fp_274', 'morgan_fp_275', 'morgan_fp_276', 'morgan_fp_277', 'morgan_fp_278', 'morgan_fp_279', 'morgan_fp_280', 'morgan_fp_281', 'morgan_fp_282', 'morgan_fp_283', 'morgan_fp_284', 'morgan_fp_285', 'morgan_fp_286', 'morgan_fp_287', 'morgan_fp_288', 'morgan_fp_289', 'morgan_fp_290', 'morgan_fp_291', 'morgan_fp_292', 'morgan_fp_293', 'morgan_fp_294', 'morgan_fp_295', 'morgan_fp_296', 'morgan_fp_297', 'morgan_fp_298', 'morgan_fp_299', 'morgan_fp_300', 'morgan_fp_301', 'morgan_fp_302', 'morgan_fp_303', 'morgan_fp_304', 'morgan_fp_305', 'morgan_fp_306', 'morgan_fp_307', 'morgan_fp_308', 'morgan_fp_309', 'morgan_fp_310', 'morgan_fp_311', 'morgan_fp_312', 'morgan_fp_313', 'morgan_fp_314', 'morgan_fp_315', 'morgan_fp_316', 'morgan_fp_317', 'morgan_fp_318', 'morgan_fp_319', 'morgan_fp_320', 'morgan_fp_321', 'morgan_fp_322', 'morgan_fp_323', 'morgan_fp_324', 'morgan_fp_325', 'morgan_fp_326', 'morgan_fp_327', 'morgan_fp_328', 'morgan_fp_329', 'morgan_fp_330', 'morgan_fp_331', 'morgan_fp_332', 'morgan_fp_333', 'morgan_fp_334', 'morgan_fp_335', 'morgan_fp_336', 'morgan_fp_337', 'morgan_fp_338', 'morgan_fp_339', 'morgan_fp_340', 'morgan_fp_341', 'morgan_fp_342', 'morgan_fp_343', 'morgan_fp_344', 'morgan_fp_345', 'morgan_fp_346', 'morgan_fp_347', 'morgan_fp_348', 'morgan_fp_349', 'morgan_fp_350', 'morgan_fp_351', 'morgan_fp_352', 'morgan_fp_353', 'morgan_fp_354', 'morgan_fp_355', 'morgan_fp_356', 'morgan_fp_357', 'morgan_fp_358', 'morgan_fp_359', 'morgan_fp_360', 'morgan_fp_361', 'morgan_fp_362', 'morgan_fp_363', 'morgan_fp_364', 'morgan_fp_365', 'morgan_fp_366', 'morgan_fp_367', 'morgan_fp_368', 'morgan_fp_369', 'morgan_fp_370', 'morgan_fp_371', 'morgan_fp_372', 'morgan_fp_373', 'morgan_fp_374', 'morgan_fp_375', 'morgan_fp_376', 'morgan_fp_377', 'morgan_fp_378', 'morgan_fp_379', 'morgan_fp_380', 'morgan_fp_381', 'morgan_fp_382', 'morgan_fp_383', 'morgan_fp_384', 'morgan_fp_385', 'morgan_fp_386', 'morgan_fp_387', 'morgan_fp_388', 'morgan_fp_389', 'morgan_fp_390', 'morgan_fp_391', 'morgan_fp_392', 'morgan_fp_393', 'morgan_fp_394', 'morgan_fp_395', 'morgan_fp_396', 'morgan_fp_397', 'morgan_fp_398', 'morgan_fp_399', 'morgan_fp_400', 'morgan_fp_401', 'morgan_fp_402', 'morgan_fp_403', 'morgan_fp_404', 'morgan_fp_405', 'morgan_fp_406', 'morgan_fp_407', 'morgan_fp_408', 'morgan_fp_409', 'morgan_fp_410', 'morgan_fp_411', 'morgan_fp_412', 'morgan_fp_413', 'morgan_fp_414', 'morgan_fp_415', 'morgan_fp_416', 'morgan_fp_417', 'morgan_fp_418', 'morgan_fp_419', 'morgan_fp_420', 'morgan_fp_421', 'morgan_fp_422', 'morgan_fp_423', 'morgan_fp_424', 'morgan_fp_425', 'morgan_fp_426', 'morgan_fp_427', 'morgan_fp_428', 'morgan_fp_429', 'morgan_fp_430', 'morgan_fp_431', 'morgan_fp_432', 'morgan_fp_433', 'morgan_fp_434', 'morgan_fp_435', 'morgan_fp_436', 'morgan_fp_437', 'morgan_fp_438', 'morgan_fp_439', 'morgan_fp_440', 'morgan_fp_441', 'morgan_fp_442', 'morgan_fp_443', 'morgan_fp_444', 'morgan_fp_445', 'morgan_fp_446', 'morgan_fp_447', 'morgan_fp_448', 'morgan_fp_449', 'morgan_fp_450', 'morgan_fp_451', 'morgan_fp_452', 'morgan_fp_453', 'morgan_fp_454', 'morgan_fp_455', 'morgan_fp_456', 'morgan_fp_457', 'morgan_fp_458', 'morgan_fp_459', 'morgan_fp_460', 'morgan_fp_461', 'morgan_fp_462', 'morgan_fp_463', 'morgan_fp_464', 'morgan_fp_465', 'morgan_fp_466', 'morgan_fp_467', 'morgan_fp_468', 'morgan_fp_469', 'morgan_fp_470', 'morgan_fp_471', 'morgan_fp_472', 'morgan_fp_473', 'morgan_fp_474', 'morgan_fp_475', 'morgan_fp_476', 'morgan_fp_477', 'morgan_fp_478', 'morgan_fp_479', 'morgan_fp_480', 'morgan_fp_481', 'morgan_fp_482', 'morgan_fp_483', 'morgan_fp_484', 'morgan_fp_485', 'morgan_fp_486', 'morgan_fp_487', 'morgan_fp_488', 'morgan_fp_489', 'morgan_fp_490', 'morgan_fp_491', 'morgan_fp_492', 'morgan_fp_493', 'morgan_fp_494', 'morgan_fp_495', 'morgan_fp_496', 'morgan_fp_497', 'morgan_fp_498', 'morgan_fp_499', 'morgan_fp_500', 'morgan_fp_501', 'morgan_fp_502', 'morgan_fp_503', 'morgan_fp_504', 'morgan_fp_505', 'morgan_fp_506', 'morgan_fp_507', 'morgan_fp_508', 'morgan_fp_509', 'morgan_fp_510', 'morgan_fp_511', 'morgan_fp_512', 'morgan_fp_513', 'morgan_fp_514', 'morgan_fp_515', 'morgan_fp_516', 'morgan_fp_517', 'morgan_fp_518', 'morgan_fp_519', 'morgan_fp_520', 'morgan_fp_521', 'morgan_fp_522', 'morgan_fp_523', 'morgan_fp_524', 'morgan_fp_525', 'morgan_fp_526', 'morgan_fp_527', 'morgan_fp_528', 'morgan_fp_529', 'morgan_fp_530', 'morgan_fp_531', 'morgan_fp_532', 'morgan_fp_533', 'morgan_fp_534', 'morgan_fp_535', 'morgan_fp_536', 'morgan_fp_537', 'morgan_fp_538', 'morgan_fp_539', 'morgan_fp_540', 'morgan_fp_541', 'morgan_fp_542', 'morgan_fp_543', 'morgan_fp_544', 'morgan_fp_545', 'morgan_fp_546', 'morgan_fp_547', 'morgan_fp_548', 'morgan_fp_549', 'morgan_fp_550', 'morgan_fp_551', 'morgan_fp_552', 'morgan_fp_553', 'morgan_fp_554', 'morgan_fp_555', 'morgan_fp_556', 'morgan_fp_557', 'morgan_fp_558', 'morgan_fp_559', 'morgan_fp_560', 'morgan_fp_561', 'morgan_fp_562', 'morgan_fp_563', 'morgan_fp_564', 'morgan_fp_565', 'morgan_fp_566', 'morgan_fp_567', 'morgan_fp_568', 'morgan_fp_569', 'morgan_fp_570', 'morgan_fp_571', 'morgan_fp_572', 'morgan_fp_573', 'morgan_fp_574', 'morgan_fp_575', 'morgan_fp_576', 'morgan_fp_577', 'morgan_fp_578', 'morgan_fp_579', 'morgan_fp_580', 'morgan_fp_581', 'morgan_fp_582', 'morgan_fp_583', 'morgan_fp_584', 'morgan_fp_585', 'morgan_fp_586', 'morgan_fp_587', 'morgan_fp_588', 'morgan_fp_589', 'morgan_fp_590', 'morgan_fp_591', 'morgan_fp_592', 'morgan_fp_593', 'morgan_fp_594', 'morgan_fp_595', 'morgan_fp_596', 'morgan_fp_597', 'morgan_fp_598', 'morgan_fp_599', 'morgan_fp_600', 'morgan_fp_601', 'morgan_fp_602', 'morgan_fp_603', 'morgan_fp_604', 'morgan_fp_605', 'morgan_fp_606', 'morgan_fp_607', 'morgan_fp_608', 'morgan_fp_609', 'morgan_fp_610', 'morgan_fp_611', 'morgan_fp_612', 'morgan_fp_613', 'morgan_fp_614', 'morgan_fp_615', 'morgan_fp_616', 'morgan_fp_617', 'morgan_fp_618', 'morgan_fp_619', 'morgan_fp_620', 'morgan_fp_621', 'morgan_fp_622', 'morgan_fp_623', 'morgan_fp_624', 'morgan_fp_625', 'morgan_fp_626', 'morgan_fp_627', 'morgan_fp_628', 'morgan_fp_629', 'morgan_fp_630', 'morgan_fp_631', 'morgan_fp_632', 'morgan_fp_633', 'morgan_fp_634', 'morgan_fp_635', 'morgan_fp_636', 'morgan_fp_637', 'morgan_fp_638', 'morgan_fp_639', 'morgan_fp_640', 'morgan_fp_641', 'morgan_fp_642', 'morgan_fp_643', 'morgan_fp_644', 'morgan_fp_645', 'morgan_fp_646', 'morgan_fp_647', 'morgan_fp_648', 'morgan_fp_649', 'morgan_fp_650', 'morgan_fp_651', 'morgan_fp_652', 'morgan_fp_653', 'morgan_fp_654', 'morgan_fp_655', 'morgan_fp_656', 'morgan_fp_657', 'morgan_fp_658', 'morgan_fp_659', 'morgan_fp_660', 'morgan_fp_661', 'morgan_fp_662', 'morgan_fp_663', 'morgan_fp_664', 'morgan_fp_665', 'morgan_fp_666', 'morgan_fp_667', 'morgan_fp_668', 'morgan_fp_669', 'morgan_fp_670', 'morgan_fp_671', 'morgan_fp_672', 'morgan_fp_673', 'morgan_fp_674', 'morgan_fp_675', 'morgan_fp_676', 'morgan_fp_677', 'morgan_fp_678', 'morgan_fp_679', 'morgan_fp_680', 'morgan_fp_681', 'morgan_fp_682', 'morgan_fp_683', 'morgan_fp_684', 'morgan_fp_685', 'morgan_fp_686', 'morgan_fp_687', 'morgan_fp_688', 'morgan_fp_689', 'morgan_fp_690', 'morgan_fp_691', 'morgan_fp_692', 'morgan_fp_693', 'morgan_fp_694', 'morgan_fp_695', 'morgan_fp_696', 'morgan_fp_697', 'morgan_fp_698', 'morgan_fp_699', 'morgan_fp_700', 'morgan_fp_701', 'morgan_fp_702', 'morgan_fp_703', 'morgan_fp_704', 'morgan_fp_705', 'morgan_fp_706', 'morgan_fp_707', 'morgan_fp_708', 'morgan_fp_709', 'morgan_fp_710', 'morgan_fp_711', 'morgan_fp_712', 'morgan_fp_713', 'morgan_fp_714', 'morgan_fp_715', 'morgan_fp_716', 'morgan_fp_717', 'morgan_fp_718', 'morgan_fp_719', 'morgan_fp_720', 'morgan_fp_721', 'morgan_fp_722', 'morgan_fp_723', 'morgan_fp_724', 'morgan_fp_725', 'morgan_fp_726', 'morgan_fp_727', 'morgan_fp_728', 'morgan_fp_729', 'morgan_fp_730', 'morgan_fp_731', 'morgan_fp_732', 'morgan_fp_733', 'morgan_fp_734', 'morgan_fp_735', 'morgan_fp_736', 'morgan_fp_737', 'morgan_fp_738', 'morgan_fp_739', 'morgan_fp_740', 'morgan_fp_741', 'morgan_fp_742', 'morgan_fp_743', 'morgan_fp_744', 'morgan_fp_745', 'morgan_fp_746', 'morgan_fp_747', 'morgan_fp_748', 'morgan_fp_749', 'morgan_fp_750', 'morgan_fp_751', 'morgan_fp_752', 'morgan_fp_753', 'morgan_fp_754', 'morgan_fp_755', 'morgan_fp_756', 'morgan_fp_757', 'morgan_fp_758', 'morgan_fp_759', 'morgan_fp_760', 'morgan_fp_761', 'morgan_fp_762', 'morgan_fp_763', 'morgan_fp_764', 'morgan_fp_765', 'morgan_fp_766', 'morgan_fp_767', 'morgan_fp_768', 'morgan_fp_769', 'morgan_fp_770', 'morgan_fp_771', 'morgan_fp_772', 'morgan_fp_773', 'morgan_fp_774', 'morgan_fp_775', 'morgan_fp_776', 'morgan_fp_777', 'morgan_fp_778', 'morgan_fp_779', 'morgan_fp_780', 'morgan_fp_781', 'morgan_fp_782', 'morgan_fp_783', 'morgan_fp_784', 'morgan_fp_785', 'morgan_fp_786', 'morgan_fp_787', 'morgan_fp_788', 'morgan_fp_789', 'morgan_fp_790', 'morgan_fp_791', 'morgan_fp_792', 'morgan_fp_793', 'morgan_fp_794', 'morgan_fp_795', 'morgan_fp_796', 'morgan_fp_797', 'morgan_fp_798', 'morgan_fp_799', 'morgan_fp_800', 'morgan_fp_801', 'morgan_fp_802', 'morgan_fp_803', 'morgan_fp_804', 'morgan_fp_805', 'morgan_fp_806', 'morgan_fp_807', 'morgan_fp_808', 'morgan_fp_809', 'morgan_fp_810', 'morgan_fp_811', 'morgan_fp_812', 'morgan_fp_813', 'morgan_fp_814', 'morgan_fp_815', 'morgan_fp_816', 'morgan_fp_817', 'morgan_fp_818', 'morgan_fp_819', 'morgan_fp_820', 'morgan_fp_821', 'morgan_fp_822', 'morgan_fp_823', 'morgan_fp_824', 'morgan_fp_825', 'morgan_fp_826', 'morgan_fp_827', 'morgan_fp_828', 'morgan_fp_829', 'morgan_fp_830', 'morgan_fp_831', 'morgan_fp_832', 'morgan_fp_833', 'morgan_fp_834', 'morgan_fp_835', 'morgan_fp_836', 'morgan_fp_837', 'morgan_fp_838', 'morgan_fp_839', 'morgan_fp_840', 'morgan_fp_841', 'morgan_fp_842', 'morgan_fp_843', 'morgan_fp_844', 'morgan_fp_845', 'morgan_fp_846', 'morgan_fp_847', 'morgan_fp_848', 'morgan_fp_849', 'morgan_fp_850', 'morgan_fp_851', 'morgan_fp_852', 'morgan_fp_853', 'morgan_fp_854', 'morgan_fp_855', 'morgan_fp_856', 'morgan_fp_857', 'morgan_fp_858', 'morgan_fp_859', 'morgan_fp_860', 'morgan_fp_861', 'morgan_fp_862', 'morgan_fp_863', 'morgan_fp_864', 'morgan_fp_865', 'morgan_fp_866', 'morgan_fp_867', 'morgan_fp_868', 'morgan_fp_869', 'morgan_fp_870', 'morgan_fp_871', 'morgan_fp_872', 'morgan_fp_873', 'morgan_fp_874', 'morgan_fp_875', 'morgan_fp_876', 'morgan_fp_877', 'morgan_fp_878', 'morgan_fp_879', 'morgan_fp_880', 'morgan_fp_881', 'morgan_fp_882', 'morgan_fp_883', 'morgan_fp_884', 'morgan_fp_885', 'morgan_fp_886', 'morgan_fp_887', 'morgan_fp_888', 'morgan_fp_889', 'morgan_fp_890', 'morgan_fp_891', 'morgan_fp_892', 'morgan_fp_893', 'morgan_fp_894', 'morgan_fp_895', 'morgan_fp_896', 'morgan_fp_897', 'morgan_fp_898', 'morgan_fp_899', 'morgan_fp_900', 'morgan_fp_901', 'morgan_fp_902', 'morgan_fp_903', 'morgan_fp_904', 'morgan_fp_905', 'morgan_fp_906', 'morgan_fp_907', 'morgan_fp_908', 'morgan_fp_909', 'morgan_fp_910', 'morgan_fp_911', 'morgan_fp_912', 'morgan_fp_913', 'morgan_fp_914', 'morgan_fp_915', 'morgan_fp_916', 'morgan_fp_917', 'morgan_fp_918', 'morgan_fp_919', 'morgan_fp_920', 'morgan_fp_921', 'morgan_fp_922', 'morgan_fp_923', 'morgan_fp_924', 'morgan_fp_925', 'morgan_fp_926', 'morgan_fp_927', 'morgan_fp_928', 'morgan_fp_929', 'morgan_fp_930', 'morgan_fp_931', 'morgan_fp_932', 'morgan_fp_933', 'morgan_fp_934', 'morgan_fp_935', 'morgan_fp_936', 'morgan_fp_937', 'morgan_fp_938', 'morgan_fp_939', 'morgan_fp_940', 'morgan_fp_941', 'morgan_fp_942', 'morgan_fp_943', 'morgan_fp_944', 'morgan_fp_945', 'morgan_fp_946', 'morgan_fp_947', 'morgan_fp_948', 'morgan_fp_949', 'morgan_fp_950', 'morgan_fp_951', 'morgan_fp_952', 'morgan_fp_953', 'morgan_fp_954', 'morgan_fp_955', 'morgan_fp_956', 'morgan_fp_957', 'morgan_fp_958', 'morgan_fp_959', 'morgan_fp_960', 'morgan_fp_961', 'morgan_fp_962', 'morgan_fp_963', 'morgan_fp_964', 'morgan_fp_965', 'morgan_fp_966', 'morgan_fp_967', 'morgan_fp_968', 'morgan_fp_969', 'morgan_fp_970', 'morgan_fp_971', 'morgan_fp_972', 'morgan_fp_973', 'morgan_fp_974', 'morgan_fp_975', 'morgan_fp_976', 'morgan_fp_977', 'morgan_fp_978', 'morgan_fp_979', 'morgan_fp_980', 'morgan_fp_981', 'morgan_fp_982', 'morgan_fp_983', 'morgan_fp_984', 'morgan_fp_985', 'morgan_fp_986', 'morgan_fp_987', 'morgan_fp_988', 'morgan_fp_989', 'morgan_fp_990', 'morgan_fp_991', 'morgan_fp_992', 'morgan_fp_993', 'morgan_fp_994', 'morgan_fp_995', 'morgan_fp_996', 'morgan_fp_997', 'morgan_fp_998', 'morgan_fp_999', 'morgan_fp_1000', 'morgan_fp_1001', 'morgan_fp_1002', 'morgan_fp_1003', 'morgan_fp_1004', 'morgan_fp_1005', 'morgan_fp_1006', 'morgan_fp_1007', 'morgan_fp_1008', 'morgan_fp_1009', 'morgan_fp_1010', 'morgan_fp_1011', 'morgan_fp_1012', 'morgan_fp_1013', 'morgan_fp_1014', 'morgan_fp_1015', 'morgan_fp_1016', 'morgan_fp_1017', 'morgan_fp_1018', 'morgan_fp_1019', 'morgan_fp_1020', 'morgan_fp_1021', 'morgan_fp_1022', 'morgan_fp_1023', 'morgan_fp_1024', 'morgan_fp_1025', 'morgan_fp_1026', 'morgan_fp_1027', 'morgan_fp_1028', 'morgan_fp_1029', 'morgan_fp_1030', 'morgan_fp_1031', 'morgan_fp_1032', 'morgan_fp_1033', 'morgan_fp_1034', 'morgan_fp_1035', 'morgan_fp_1036', 'morgan_fp_1037', 'morgan_fp_1038', 'morgan_fp_1039', 'morgan_fp_1040', 'morgan_fp_1041', 'morgan_fp_1042', 'morgan_fp_1043', 'morgan_fp_1044', 'morgan_fp_1045', 'morgan_fp_1046', 'morgan_fp_1047', 'morgan_fp_1048', 'morgan_fp_1049', 'morgan_fp_1050', 'morgan_fp_1051', 'morgan_fp_1052', 'morgan_fp_1053', 'morgan_fp_1054', 'morgan_fp_1055', 'morgan_fp_1056', 'morgan_fp_1057', 'morgan_fp_1058', 'morgan_fp_1059', 'morgan_fp_1060', 'morgan_fp_1061', 'morgan_fp_1062', 'morgan_fp_1063', 'morgan_fp_1064', 'morgan_fp_1065', 'morgan_fp_1066', 'morgan_fp_1067', 'morgan_fp_1068', 'morgan_fp_1069', 'morgan_fp_1070', 'morgan_fp_1071', 'morgan_fp_1072', 'morgan_fp_1073', 'morgan_fp_1074', 'morgan_fp_1075', 'morgan_fp_1076', 'morgan_fp_1077', 'morgan_fp_1078', 'morgan_fp_1079', 'morgan_fp_1080', 'morgan_fp_1081', 'morgan_fp_1082', 'morgan_fp_1083', 'morgan_fp_1084', 'morgan_fp_1085', 'morgan_fp_1086', 'morgan_fp_1087', 'morgan_fp_1088', 'morgan_fp_1089', 'morgan_fp_1090', 'morgan_fp_1091', 'morgan_fp_1092', 'morgan_fp_1093', 'morgan_fp_1094', 'morgan_fp_1095', 'morgan_fp_1096', 'morgan_fp_1097', 'morgan_fp_1098', 'morgan_fp_1099', 'morgan_fp_1100', 'morgan_fp_1101', 'morgan_fp_1102', 'morgan_fp_1103', 'morgan_fp_1104', 'morgan_fp_1105', 'morgan_fp_1106', 'morgan_fp_1107', 'morgan_fp_1108', 'morgan_fp_1109', 'morgan_fp_1110', 'morgan_fp_1111', 'morgan_fp_1112', 'morgan_fp_1113', 'morgan_fp_1114', 'morgan_fp_1115', 'morgan_fp_1116', 'morgan_fp_1117', 'morgan_fp_1118', 'morgan_fp_1119', 'morgan_fp_1120', 'morgan_fp_1121', 'morgan_fp_1122', 'morgan_fp_1123', 'morgan_fp_1124', 'morgan_fp_1125', 'morgan_fp_1126', 'morgan_fp_1127', 'morgan_fp_1128', 'morgan_fp_1129', 'morgan_fp_1130', 'morgan_fp_1131', 'morgan_fp_1132', 'morgan_fp_1133', 'morgan_fp_1134', 'morgan_fp_1135', 'morgan_fp_1136', 'morgan_fp_1137', 'morgan_fp_1138', 'morgan_fp_1139', 'morgan_fp_1140', 'morgan_fp_1141', 'morgan_fp_1142', 'morgan_fp_1143', 'morgan_fp_1144', 'morgan_fp_1145', 'morgan_fp_1146', 'morgan_fp_1147', 'morgan_fp_1148', 'morgan_fp_1149', 'morgan_fp_1150', 'morgan_fp_1151', 'morgan_fp_1152', 'morgan_fp_1153', 'morgan_fp_1154', 'morgan_fp_1155', 'morgan_fp_1156', 'morgan_fp_1157', 'morgan_fp_1158', 'morgan_fp_1159', 'morgan_fp_1160', 'morgan_fp_1161', 'morgan_fp_1162', 'morgan_fp_1163', 'morgan_fp_1164', 'morgan_fp_1165', 'morgan_fp_1166', 'morgan_fp_1167', 'morgan_fp_1168', 'morgan_fp_1169', 'morgan_fp_1170', 'morgan_fp_1171', 'morgan_fp_1172', 'morgan_fp_1173', 'morgan_fp_1174', 'morgan_fp_1175', 'morgan_fp_1176', 'morgan_fp_1177', 'morgan_fp_1178', 'morgan_fp_1179', 'morgan_fp_1180', 'morgan_fp_1181', 'morgan_fp_1182', 'morgan_fp_1183', 'morgan_fp_1184', 'morgan_fp_1185', 'morgan_fp_1186', 'morgan_fp_1187', 'morgan_fp_1188', 'morgan_fp_1189', 'morgan_fp_1190', 'morgan_fp_1191', 'morgan_fp_1192', 'morgan_fp_1193', 'morgan_fp_1194', 'morgan_fp_1195', 'morgan_fp_1196', 'morgan_fp_1197', 'morgan_fp_1198', 'morgan_fp_1199', 'morgan_fp_1200', 'morgan_fp_1201', 'morgan_fp_1202', 'morgan_fp_1203', 'morgan_fp_1204', 'morgan_fp_1205', 'morgan_fp_1206', 'morgan_fp_1207', 'morgan_fp_1208', 'morgan_fp_1209', 'morgan_fp_1210', 'morgan_fp_1211', 'morgan_fp_1212', 'morgan_fp_1213', 'morgan_fp_1214', 'morgan_fp_1215', 'morgan_fp_1216', 'morgan_fp_1217', 'morgan_fp_1218', 'morgan_fp_1219', 'morgan_fp_1220', 'morgan_fp_1221', 'morgan_fp_1222', 'morgan_fp_1223', 'morgan_fp_1224', 'morgan_fp_1225', 'morgan_fp_1226', 'morgan_fp_1227', 'morgan_fp_1228', 'morgan_fp_1229', 'morgan_fp_1230', 'morgan_fp_1231', 'morgan_fp_1232', 'morgan_fp_1233', 'morgan_fp_1234', 'morgan_fp_1235', 'morgan_fp_1236', 'morgan_fp_1237', 'morgan_fp_1238', 'morgan_fp_1239', 'morgan_fp_1240', 'morgan_fp_1241', 'morgan_fp_1242', 'morgan_fp_1243', 'morgan_fp_1244', 'morgan_fp_1245', 'morgan_fp_1246', 'morgan_fp_1247', 'morgan_fp_1248', 'morgan_fp_1249', 'morgan_fp_1250', 'morgan_fp_1251', 'morgan_fp_1252', 'morgan_fp_1253', 'morgan_fp_1254', 'morgan_fp_1255', 'morgan_fp_1256', 'morgan_fp_1257', 'morgan_fp_1258', 'morgan_fp_1259', 'morgan_fp_1260', 'morgan_fp_1261', 'morgan_fp_1262', 'morgan_fp_1263', 'morgan_fp_1264', 'morgan_fp_1265', 'morgan_fp_1266', 'morgan_fp_1267', 'morgan_fp_1268', 'morgan_fp_1269', 'morgan_fp_1270', 'morgan_fp_1271', 'morgan_fp_1272', 'morgan_fp_1273', 'morgan_fp_1274', 'morgan_fp_1275', 'morgan_fp_1276', 'morgan_fp_1277', 'morgan_fp_1278', 'morgan_fp_1279', 'morgan_fp_1280', 'morgan_fp_1281', 'morgan_fp_1282', 'morgan_fp_1283', 'morgan_fp_1284', 'morgan_fp_1285', 'morgan_fp_1286', 'morgan_fp_1287', 'morgan_fp_1288', 'morgan_fp_1289', 'morgan_fp_1290', 'morgan_fp_1291', 'morgan_fp_1292', 'morgan_fp_1293', 'morgan_fp_1294', 'morgan_fp_1295', 'morgan_fp_1296', 'morgan_fp_1297', 'morgan_fp_1298', 'morgan_fp_1299', 'morgan_fp_1300', 'morgan_fp_1301', 'morgan_fp_1302', 'morgan_fp_1303', 'morgan_fp_1304', 'morgan_fp_1305', 'morgan_fp_1306', 'morgan_fp_1307', 'morgan_fp_1308', 'morgan_fp_1309', 'morgan_fp_1310', 'morgan_fp_1311', 'morgan_fp_1312', 'morgan_fp_1313', 'morgan_fp_1314', 'morgan_fp_1315', 'morgan_fp_1316', 'morgan_fp_1317', 'morgan_fp_1318', 'morgan_fp_1319', 'morgan_fp_1320', 'morgan_fp_1321', 'morgan_fp_1322', 'morgan_fp_1323', 'morgan_fp_1324', 'morgan_fp_1325', 'morgan_fp_1326', 'morgan_fp_1327', 'morgan_fp_1328', 'morgan_fp_1329', 'morgan_fp_1330', 'morgan_fp_1331', 'morgan_fp_1332', 'morgan_fp_1333', 'morgan_fp_1334', 'morgan_fp_1335', 'morgan_fp_1336', 'morgan_fp_1337', 'morgan_fp_1338', 'morgan_fp_1339', 'morgan_fp_1340', 'morgan_fp_1341', 'morgan_fp_1342', 'morgan_fp_1343', 'morgan_fp_1344', 'morgan_fp_1345', 'morgan_fp_1346', 'morgan_fp_1347', 'morgan_fp_1348', 'morgan_fp_1349', 'morgan_fp_1350', 'morgan_fp_1351', 'morgan_fp_1352', 'morgan_fp_1353', 'morgan_fp_1354', 'morgan_fp_1355', 'morgan_fp_1356', 'morgan_fp_1357', 'morgan_fp_1358', 'morgan_fp_1359', 'morgan_fp_1360', 'morgan_fp_1361', 'morgan_fp_1362', 'morgan_fp_1363', 'morgan_fp_1364', 'morgan_fp_1365', 'morgan_fp_1366', 'morgan_fp_1367', 'morgan_fp_1368', 'morgan_fp_1369', 'morgan_fp_1370', 'morgan_fp_1371', 'morgan_fp_1372', 'morgan_fp_1373', 'morgan_fp_1374', 'morgan_fp_1375', 'morgan_fp_1376', 'morgan_fp_1377', 'morgan_fp_1378', 'morgan_fp_1379', 'morgan_fp_1380', 'morgan_fp_1381', 'morgan_fp_1382', 'morgan_fp_1383', 'morgan_fp_1384', 'morgan_fp_1385', 'morgan_fp_1386', 'morgan_fp_1387', 'morgan_fp_1388', 'morgan_fp_1389', 'morgan_fp_1390', 'morgan_fp_1391', 'morgan_fp_1392', 'morgan_fp_1393', 'morgan_fp_1394', 'morgan_fp_1395', 'morgan_fp_1396', 'morgan_fp_1397', 'morgan_fp_1398', 'morgan_fp_1399', 'morgan_fp_1400', 'morgan_fp_1401', 'morgan_fp_1402', 'morgan_fp_1403', 'morgan_fp_1404', 'morgan_fp_1405', 'morgan_fp_1406', 'morgan_fp_1407', 'morgan_fp_1408', 'morgan_fp_1409', 'morgan_fp_1410', 'morgan_fp_1411', 'morgan_fp_1412', 'morgan_fp_1413', 'morgan_fp_1414', 'morgan_fp_1415', 'morgan_fp_1416', 'morgan_fp_1417', 'morgan_fp_1418', 'morgan_fp_1419', 'morgan_fp_1420', 'morgan_fp_1421', 'morgan_fp_1422', 'morgan_fp_1423', 'morgan_fp_1424', 'morgan_fp_1425', 'morgan_fp_1426', 'morgan_fp_1427', 'morgan_fp_1428', 'morgan_fp_1429', 'morgan_fp_1430', 'morgan_fp_1431', 'morgan_fp_1432', 'morgan_fp_1433', 'morgan_fp_1434', 'morgan_fp_1435', 'morgan_fp_1436', 'morgan_fp_1437', 'morgan_fp_1438', 'morgan_fp_1439', 'morgan_fp_1440', 'morgan_fp_1441', 'morgan_fp_1442', 'morgan_fp_1443', 'morgan_fp_1444', 'morgan_fp_1445', 'morgan_fp_1446', 'morgan_fp_1447', 'morgan_fp_1448', 'morgan_fp_1449', 'morgan_fp_1450', 'morgan_fp_1451', 'morgan_fp_1452', 'morgan_fp_1453', 'morgan_fp_1454', 'morgan_fp_1455', 'morgan_fp_1456', 'morgan_fp_1457', 'morgan_fp_1458', 'morgan_fp_1459', 'morgan_fp_1460', 'morgan_fp_1461', 'morgan_fp_1462', 'morgan_fp_1463', 'morgan_fp_1464', 'morgan_fp_1465', 'morgan_fp_1466', 'morgan_fp_1467', 'morgan_fp_1468', 'morgan_fp_1469', 'morgan_fp_1470', 'morgan_fp_1471', 'morgan_fp_1472', 'morgan_fp_1473', 'morgan_fp_1474', 'morgan_fp_1475', 'morgan_fp_1476', 'morgan_fp_1477', 'morgan_fp_1478', 'morgan_fp_1479', 'morgan_fp_1480', 'morgan_fp_1481', 'morgan_fp_1482', 'morgan_fp_1483', 'morgan_fp_1484', 'morgan_fp_1485', 'morgan_fp_1486', 'morgan_fp_1487', 'morgan_fp_1488', 'morgan_fp_1489', 'morgan_fp_1490', 'morgan_fp_1491', 'morgan_fp_1492', 'morgan_fp_1493', 'morgan_fp_1494', 'morgan_fp_1495', 'morgan_fp_1496', 'morgan_fp_1497', 'morgan_fp_1498', 'morgan_fp_1499', 'morgan_fp_1500', 'morgan_fp_1501', 'morgan_fp_1502', 'morgan_fp_1503', 'morgan_fp_1504', 'morgan_fp_1505', 'morgan_fp_1506', 'morgan_fp_1507', 'morgan_fp_1508', 'morgan_fp_1509', 'morgan_fp_1510', 'morgan_fp_1511', 'morgan_fp_1512', 'morgan_fp_1513', 'morgan_fp_1514', 'morgan_fp_1515', 'morgan_fp_1516', 'morgan_fp_1517', 'morgan_fp_1518', 'morgan_fp_1519', 'morgan_fp_1520', 'morgan_fp_1521', 'morgan_fp_1522', 'morgan_fp_1523', 'morgan_fp_1524', 'morgan_fp_1525', 'morgan_fp_1526', 'morgan_fp_1527', 'morgan_fp_1528', 'morgan_fp_1529', 'morgan_fp_1530', 'morgan_fp_1531', 'morgan_fp_1532', 'morgan_fp_1533', 'morgan_fp_1534', 'morgan_fp_1535', 'morgan_fp_1536', 'morgan_fp_1537', 'morgan_fp_1538', 'morgan_fp_1539', 'morgan_fp_1540', 'morgan_fp_1541', 'morgan_fp_1542', 'morgan_fp_1543', 'morgan_fp_1544', 'morgan_fp_1545', 'morgan_fp_1546', 'morgan_fp_1547', 'morgan_fp_1548', 'morgan_fp_1549', 'morgan_fp_1550', 'morgan_fp_1551', 'morgan_fp_1552', 'morgan_fp_1553', 'morgan_fp_1554', 'morgan_fp_1555', 'morgan_fp_1556', 'morgan_fp_1557', 'morgan_fp_1558', 'morgan_fp_1559', 'morgan_fp_1560', 'morgan_fp_1561', 'morgan_fp_1562', 'morgan_fp_1563', 'morgan_fp_1564', 'morgan_fp_1565', 'morgan_fp_1566', 'morgan_fp_1567', 'morgan_fp_1568', 'morgan_fp_1569', 'morgan_fp_1570', 'morgan_fp_1571', 'morgan_fp_1572', 'morgan_fp_1573', 'morgan_fp_1574', 'morgan_fp_1575', 'morgan_fp_1576', 'morgan_fp_1577', 'morgan_fp_1578', 'morgan_fp_1579', 'morgan_fp_1580', 'morgan_fp_1581', 'morgan_fp_1582', 'morgan_fp_1583', 'morgan_fp_1584', 'morgan_fp_1585', 'morgan_fp_1586', 'morgan_fp_1587', 'morgan_fp_1588', 'morgan_fp_1589', 'morgan_fp_1590', 'morgan_fp_1591', 'morgan_fp_1592', 'morgan_fp_1593', 'morgan_fp_1594', 'morgan_fp_1595', 'morgan_fp_1596', 'morgan_fp_1597', 'morgan_fp_1598', 'morgan_fp_1599', 'morgan_fp_1600', 'morgan_fp_1601', 'morgan_fp_1602', 'morgan_fp_1603', 'morgan_fp_1604', 'morgan_fp_1605', 'morgan_fp_1606', 'morgan_fp_1607', 'morgan_fp_1608', 'morgan_fp_1609', 'morgan_fp_1610', 'morgan_fp_1611', 'morgan_fp_1612', 'morgan_fp_1613', 'morgan_fp_1614', 'morgan_fp_1615', 'morgan_fp_1616', 'morgan_fp_1617', 'morgan_fp_1618', 'morgan_fp_1619', 'morgan_fp_1620', 'morgan_fp_1621', 'morgan_fp_1622', 'morgan_fp_1623', 'morgan_fp_1624', 'morgan_fp_1625', 'morgan_fp_1626', 'morgan_fp_1627', 'morgan_fp_1628', 'morgan_fp_1629', 'morgan_fp_1630', 'morgan_fp_1631', 'morgan_fp_1632', 'morgan_fp_1633', 'morgan_fp_1634', 'morgan_fp_1635', 'morgan_fp_1636', 'morgan_fp_1637', 'morgan_fp_1638', 'morgan_fp_1639', 'morgan_fp_1640', 'morgan_fp_1641', 'morgan_fp_1642', 'morgan_fp_1643', 'morgan_fp_1644', 'morgan_fp_1645', 'morgan_fp_1646', 'morgan_fp_1647', 'morgan_fp_1648', 'morgan_fp_1649', 'morgan_fp_1650', 'morgan_fp_1651', 'morgan_fp_1652', 'morgan_fp_1653', 'morgan_fp_1654', 'morgan_fp_1655', 'morgan_fp_1656', 'morgan_fp_1657', 'morgan_fp_1658', 'morgan_fp_1659', 'morgan_fp_1660', 'morgan_fp_1661', 'morgan_fp_1662', 'morgan_fp_1663', 'morgan_fp_1664', 'morgan_fp_1665', 'morgan_fp_1666', 'morgan_fp_1667', 'morgan_fp_1668', 'morgan_fp_1669', 'morgan_fp_1670', 'morgan_fp_1671', 'morgan_fp_1672', 'morgan_fp_1673', 'morgan_fp_1674', 'morgan_fp_1675', 'morgan_fp_1676', 'morgan_fp_1677', 'morgan_fp_1678', 'morgan_fp_1679', 'morgan_fp_1680', 'morgan_fp_1681', 'morgan_fp_1682', 'morgan_fp_1683', 'morgan_fp_1684', 'morgan_fp_1685', 'morgan_fp_1686', 'morgan_fp_1687', 'morgan_fp_1688', 'morgan_fp_1689', 'morgan_fp_1690', 'morgan_fp_1691', 'morgan_fp_1692', 'morgan_fp_1693', 'morgan_fp_1694', 'morgan_fp_1695', 'morgan_fp_1696', 'morgan_fp_1697', 'morgan_fp_1698', 'morgan_fp_1699', 'morgan_fp_1700', 'morgan_fp_1701', 'morgan_fp_1702', 'morgan_fp_1703', 'morgan_fp_1704', 'morgan_fp_1705', 'morgan_fp_1706', 'morgan_fp_1707', 'morgan_fp_1708', 'morgan_fp_1709', 'morgan_fp_1710', 'morgan_fp_1711', 'morgan_fp_1712', 'morgan_fp_1713', 'morgan_fp_1714', 'morgan_fp_1715', 'morgan_fp_1716', 'morgan_fp_1717', 'morgan_fp_1718', 'morgan_fp_1719', 'morgan_fp_1720', 'morgan_fp_1721', 'morgan_fp_1722', 'morgan_fp_1723', 'morgan_fp_1724', 'morgan_fp_1725', 'morgan_fp_1726', 'morgan_fp_1727', 'morgan_fp_1728', 'morgan_fp_1729', 'morgan_fp_1730', 'morgan_fp_1731', 'morgan_fp_1732', 'morgan_fp_1733', 'morgan_fp_1734', 'morgan_fp_1735', 'morgan_fp_1736', 'morgan_fp_1737', 'morgan_fp_1738', 'morgan_fp_1739', 'morgan_fp_1740', 'morgan_fp_1741', 'morgan_fp_1742', 'morgan_fp_1743', 'morgan_fp_1744', 'morgan_fp_1745', 'morgan_fp_1746', 'morgan_fp_1747', 'morgan_fp_1748', 'morgan_fp_1749', 'morgan_fp_1750', 'morgan_fp_1751', 'morgan_fp_1752', 'morgan_fp_1753', 'morgan_fp_1754', 'morgan_fp_1755', 'morgan_fp_1756', 'morgan_fp_1757', 'morgan_fp_1758', 'morgan_fp_1759', 'morgan_fp_1760', 'morgan_fp_1761', 'morgan_fp_1762', 'morgan_fp_1763', 'morgan_fp_1764', 'morgan_fp_1765', 'morgan_fp_1766', 'morgan_fp_1767', 'morgan_fp_1768', 'morgan_fp_1769', 'morgan_fp_1770', 'morgan_fp_1771', 'morgan_fp_1772', 'morgan_fp_1773', 'morgan_fp_1774', 'morgan_fp_1775', 'morgan_fp_1776', 'morgan_fp_1777', 'morgan_fp_1778', 'morgan_fp_1779', 'morgan_fp_1780', 'morgan_fp_1781', 'morgan_fp_1782', 'morgan_fp_1783', 'morgan_fp_1784', 'morgan_fp_1785', 'morgan_fp_1786', 'morgan_fp_1787', 'morgan_fp_1788', 'morgan_fp_1789', 'morgan_fp_1790', 'morgan_fp_1791', 'morgan_fp_1792', 'morgan_fp_1793', 'morgan_fp_1794', 'morgan_fp_1795', 'morgan_fp_1796', 'morgan_fp_1797', 'morgan_fp_1798', 'morgan_fp_1799', 'morgan_fp_1800', 'morgan_fp_1801', 'morgan_fp_1802', 'morgan_fp_1803', 'morgan_fp_1804', 'morgan_fp_1805', 'morgan_fp_1806', 'morgan_fp_1807', 'morgan_fp_1808', 'morgan_fp_1809', 'morgan_fp_1810', 'morgan_fp_1811', 'morgan_fp_1812', 'morgan_fp_1813', 'morgan_fp_1814', 'morgan_fp_1815', 'morgan_fp_1816', 'morgan_fp_1817', 'morgan_fp_1818', 'morgan_fp_1819', 'morgan_fp_1820', 'morgan_fp_1821', 'morgan_fp_1822', 'morgan_fp_1823', 'morgan_fp_1824', 'morgan_fp_1825', 'morgan_fp_1826', 'morgan_fp_1827', 'morgan_fp_1828', 'morgan_fp_1829', 'morgan_fp_1830', 'morgan_fp_1831', 'morgan_fp_1832', 'morgan_fp_1833', 'morgan_fp_1834', 'morgan_fp_1835', 'morgan_fp_1836', 'morgan_fp_1837', 'morgan_fp_1838', 'morgan_fp_1839', 'morgan_fp_1840', 'morgan_fp_1841', 'morgan_fp_1842', 'morgan_fp_1843', 'morgan_fp_1844', 'morgan_fp_1845', 'morgan_fp_1846', 'morgan_fp_1847', 'morgan_fp_1848', 'morgan_fp_1849', 'morgan_fp_1850', 'morgan_fp_1851', 'morgan_fp_1852', 'morgan_fp_1853', 'morgan_fp_1854', 'morgan_fp_1855', 'morgan_fp_1856', 'morgan_fp_1857', 'morgan_fp_1858', 'morgan_fp_1859', 'morgan_fp_1860', 'morgan_fp_1861', 'morgan_fp_1862', 'morgan_fp_1863', 'morgan_fp_1864', 'morgan_fp_1865', 'morgan_fp_1866', 'morgan_fp_1867', 'morgan_fp_1868', 'morgan_fp_1869', 'morgan_fp_1870', 'morgan_fp_1871', 'morgan_fp_1872', 'morgan_fp_1873', 'morgan_fp_1874', 'morgan_fp_1875', 'morgan_fp_1876', 'morgan_fp_1877', 'morgan_fp_1878', 'morgan_fp_1879', 'morgan_fp_1880', 'morgan_fp_1881', 'morgan_fp_1882', 'morgan_fp_1883', 'morgan_fp_1884', 'morgan_fp_1885', 'morgan_fp_1886', 'morgan_fp_1887', 'morgan_fp_1888', 'morgan_fp_1889', 'morgan_fp_1890', 'morgan_fp_1891', 'morgan_fp_1892', 'morgan_fp_1893', 'morgan_fp_1894', 'morgan_fp_1895', 'morgan_fp_1896', 'morgan_fp_1897', 'morgan_fp_1898', 'morgan_fp_1899', 'morgan_fp_1900', 'morgan_fp_1901', 'morgan_fp_1902', 'morgan_fp_1903', 'morgan_fp_1904', 'morgan_fp_1905', 'morgan_fp_1906', 'morgan_fp_1907', 'morgan_fp_1908', 'morgan_fp_1909', 'morgan_fp_1910', 'morgan_fp_1911', 'morgan_fp_1912', 'morgan_fp_1913', 'morgan_fp_1914', 'morgan_fp_1915', 'morgan_fp_1916', 'morgan_fp_1917', 'morgan_fp_1918', 'morgan_fp_1919', 'morgan_fp_1920', 'morgan_fp_1921', 'morgan_fp_1922', 'morgan_fp_1923', 'morgan_fp_1924', 'morgan_fp_1925', 'morgan_fp_1926', 'morgan_fp_1927', 'morgan_fp_1928', 'morgan_fp_1929', 'morgan_fp_1930', 'morgan_fp_1931', 'morgan_fp_1932', 'morgan_fp_1933', 'morgan_fp_1934', 'morgan_fp_1935', 'morgan_fp_1936', 'morgan_fp_1937', 'morgan_fp_1938', 'morgan_fp_1939', 'morgan_fp_1940', 'morgan_fp_1941', 'morgan_fp_1942', 'morgan_fp_1943', 'morgan_fp_1944', 'morgan_fp_1945', 'morgan_fp_1946', 'morgan_fp_1947', 'morgan_fp_1948', 'morgan_fp_1949', 'morgan_fp_1950', 'morgan_fp_1951', 'morgan_fp_1952', 'morgan_fp_1953', 'morgan_fp_1954', 'morgan_fp_1955', 'morgan_fp_1956', 'morgan_fp_1957', 'morgan_fp_1958', 'morgan_fp_1959', 'morgan_fp_1960', 'morgan_fp_1961', 'morgan_fp_1962', 'morgan_fp_1963', 'morgan_fp_1964', 'morgan_fp_1965', 'morgan_fp_1966', 'morgan_fp_1967', 'morgan_fp_1968', 'morgan_fp_1969', 'morgan_fp_1970', 'morgan_fp_1971', 'morgan_fp_1972', 'morgan_fp_1973', 'morgan_fp_1974', 'morgan_fp_1975', 'morgan_fp_1976', 'morgan_fp_1977', 'morgan_fp_1978', 'morgan_fp_1979', 'morgan_fp_1980', 'morgan_fp_1981', 'morgan_fp_1982', 'morgan_fp_1983', 'morgan_fp_1984', 'morgan_fp_1985', 'morgan_fp_1986', 'morgan_fp_1987', 'morgan_fp_1988', 'morgan_fp_1989', 'morgan_fp_1990', 'morgan_fp_1991', 'morgan_fp_1992', 'morgan_fp_1993', 'morgan_fp_1994', 'morgan_fp_1995', 'morgan_fp_1996', 'morgan_fp_1997', 'morgan_fp_1998', 'morgan_fp_1999', 'morgan_fp_2000', 'morgan_fp_2001', 'morgan_fp_2002', 'morgan_fp_2003', 'morgan_fp_2004', 'morgan_fp_2005', 'morgan_fp_2006', 'morgan_fp_2007', 'morgan_fp_2008', 'morgan_fp_2009', 'morgan_fp_2010', 'morgan_fp_2011', 'morgan_fp_2012', 'morgan_fp_2013', 'morgan_fp_2014', 'morgan_fp_2015', 'morgan_fp_2016', 'morgan_fp_2017', 'morgan_fp_2018', 'morgan_fp_2019', 'morgan_fp_2020', 'morgan_fp_2021', 'morgan_fp_2022', 'morgan_fp_2023', 'morgan_fp_2024', 'morgan_fp_2025', 'morgan_fp_2026', 'morgan_fp_2027', 'morgan_fp_2028', 'morgan_fp_2029', 'morgan_fp_2030', 'morgan_fp_2031', 'morgan_fp_2032', 'morgan_fp_2033', 'morgan_fp_2034', 'morgan_fp_2035', 'morgan_fp_2036', 'morgan_fp_2037', 'morgan_fp_2038', 'morgan_fp_2039', 'morgan_fp_2040', 'morgan_fp_2041', 'morgan_fp_2042', 'morgan_fp_2043', 'morgan_fp_2044', 'morgan_fp_2045', 'morgan_fp_2046', 'morgan_fp_2047']\n",
      "\n",
      "Extracted global features for each split:\n",
      "X_train_global_features shape: (13119, 2266)\n",
      "X_val_global_features shape: (2812, 2266)\n",
      "X_test_global_features shape: (2812, 2266)\n",
      "\n",
      "First 5 rows of X_train_global_features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>312.243</td>\n",
       "      <td>328.121178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>334.222</td>\n",
       "      <td>353.126323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>418.303</td>\n",
       "      <td>447.215806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>360.102</td>\n",
       "      <td>374.026604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>453.727</td>\n",
       "      <td>472.111375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  HeavyAtomMolWt  ExactMolWt  \\\n",
       "0        0.494176  0.476742  12.560000  328.371         312.243  328.121178   \n",
       "1        0.459195  0.604738  12.923077  353.374         334.222  353.126323   \n",
       "2       -0.173158  0.359463  15.909091  447.535         418.303  447.215806   \n",
       "3       -0.686457  0.479732  11.217391  375.222         360.102  374.026604   \n",
       "4       -3.116139  0.437556  16.121212  472.879         453.727  472.111375   \n",
       "\n",
       "   ...  morgan_fp_2038  morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  \\\n",
       "0  ...               0               0               0               0   \n",
       "1  ...               0               0               0               0   \n",
       "2  ...               0               0               0               0   \n",
       "3  ...               0               0               0               0   \n",
       "4  ...               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2042  morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2046  morgan_fp_2047  \n",
       "0               0               0  \n",
       "1               0               0  \n",
       "2               0               0  \n",
       "3               0               0  \n",
       "4               0               0  \n",
       "\n",
       "[5 rows x 2266 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1), y_val shape: (2812, 1), y_test shape: (2812, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Excracting global features of each molecule ---\")\n",
    "\n",
    "# Identify columns for global features\n",
    "# These are all columns in X_train EXCEPT 'molregno' and 'canonical_smiles'\n",
    "global_feature_columns = X_train.drop(columns=['molregno', 'canonical_smiles'], errors='ignore').columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(global_feature_columns)} global feature columns for GNN.\")\n",
    "print(f\"Global feature columns: {global_feature_columns}\")\n",
    "\n",
    "# Extract global features into new DataFrames\n",
    "# These DataFrames will be the source for data.global_features in GNN Data objects\n",
    "X_train_global_features = X_train[global_feature_columns]\n",
    "X_val_global_features = X_val[global_feature_columns]\n",
    "X_test_global_features = X_test[global_feature_columns]\n",
    "\n",
    "print(\"\\nExtracted global features for each split:\")\n",
    "print(f\"X_train_global_features shape: {X_train_global_features.shape}\")\n",
    "print(f\"X_val_global_features shape: {X_val_global_features.shape}\")\n",
    "print(f\"X_test_global_features shape: {X_test_global_features.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_train_global_features:\")\n",
    "display(X_train_global_features.head())\n",
    "\n",
    "\n",
    "# Confirm original X DataFrames (with molregno and canonical_smiles) are still available\n",
    "# These will be used for iterating and building individual graph objects.\n",
    "print(\"\\nOriginal X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "display(X_train.head()) # Show that original X_train still has molregno and canonical_smiles\n",
    "display(y_train.head()) # Show the pGI50 target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff4a6c-ef8d-4aef-863c-22bc039fe563",
   "metadata": {},
   "source": [
    "## Create Graph Objects of Each Molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8982d07-be78-4701-a6bc-45ebcd87fd11",
   "metadata": {},
   "source": [
    "### Define Helper Function to Create Graph Object of One Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c331d7-1a8a-47dc-b1f4-99b7429ae398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_pyg_data(mol, pgi50_value, global_features_vector, molregno, smiles_string):\n",
    "    if mol is None:\n",
    "        return None  # Handle cases where SMILES parsing fails\n",
    "\n",
    "    # Compute Gasteiger charges (how electron-dense the area occupied by this atom is, crucial for interactions)\n",
    "    try:\n",
    "        AllChem.ComputeGasteigerCharges(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute Gasteiger charges for molregno {molregno}: {e}\")\n",
    "        # If computation fails, atoms will default to 0.0 for this property\n",
    "        pass\n",
    "\n",
    "    # Node Features (x): Atom Properties\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        # Initialize a list for this atom's features\n",
    "        features = []\n",
    "\n",
    "        # Atomic Number (int, not one-hot coded)\n",
    "        features.append(atom.GetAtomicNum())\n",
    "\n",
    "        # Basic Connectivity\n",
    "        features.append(atom.GetDegree())  # Num of directly-bonded heavy (non-Hydrogen) atoms\n",
    "        features.append(atom.GetTotalDegree())  # Total numb of neighbors (including all Hydrogens)\n",
    "\n",
    "        # Charge and Valence\n",
    "        features.append(atom.GetFormalCharge())  # Formal charge (integer charge based on bonding rules)\n",
    "        features.append(atom.GetNumExplicitHs())  # Number of explicitly defined hydrogens attached\n",
    "        features.append(atom.GetNumImplicitHs())  # Number of hydrogens implicitly defined by valence\n",
    "        features.append(atom.GetTotalNumHs())  # Total number of hydrogens attached (explicit + implicit)\n",
    "        features.append(atom.GetValence(Chem.ValenceType.IMPLICIT))  # Implicit Valence: Number of bonds formed by implicit hydrogens\n",
    "        features.append(atom.GetValence(Chem.ValenceType.EXPLICIT))  # Explicit Valence: Sum of bond orders (1 for single, 2 for double, etc.) to explicitly defined atoms\n",
    "        features.append(atom.GetTotalValence())  # Total Valence: Total number of bonds (sum of explicit & implicit valence)\n",
    "\n",
    "        # Hybridization (convert enum to int) (e.g., sp3, sp2)\n",
    "        features.append(int(atom.GetHybridization()))\n",
    "\n",
    "        # Aromaticity and Ring Information (boolean converted to int)\n",
    "        features.append(int(atom.GetIsAromatic()))        # Whether the atom is part of an aromatic system\n",
    "        features.append(int(atom.IsInRing()))             # Whether the atom is in ANY ring structure\n",
    "        features.append(int(atom.IsInRingSize(3)))        # Whether the atom is in a 3-membered ring\n",
    "        features.append(int(atom.IsInRingSize(4)))        # Whether the atom is in a 4-membered ring\n",
    "        features.append(int(atom.IsInRingSize(5)))        # Whether the atom is in a 5-membered ring\n",
    "        features.append(int(atom.IsInRingSize(6)))        # Whether the atom is in a 6-membered ring\n",
    "        features.append(int(atom.IsInRingSize(7)))        # Whether the atom is in a 7-membered ring\n",
    "        features.append(int(atom.IsInRingSize(8)))        # Whether the atom is in an 8-membered ring\n",
    "\n",
    "        # Chirality (convert enum to int)(stereochemical information, crucial for biological activity)\n",
    "        features.append(int(atom.GetChiralTag()))\n",
    "\n",
    "         # Partial Charges (from Gasteiger calculation)\n",
    "        gasteiger_charge = 0.0\n",
    "        if atom.HasProp('_GasteigerCharge'):\n",
    "            try:\n",
    "                gasteiger_charge = float(atom.GetProp('_GasteigerCharge'))\n",
    "            except ValueError:\n",
    "                pass # Handle potential 'nan' or non-float values gracefully\n",
    "        features.append(gasteiger_charge)\n",
    "\n",
    "        # Add to the list of all atom features for this molecule\n",
    "        atom_features.append(features)\n",
    "        \n",
    "    # Convert the list of lists to a PyTorch tensor\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "    # Edge Index (edge_index): Bond connectivity\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i]) # Add reverse edge for undirected graph\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Handle molecules with no bonds (single atom, e.g., for [Ne])\n",
    "    if edge_index.numel() == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long) # Create an empty edge_index tensor\n",
    "\n",
    "    # Graph-level Target (y): pGI50\n",
    "    y = torch.tensor([pgi50_value], dtype=torch.float)\n",
    "\n",
    "    # 4. Global Features (global_features)\n",
    "    try:\n",
    "        global_features_vector = global_features_vector.astype(float)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting global_features_vector to float for molregno {molregno}: {e}\")\n",
    "        \n",
    "    global_features_tensor = torch.tensor(global_features_vector, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Create the PyTorch Geometric Data object\n",
    "    data = Data(x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                global_features=global_features_tensor,\n",
    "                molregno=molregno,\n",
    "                smiles=smiles_string)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b691-faa4-494d-b48b-49685b04d746",
   "metadata": {},
   "source": [
    "### Apply Helper Function on Data to Create Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c56a6f9-50ad-4596-8935-38c2af5e1a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating PyG Data objects for Training Set ---\n",
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of X_train_global_features: <class 'pandas.core.frame.DataFrame'>\n",
      "Length of train_df after concatenation: 13119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5877529cbb9425bbf9c93953bdf1ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Train Molecules:   0%|          | 0/13119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 13119 / 13119 graph objects for the training set.\n",
      "Total training graphs: 13119\n",
      "\n",
      "--- Creating PyG Data objects for Validation Set ---\n",
      "Length of val_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fb6b11cca64aeaa2dbf8ba24e0c4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Validation Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the validation set.\n",
      "Total validation graphs: 2812\n",
      "\n",
      "--- Creating PyG Data objects for Test Set ---\n",
      "Length of test_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8517eba41bb47308aac7a64d6d55d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Test Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the test set.\n",
      "Total test graphs: 2812\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "val_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Process Training Data\n",
    "print(\"\\n--- Creating PyG Data objects for Training Set ---\")\n",
    "\n",
    "print(f\"Type of X_train: {type(X_train)}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")\n",
    "print(f\"Type of X_train_global_features: {type(X_train_global_features)}\")\n",
    "\n",
    "# Ensure X_train, y_train, X_train_global_features have the same index for alignment\n",
    "train_df = pd.concat([X_train.reset_index(drop=True),\n",
    "                      y_train.reset_index(drop=True),\n",
    "                      X_train_global_features.reset_index(drop=True)],\n",
    "                     axis=1)\n",
    "print(f\"Length of train_df after concatenation: {len(train_df)}\")\n",
    "\n",
    "successful_train_graphs = 0\n",
    "for index, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing Train Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    \n",
    "    # Extract global features based on the column names extracted after loading data splits\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    # Convert SMILES to RDKit Mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Create PyG Data object\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0: # Ensure valid mol and has nodes\n",
    "        train_data_list.append(pyg_data)\n",
    "        successful_train_graphs += 1\n",
    "    else:\n",
    "        print(f\"Warning: Could not process SMILES: {smiles} (Molregno: {molregno})\")\n",
    "\n",
    "print(f\"Successfully created {successful_train_graphs} / {len(train_df)} graph objects for the training set.\")\n",
    "print(f\"Total training graphs: {len(train_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Validation Data\n",
    "print(\"\\n--- Creating PyG Data objects for Validation Set ---\")\n",
    "val_df = pd.concat([X_val.reset_index(drop=True),\n",
    "                    y_val.reset_index(drop=True),\n",
    "                    X_val_global_features.reset_index(drop=True)],\n",
    "                   axis=1)\n",
    "print(f\"Length of val_df after concatenation: {len(val_df)}\")\n",
    "\n",
    "successful_val_graphs = 0\n",
    "for index, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing Validation Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        val_data_list.append(pyg_data)\n",
    "        successful_val_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_val_graphs} / {len(val_df)} graph objects for the validation set.\")\n",
    "print(f\"Total validation graphs: {len(val_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Test Data\n",
    "print(\"\\n--- Creating PyG Data objects for Test Set ---\")\n",
    "test_df = pd.concat([X_test.reset_index(drop=True),\n",
    "                     y_test.reset_index(drop=True),\n",
    "                     X_test_global_features.reset_index(drop=True)],\n",
    "                    axis=1)\n",
    "print(f\"Length of test_df after concatenation: {len(test_df)}\")\n",
    "\n",
    "successful_test_graphs = 0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        test_data_list.append(pyg_data)\n",
    "        successful_test_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_test_graphs} / {len(test_df)} graph objects for the test set.\")\n",
    "print(f\"Total test graphs: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835fa21-03a3-42ff-9df3-4d33a258ea09",
   "metadata": {},
   "source": [
    "#### Verify Creation of Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9ff332-5b18-4665-8b6c-dcd4948bb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [6.0, 6.0, 6.033141613006592, 6.033141613006592, 6.033141613006592]\n",
      "    Global features (data.global_features) min: -3.1400\n",
      "    Global features (data.global_features) max: 1072410.2500\n",
      "    Global features (data.global_features) mean: 474.7916\n",
      "    Global features (data.global_features) std: 22525.9316\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2ba18-2eac-40de-abed-6e864d14f225",
   "metadata": {},
   "source": [
    "### Standardize Global Features of Each Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4617613f-0ddd-4ca9-b99b-005acf0e4878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global features in torch_geometric.data.Data objects have been scaled!\n"
     ]
    }
   ],
   "source": [
    "# Collect all global features to fit the scaler\n",
    "# Concatenate all global_features tensors. Each data.global_features is already (1, feature_dim),\n",
    "# so torch.cat(..., dim=0) will result in (num_total_graphs, feature_dim).\n",
    "list_of_global_features_tensors = [data.global_features for data in train_data_list + val_data_list]\n",
    "all_global_features_combined = torch.cat(list_of_global_features_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "# Initialize and fit the scaler on the combined global features from training and validation sets\n",
    "global_feature_scaler = StandardScaler()\n",
    "global_feature_scaler.fit(all_global_features_combined)\n",
    "\n",
    "# Apply scaling to the 'global_features' in Data objects for all splits\n",
    "for data_list in [train_data_list, val_data_list, test_data_list]:\n",
    "    for data in data_list:\n",
    "        # Ensure it's numpy before scaling, then back to torch\n",
    "        original_global_features_np = data.global_features.cpu().numpy()\n",
    "        scaled_global_features_np = global_feature_scaler.transform(original_global_features_np)\n",
    "        # Put it back on the correct device\n",
    "        data.global_features = torch.tensor(scaled_global_features_np, dtype=torch.float32).to(data.global_features.device)\n",
    "\n",
    "print(\"\\nGlobal features in torch_geometric.data.Data objects have been scaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c07769-10ea-426a-810d-8cb48c2b0c03",
   "metadata": {},
   "source": [
    "#### Verify Scaling of Global Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42567868-2a41-42be-af81-96be63072dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd7f3f-57a4-47d2-85df-3485e21df7a2",
   "metadata": {},
   "source": [
    "### Save Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af3d05e3-8fe7-4ec2-a05c-71d6dc567cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed graph data saved to: ..\\data\\splits\\pyg_data_graphs\n",
      "Train data list size: 13119\n",
      "Validation data list size: 2812\n",
      "Test data list size: 2812\n"
     ]
    }
   ],
   "source": [
    "# Directory for saving the processed graph data\n",
    "save_dir = Path('../data/splits/pyg_data_graphs')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = save_dir / 'train_data_list.pt'\n",
    "val_data_path = save_dir / 'val_data_list.pt'\n",
    "test_data_path = save_dir / 'test_data_list.pt'\n",
    "\n",
    "# Save the lists of Data objects\n",
    "torch.save(train_data_list, train_data_path)\n",
    "torch.save(val_data_list, val_data_path)\n",
    "torch.save(test_data_list, test_data_path)\n",
    "\n",
    "print(f\"Processed graph data saved to: {save_dir}\")\n",
    "print(f\"Train data list size: {len(train_data_list)}\")\n",
    "print(f\"Validation data list size: {len(val_data_list)}\")\n",
    "print(f\"Test data list size: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046262d-755c-4d54-86e8-33b883e70dce",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189570-12e8-4380-8cd2-ccd3c52985f8",
   "metadata": {},
   "source": [
    "## Load Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6919bfb2-bb97-4f6a-995f-25842c26ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13119 training graphs.\n",
      "Loaded 2812 validation graphs.\n",
      "Loaded 2812 test graphs.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the graph objects are saved\n",
    "load_dir = Path('../data/splits/pyg_data_graphs')\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = load_dir / 'train_data_list.pt'\n",
    "val_data_path = load_dir / 'val_data_list.pt'\n",
    "test_data_path = load_dir / 'test_data_list.pt'\n",
    "\n",
    "# Load the lists of Data objects\n",
    "try:\n",
    "    train_data_list = torch.load(train_data_path, weights_only=False)\n",
    "    val_data_list = torch.load(val_data_path, weights_only=False)\n",
    "    test_data_list = torch.load(test_data_path, weights_only=False)\n",
    "\n",
    "    print(f\"Loaded {len(train_data_list)} training graphs.\")\n",
    "    print(f\"Loaded {len(val_data_list)} validation graphs.\")\n",
    "    print(f\"Loaded {len(test_data_list)} test graphs.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data not found in {load_dir}. Please run the data processing and saving step first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25775b7b-f6bf-487c-bd1d-afa380af03c7",
   "metadata": {},
   "source": [
    "### Verify Loading of Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62061207-247d-4bd6-a197-daaa1a6fb4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e15dce-d1ff-42de-bc1c-ce38b7cf630a",
   "metadata": {},
   "source": [
    "## Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52b2f3-fb35-4d0d-b181-c7af49150abd",
   "metadata": {},
   "source": [
    "### Define Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30682f4-705d-48b2-8da1-24a8afa2325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 128, 1024, log=True) # Number of neurons in hidden layer\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256]) # Batch size for DataLoaders\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 150, 600)  # Number of training epochs\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4) # Number of GNN layers\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5) # Dropout rate\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-3, log=True)\n",
    "\n",
    "    # Determine feature dimensions dynamically from loaded/created graph objects\n",
    "    # Ensure train_data_list is not empty before accessing its first element\n",
    "    if not train_data_list:\n",
    "        raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions.\")\n",
    "\n",
    "    # node_feature_dim: Number of features per atom\n",
    "    # global_feature_dim: Number of global features per molecule\n",
    "    node_feature_dim = train_data_list[0].x.shape[1]\n",
    "    global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        node_feature_dim=node_feature_dim,\n",
    "        global_feature_dim=global_feature_dim,\n",
    "        hidden_channels=hidden_channels,  # From Optuna trial\n",
    "        num_layers=num_layers  # From Optuna trial\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss function and Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # PyTorch Geometric DataLoaders\n",
    "    num_workers = 0\n",
    "    train_loader = PyGDataLoader(train_data_list, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = PyGDataLoader(val_data_list, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        start_epoch_time = time.time()\n",
    "        for batch_idx, data_batch in enumerate(train_loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch)\n",
    "                \n",
    "            # Ensure outputs and target are same shape for loss calculation\n",
    "            loss = criterion(outputs.view(-1), data_batch.y.view(-1)) # .view(-1) flattens to ensure shape compatibility\n",
    "\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in model outputs at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in loss at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"!!! CRITICAL: NaN/Inf in gradient of {name} - Epoch {epoch+1}, Batch {batch_idx+1}\")\n",
    "                        # Add a break here for deeper inspection if this happens\n",
    "                        # import sys; sys.exit(\"Gradient instability detected.\")\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad(): # Disable gradient calculations for validation\n",
    "            for data_batch in val_loader:\n",
    "                data_batch = data_batch.to(device)\n",
    "                val_outputs = model(data_batch)\n",
    "                val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(data_batch.y.cpu().numpy().flatten()) # Extract y from PyG Data object\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "        if device.type == 'cuda': # Ensure GPU operations are finished before timing an epoch\n",
    "            torch.cuda.synchronize()\n",
    "        end_epoch_time = time.time()\n",
    "\n",
    "        print(f\"Trial {trial.number}, Epoch {epoch+1}/{n_epochs}, Val RMSE: {val_rmse:.4f}, Time: {end_epoch_time - start_epoch_time:.2f}s\")\n",
    "\n",
    "        # Optuna Pruning: Report current validation RMSE to Optuna\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Manual Early Stopping Check\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_counter = 0 # Reset patience if improvement is found\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}\")\n",
    "                break # Exit training loop for current trial\n",
    "\n",
    "    # Final evaluation on validation set after training (or early stopping)\n",
    "    model.eval()\n",
    "    final_val_predictions = []\n",
    "    final_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data_batch in val_loader:\n",
    "            data_batch = data_batch.to(device)\n",
    "            val_outputs = model(data_batch)\n",
    "            final_val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            final_val_targets.extend(data_batch.y.cpu().numpy().flatten())\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(final_val_targets, final_val_predictions))\n",
    "    final_r2 = r2_score(final_val_targets, final_val_predictions)\n",
    "\n",
    "    # Store R2 score as well in the study for later analysis\n",
    "    trial.set_user_attr(\"final_r2_score\", float(final_r2))\n",
    "\n",
    "    return final_rmse # Optuna minimizes this value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4cec-523b-4256-9831-6625f53855f6",
   "metadata": {},
   "source": [
    "### Run Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a66f01e-b7b3-4e2f-8704-f92e31c2d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for GNN will be stored at: sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db\n",
      "Loaded existing study 'gnn_regression_pGI50' from sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db. Resuming optimization.\n",
      "\n",
      "Starting Optuna optimization for GNN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab1d8a5d2b146c59fc74185701bc990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40, Epoch 1/415, Val RMSE: 2.7803, Time: 5.50s\n",
      "Trial 40, Epoch 2/415, Val RMSE: 0.8786, Time: 5.05s\n",
      "Trial 40, Epoch 3/415, Val RMSE: 0.7775, Time: 5.01s\n",
      "Trial 40, Epoch 4/415, Val RMSE: 0.7488, Time: 5.02s\n",
      "Trial 40, Epoch 5/415, Val RMSE: 0.7310, Time: 4.96s\n",
      "Trial 40, Epoch 6/415, Val RMSE: 0.7187, Time: 5.01s\n",
      "Trial 40, Epoch 7/415, Val RMSE: 0.7086, Time: 4.95s\n",
      "Trial 40, Epoch 8/415, Val RMSE: 0.7002, Time: 4.99s\n",
      "Trial 40, Epoch 9/415, Val RMSE: 0.6919, Time: 5.04s\n",
      "Trial 40, Epoch 10/415, Val RMSE: 0.6826, Time: 5.03s\n",
      "Trial 40, Epoch 11/415, Val RMSE: 0.6813, Time: 5.10s\n",
      "Trial 40, Epoch 12/415, Val RMSE: 0.6720, Time: 5.03s\n",
      "Trial 40, Epoch 13/415, Val RMSE: 0.6707, Time: 5.04s\n",
      "Trial 40, Epoch 14/415, Val RMSE: 0.6635, Time: 5.04s\n",
      "Trial 40, Epoch 15/415, Val RMSE: 0.6597, Time: 5.14s\n",
      "Trial 40, Epoch 16/415, Val RMSE: 0.6562, Time: 5.11s\n",
      "Trial 40, Epoch 17/415, Val RMSE: 0.6596, Time: 5.13s\n",
      "Trial 40, Epoch 18/415, Val RMSE: 0.6527, Time: 5.05s\n",
      "Trial 40, Epoch 19/415, Val RMSE: 0.6542, Time: 5.06s\n",
      "Trial 40, Epoch 20/415, Val RMSE: 0.6505, Time: 5.00s\n",
      "Trial 40, Epoch 21/415, Val RMSE: 0.6506, Time: 4.95s\n",
      "Trial 40, Epoch 22/415, Val RMSE: 0.6494, Time: 4.92s\n",
      "Trial 40, Epoch 23/415, Val RMSE: 0.6479, Time: 5.21s\n",
      "Trial 40, Epoch 24/415, Val RMSE: 0.6476, Time: 5.18s\n",
      "Trial 40, Epoch 25/415, Val RMSE: 0.6521, Time: 5.04s\n",
      "Trial 40, Epoch 26/415, Val RMSE: 0.6425, Time: 5.01s\n",
      "Trial 40, Epoch 27/415, Val RMSE: 0.6513, Time: 4.96s\n",
      "Trial 40, Epoch 28/415, Val RMSE: 0.6512, Time: 4.92s\n",
      "Trial 40, Epoch 29/415, Val RMSE: 0.6479, Time: 5.03s\n",
      "Trial 40, Epoch 30/415, Val RMSE: 0.6441, Time: 4.97s\n",
      "Trial 40, Epoch 31/415, Val RMSE: 0.6496, Time: 4.98s\n",
      "Trial 40, Epoch 32/415, Val RMSE: 0.6550, Time: 4.98s\n",
      "Trial 40, Epoch 33/415, Val RMSE: 0.6489, Time: 5.08s\n",
      "Trial 40, Epoch 34/415, Val RMSE: 0.6521, Time: 5.02s\n",
      "Trial 40, Epoch 35/415, Val RMSE: 0.6516, Time: 5.02s\n",
      "Trial 40, Epoch 36/415, Val RMSE: 0.6479, Time: 5.21s\n",
      "Trial 40, Epoch 37/415, Val RMSE: 0.6556, Time: 5.18s\n",
      "Trial 40, Epoch 38/415, Val RMSE: 0.6559, Time: 5.10s\n",
      "Trial 40, Epoch 39/415, Val RMSE: 0.6577, Time: 4.99s\n",
      "Trial 40, Epoch 40/415, Val RMSE: 0.6646, Time: 4.97s\n",
      "Trial 40, Epoch 41/415, Val RMSE: 0.6514, Time: 5.01s\n",
      "Trial 40, Epoch 42/415, Val RMSE: 0.6523, Time: 4.98s\n",
      "Trial 40, Epoch 43/415, Val RMSE: 0.6597, Time: 4.99s\n",
      "Trial 40, Epoch 44/415, Val RMSE: 0.6527, Time: 4.99s\n",
      "Trial 40, Epoch 45/415, Val RMSE: 0.6529, Time: 5.26s\n",
      "Trial 40, Epoch 46/415, Val RMSE: 0.6577, Time: 5.17s\n",
      "Trial 40, Epoch 47/415, Val RMSE: 0.6580, Time: 5.08s\n",
      "Trial 40, Epoch 48/415, Val RMSE: 0.6525, Time: 4.99s\n",
      "Trial 40, Epoch 49/415, Val RMSE: 0.6575, Time: 4.98s\n",
      "Trial 40, Epoch 50/415, Val RMSE: 0.6579, Time: 5.06s\n",
      "Trial 40, Epoch 51/415, Val RMSE: 0.6579, Time: 4.89s\n",
      "Trial 40, Epoch 52/415, Val RMSE: 0.6614, Time: 5.00s\n",
      "Trial 40, Epoch 53/415, Val RMSE: 0.6576, Time: 4.98s\n",
      "Trial 40, Epoch 54/415, Val RMSE: 0.6585, Time: 4.98s\n",
      "Trial 40, Epoch 55/415, Val RMSE: 0.6625, Time: 4.93s\n",
      "Trial 40, Epoch 56/415, Val RMSE: 0.6581, Time: 4.97s\n",
      "Trial 40, Epoch 57/415, Val RMSE: 0.6597, Time: 5.00s\n",
      "Trial 40, Epoch 58/415, Val RMSE: 0.6612, Time: 4.94s\n",
      "Trial 40, Epoch 59/415, Val RMSE: 0.6591, Time: 5.00s\n",
      "Trial 40, Epoch 60/415, Val RMSE: 0.6617, Time: 5.04s\n",
      "Trial 40, Epoch 61/415, Val RMSE: 0.6586, Time: 4.97s\n",
      "Trial 40, Epoch 62/415, Val RMSE: 0.6624, Time: 5.01s\n",
      "Trial 40, Epoch 63/415, Val RMSE: 0.6669, Time: 4.99s\n",
      "Trial 40, Epoch 64/415, Val RMSE: 0.6657, Time: 4.97s\n",
      "Trial 40, Epoch 65/415, Val RMSE: 0.6627, Time: 4.99s\n",
      "Trial 40, Epoch 66/415, Val RMSE: 0.6663, Time: 4.98s\n",
      "Trial 40, Epoch 67/415, Val RMSE: 0.6656, Time: 5.02s\n",
      "Trial 40, Epoch 68/415, Val RMSE: 0.6641, Time: 4.96s\n",
      "Trial 40, Epoch 69/415, Val RMSE: 0.6662, Time: 4.96s\n",
      "Trial 40, Epoch 70/415, Val RMSE: 0.6636, Time: 5.00s\n",
      "Trial 40, Epoch 71/415, Val RMSE: 0.6664, Time: 5.03s\n",
      "Trial 40, Epoch 72/415, Val RMSE: 0.6663, Time: 4.97s\n",
      "Trial 40, Epoch 73/415, Val RMSE: 0.6663, Time: 5.12s\n",
      "Trial 40, Epoch 74/415, Val RMSE: 0.6684, Time: 4.91s\n",
      "Trial 40, Epoch 75/415, Val RMSE: 0.6675, Time: 4.96s\n",
      "Trial 40, Epoch 76/415, Val RMSE: 0.6633, Time: 4.97s\n",
      "Early stopping at epoch 76 for trial 40\n",
      "[I 2025-07-16 21:06:37,802] Trial 40 finished with value: 0.663321068578847 and parameters: {'hidden_channels': 283, 'lr': 3.925937131209865e-05, 'batch_size': 64, 'n_epochs': 415, 'num_layers': 1, 'dropout_rate': 0.4262027911580313, 'weight_decay': 7.773739094959312e-07}. Best is trial 40 with value: 0.663321068578847.\n",
      "Trial 41, Epoch 1/346, Val RMSE: 1.6094, Time: 5.01s\n",
      "Trial 41, Epoch 2/346, Val RMSE: 0.7797, Time: 4.99s\n",
      "Trial 41, Epoch 3/346, Val RMSE: 0.7378, Time: 4.93s\n",
      "Trial 41, Epoch 4/346, Val RMSE: 0.7212, Time: 5.00s\n",
      "Trial 41, Epoch 5/346, Val RMSE: 0.7143, Time: 4.98s\n",
      "Trial 41, Epoch 6/346, Val RMSE: 0.6976, Time: 4.96s\n",
      "Trial 41, Epoch 7/346, Val RMSE: 0.6905, Time: 4.99s\n",
      "Trial 41, Epoch 8/346, Val RMSE: 0.6798, Time: 5.09s\n",
      "Trial 41, Epoch 9/346, Val RMSE: 0.6722, Time: 5.02s\n",
      "Trial 41, Epoch 10/346, Val RMSE: 0.6721, Time: 5.09s\n",
      "Trial 41, Epoch 11/346, Val RMSE: 0.6579, Time: 4.98s\n",
      "Trial 41, Epoch 12/346, Val RMSE: 0.6650, Time: 4.98s\n",
      "Trial 41, Epoch 13/346, Val RMSE: 0.6620, Time: 4.96s\n",
      "Trial 41, Epoch 14/346, Val RMSE: 0.6584, Time: 5.01s\n",
      "Trial 41, Epoch 15/346, Val RMSE: 0.6520, Time: 5.01s\n",
      "Trial 41, Epoch 16/346, Val RMSE: 0.6529, Time: 4.97s\n",
      "Trial 41, Epoch 17/346, Val RMSE: 0.6490, Time: 4.98s\n",
      "Trial 41, Epoch 18/346, Val RMSE: 0.6468, Time: 4.90s\n",
      "Trial 41, Epoch 19/346, Val RMSE: 0.6515, Time: 4.96s\n",
      "Trial 41, Epoch 20/346, Val RMSE: 0.6523, Time: 5.00s\n",
      "Trial 41, Epoch 21/346, Val RMSE: 0.6523, Time: 4.92s\n",
      "Trial 41, Epoch 22/346, Val RMSE: 0.6552, Time: 4.91s\n",
      "Trial 41, Epoch 23/346, Val RMSE: 0.6515, Time: 4.99s\n",
      "Trial 41, Epoch 24/346, Val RMSE: 0.6546, Time: 4.95s\n",
      "Trial 41, Epoch 25/346, Val RMSE: 0.6568, Time: 4.95s\n",
      "Trial 41, Epoch 26/346, Val RMSE: 0.6506, Time: 5.01s\n",
      "Trial 41, Epoch 27/346, Val RMSE: 0.6545, Time: 5.05s\n",
      "Trial 41, Epoch 28/346, Val RMSE: 0.6461, Time: 5.08s\n",
      "Trial 41, Epoch 29/346, Val RMSE: 0.6541, Time: 5.05s\n",
      "Trial 41, Epoch 30/346, Val RMSE: 0.6575, Time: 4.97s\n",
      "Trial 41, Epoch 31/346, Val RMSE: 0.6493, Time: 5.05s\n",
      "Trial 41, Epoch 32/346, Val RMSE: 0.6499, Time: 5.01s\n",
      "Trial 41, Epoch 33/346, Val RMSE: 0.6607, Time: 5.25s\n",
      "Trial 41, Epoch 34/346, Val RMSE: 0.6619, Time: 4.98s\n",
      "Trial 41, Epoch 35/346, Val RMSE: 0.6603, Time: 4.97s\n",
      "Trial 41, Epoch 36/346, Val RMSE: 0.6535, Time: 4.99s\n",
      "Trial 41, Epoch 37/346, Val RMSE: 0.6531, Time: 5.10s\n",
      "Trial 41, Epoch 38/346, Val RMSE: 0.6597, Time: 5.02s\n",
      "Trial 41, Epoch 39/346, Val RMSE: 0.6595, Time: 5.01s\n",
      "Trial 41, Epoch 40/346, Val RMSE: 0.6597, Time: 4.97s\n",
      "Trial 41, Epoch 41/346, Val RMSE: 0.6591, Time: 4.94s\n",
      "Trial 41, Epoch 42/346, Val RMSE: 0.6571, Time: 5.00s\n",
      "Trial 41, Epoch 43/346, Val RMSE: 0.6510, Time: 4.93s\n",
      "Trial 41, Epoch 44/346, Val RMSE: 0.6569, Time: 5.03s\n",
      "Trial 41, Epoch 45/346, Val RMSE: 0.6550, Time: 5.01s\n",
      "Trial 41, Epoch 46/346, Val RMSE: 0.6558, Time: 4.93s\n",
      "Trial 41, Epoch 47/346, Val RMSE: 0.6585, Time: 4.93s\n",
      "Trial 41, Epoch 48/346, Val RMSE: 0.6565, Time: 4.95s\n",
      "Trial 41, Epoch 49/346, Val RMSE: 0.6579, Time: 4.99s\n",
      "Trial 41, Epoch 50/346, Val RMSE: 0.6637, Time: 5.35s\n",
      "Trial 41, Epoch 51/346, Val RMSE: 0.6607, Time: 5.06s\n",
      "Trial 41, Epoch 52/346, Val RMSE: 0.6633, Time: 5.06s\n",
      "Trial 41, Epoch 53/346, Val RMSE: 0.6660, Time: 5.04s\n",
      "Trial 41, Epoch 54/346, Val RMSE: 0.6591, Time: 5.01s\n",
      "Trial 41, Epoch 55/346, Val RMSE: 0.6587, Time: 5.09s\n",
      "Trial 41, Epoch 56/346, Val RMSE: 0.6631, Time: 5.32s\n",
      "Trial 41, Epoch 57/346, Val RMSE: 0.6608, Time: 5.09s\n",
      "Trial 41, Epoch 58/346, Val RMSE: 0.6681, Time: 5.00s\n",
      "Trial 41, Epoch 59/346, Val RMSE: 0.6643, Time: 5.00s\n",
      "Trial 41, Epoch 60/346, Val RMSE: 0.6663, Time: 5.07s\n",
      "Trial 41, Epoch 61/346, Val RMSE: 0.6617, Time: 5.02s\n",
      "Trial 41, Epoch 62/346, Val RMSE: 0.6633, Time: 5.00s\n",
      "Trial 41, Epoch 63/346, Val RMSE: 0.6681, Time: 4.99s\n",
      "Trial 41, Epoch 64/346, Val RMSE: 0.6611, Time: 5.00s\n",
      "Trial 41, Epoch 65/346, Val RMSE: 0.6658, Time: 4.93s\n",
      "Trial 41, Epoch 66/346, Val RMSE: 0.6691, Time: 4.89s\n",
      "Trial 41, Epoch 67/346, Val RMSE: 0.6649, Time: 4.99s\n",
      "Trial 41, Epoch 68/346, Val RMSE: 0.6631, Time: 5.06s\n",
      "Trial 41, Epoch 69/346, Val RMSE: 0.6611, Time: 5.08s\n",
      "Trial 41, Epoch 70/346, Val RMSE: 0.6661, Time: 5.06s\n",
      "Trial 41, Epoch 71/346, Val RMSE: 0.6685, Time: 4.98s\n",
      "Trial 41, Epoch 72/346, Val RMSE: 0.6644, Time: 4.98s\n",
      "Trial 41, Epoch 73/346, Val RMSE: 0.6587, Time: 4.96s\n",
      "Trial 41, Epoch 74/346, Val RMSE: 0.6654, Time: 4.95s\n",
      "Trial 41, Epoch 75/346, Val RMSE: 0.6680, Time: 4.98s\n",
      "Trial 41, Epoch 76/346, Val RMSE: 0.6642, Time: 5.01s\n",
      "Trial 41, Epoch 77/346, Val RMSE: 0.6653, Time: 5.00s\n",
      "Trial 41, Epoch 78/346, Val RMSE: 0.6664, Time: 5.14s\n",
      "Early stopping at epoch 78 for trial 41\n",
      "[I 2025-07-16 21:13:11,001] Trial 41 finished with value: 0.6663774990493848 and parameters: {'hidden_channels': 276, 'lr': 5.629256583436667e-05, 'batch_size': 64, 'n_epochs': 346, 'num_layers': 1, 'dropout_rate': 0.43896919624205566, 'weight_decay': 2.6614271385704733e-06}. Best is trial 40 with value: 0.663321068578847.\n",
      "Trial 42, Epoch 1/346, Val RMSE: 1.3389, Time: 5.14s\n",
      "Trial 42, Epoch 2/346, Val RMSE: 0.7976, Time: 5.05s\n",
      "Trial 42, Epoch 3/346, Val RMSE: 0.7463, Time: 5.01s\n",
      "Trial 42, Epoch 4/346, Val RMSE: 0.7155, Time: 4.95s\n",
      "Trial 42, Epoch 5/346, Val RMSE: 0.7110, Time: 5.01s\n",
      "Trial 42, Epoch 6/346, Val RMSE: 0.7055, Time: 5.03s\n",
      "Trial 42, Epoch 7/346, Val RMSE: 0.6889, Time: 4.95s\n",
      "Trial 42, Epoch 8/346, Val RMSE: 0.6766, Time: 5.02s\n",
      "Trial 42, Epoch 9/346, Val RMSE: 0.6711, Time: 4.96s\n",
      "Trial 42, Epoch 10/346, Val RMSE: 0.6685, Time: 4.94s\n",
      "Trial 42, Epoch 11/346, Val RMSE: 0.6684, Time: 4.97s\n",
      "Trial 42, Epoch 12/346, Val RMSE: 0.6665, Time: 5.01s\n",
      "Trial 42, Epoch 13/346, Val RMSE: 0.6605, Time: 4.95s\n",
      "Trial 42, Epoch 14/346, Val RMSE: 0.6608, Time: 4.97s\n",
      "Trial 42, Epoch 15/346, Val RMSE: 0.6585, Time: 5.06s\n",
      "Trial 42, Epoch 16/346, Val RMSE: 0.6581, Time: 5.15s\n",
      "Trial 42, Epoch 17/346, Val RMSE: 0.6569, Time: 5.05s\n",
      "Trial 42, Epoch 18/346, Val RMSE: 0.6566, Time: 5.01s\n",
      "Trial 42, Epoch 19/346, Val RMSE: 0.6611, Time: 5.03s\n",
      "Trial 42, Epoch 20/346, Val RMSE: 0.6646, Time: 5.08s\n",
      "Trial 42, Epoch 21/346, Val RMSE: 0.6597, Time: 5.00s\n",
      "Trial 42, Epoch 22/346, Val RMSE: 0.6508, Time: 5.01s\n",
      "Trial 42, Epoch 23/346, Val RMSE: 0.6557, Time: 4.95s\n",
      "Trial 42, Epoch 24/346, Val RMSE: 0.6561, Time: 5.01s\n",
      "Trial 42, Epoch 25/346, Val RMSE: 0.6660, Time: 5.01s\n",
      "Trial 42, Epoch 26/346, Val RMSE: 0.6564, Time: 5.03s\n",
      "Trial 42, Epoch 27/346, Val RMSE: 0.6525, Time: 5.04s\n",
      "Trial 42, Epoch 28/346, Val RMSE: 0.6597, Time: 4.96s\n",
      "Trial 42, Epoch 29/346, Val RMSE: 0.6559, Time: 5.00s\n",
      "Trial 42, Epoch 30/346, Val RMSE: 0.6597, Time: 4.96s\n",
      "Trial 42, Epoch 31/346, Val RMSE: 0.6634, Time: 4.98s\n",
      "Trial 42, Epoch 32/346, Val RMSE: 0.6607, Time: 5.04s\n",
      "Trial 42, Epoch 33/346, Val RMSE: 0.6621, Time: 4.97s\n",
      "Trial 42, Epoch 34/346, Val RMSE: 0.6616, Time: 4.88s\n",
      "Trial 42, Epoch 35/346, Val RMSE: 0.6633, Time: 4.93s\n",
      "Trial 42, Epoch 36/346, Val RMSE: 0.6612, Time: 4.93s\n",
      "Trial 42, Epoch 37/346, Val RMSE: 0.6612, Time: 5.06s\n",
      "Trial 42, Epoch 38/346, Val RMSE: 0.6633, Time: 5.13s\n",
      "Trial 42, Epoch 39/346, Val RMSE: 0.6638, Time: 4.91s\n",
      "Trial 42, Epoch 40/346, Val RMSE: 0.6671, Time: 5.03s\n",
      "Trial 42, Epoch 41/346, Val RMSE: 0.6667, Time: 5.01s\n",
      "Trial 42, Epoch 42/346, Val RMSE: 0.6687, Time: 5.55s\n",
      "Trial 42, Epoch 43/346, Val RMSE: 0.6673, Time: 5.14s\n",
      "Trial 42, Epoch 44/346, Val RMSE: 0.6631, Time: 5.04s\n",
      "Trial 42, Epoch 45/346, Val RMSE: 0.6693, Time: 4.99s\n",
      "Trial 42, Epoch 46/346, Val RMSE: 0.6657, Time: 5.05s\n",
      "Trial 42, Epoch 47/346, Val RMSE: 0.6644, Time: 4.93s\n",
      "Trial 42, Epoch 48/346, Val RMSE: 0.6612, Time: 5.19s\n",
      "Trial 42, Epoch 49/346, Val RMSE: 0.6670, Time: 5.11s\n",
      "Trial 42, Epoch 50/346, Val RMSE: 0.6646, Time: 5.13s\n",
      "Trial 42, Epoch 51/346, Val RMSE: 0.6681, Time: 5.11s\n",
      "Trial 42, Epoch 52/346, Val RMSE: 0.6626, Time: 5.18s\n",
      "Trial 42, Epoch 53/346, Val RMSE: 0.6610, Time: 5.14s\n",
      "Trial 42, Epoch 54/346, Val RMSE: 0.6697, Time: 5.41s\n",
      "Trial 42, Epoch 55/346, Val RMSE: 0.6716, Time: 5.72s\n",
      "Trial 42, Epoch 56/346, Val RMSE: 0.6710, Time: 5.78s\n",
      "Trial 42, Epoch 57/346, Val RMSE: 0.6671, Time: 5.08s\n",
      "Trial 42, Epoch 58/346, Val RMSE: 0.6683, Time: 5.07s\n",
      "Trial 42, Epoch 59/346, Val RMSE: 0.6692, Time: 5.13s\n",
      "Trial 42, Epoch 60/346, Val RMSE: 0.6658, Time: 5.03s\n",
      "Trial 42, Epoch 61/346, Val RMSE: 0.6694, Time: 5.00s\n",
      "Trial 42, Epoch 62/346, Val RMSE: 0.6711, Time: 4.95s\n",
      "Trial 42, Epoch 63/346, Val RMSE: 0.6687, Time: 4.93s\n",
      "Trial 42, Epoch 64/346, Val RMSE: 0.6701, Time: 4.95s\n",
      "Trial 42, Epoch 65/346, Val RMSE: 0.6735, Time: 5.02s\n",
      "Trial 42, Epoch 66/346, Val RMSE: 0.6735, Time: 4.96s\n",
      "Trial 42, Epoch 67/346, Val RMSE: 0.6811, Time: 5.06s\n",
      "Trial 42, Epoch 68/346, Val RMSE: 0.6658, Time: 4.96s\n",
      "Trial 42, Epoch 69/346, Val RMSE: 0.6666, Time: 5.03s\n",
      "Trial 42, Epoch 70/346, Val RMSE: 0.6691, Time: 5.01s\n",
      "Trial 42, Epoch 71/346, Val RMSE: 0.6709, Time: 5.00s\n",
      "Trial 42, Epoch 72/346, Val RMSE: 0.6696, Time: 4.99s\n",
      "Early stopping at epoch 72 for trial 42\n",
      "[I 2025-07-16 21:19:17,315] Trial 42 finished with value: 0.6695730266538048 and parameters: {'hidden_channels': 284, 'lr': 5.714733666051264e-05, 'batch_size': 64, 'n_epochs': 346, 'num_layers': 1, 'dropout_rate': 0.445668599475133, 'weight_decay': 3.7390021902981787e-06}. Best is trial 40 with value: 0.663321068578847.\n",
      "Trial 43, Epoch 1/356, Val RMSE: 1.3122, Time: 5.03s\n",
      "Trial 43, Epoch 2/356, Val RMSE: 0.7784, Time: 5.02s\n",
      "Trial 43, Epoch 3/356, Val RMSE: 0.7387, Time: 5.00s\n",
      "Trial 43, Epoch 4/356, Val RMSE: 0.7215, Time: 5.02s\n",
      "Trial 43, Epoch 5/356, Val RMSE: 0.7029, Time: 5.07s\n",
      "Trial 43, Epoch 6/356, Val RMSE: 0.7026, Time: 5.03s\n",
      "Trial 43, Epoch 7/356, Val RMSE: 0.6884, Time: 4.97s\n",
      "Trial 43, Epoch 8/356, Val RMSE: 0.6814, Time: 5.01s\n",
      "Trial 43, Epoch 9/356, Val RMSE: 0.6722, Time: 4.99s\n",
      "Trial 43, Epoch 10/356, Val RMSE: 0.6677, Time: 4.91s\n",
      "Trial 43, Epoch 11/356, Val RMSE: 0.6712, Time: 4.96s\n",
      "Trial 43, Epoch 12/356, Val RMSE: 0.6626, Time: 5.04s\n",
      "Trial 43, Epoch 13/356, Val RMSE: 0.6653, Time: 4.97s\n",
      "Trial 43, Epoch 14/356, Val RMSE: 0.6520, Time: 5.04s\n",
      "Trial 43, Epoch 15/356, Val RMSE: 0.6586, Time: 5.05s\n",
      "Trial 43, Epoch 16/356, Val RMSE: 0.6547, Time: 5.08s\n",
      "Trial 43, Epoch 17/356, Val RMSE: 0.6576, Time: 5.14s\n",
      "Trial 43, Epoch 18/356, Val RMSE: 0.6542, Time: 5.09s\n",
      "Trial 43, Epoch 19/356, Val RMSE: 0.6512, Time: 5.08s\n",
      "Trial 43, Epoch 20/356, Val RMSE: 0.6481, Time: 5.22s\n",
      "Trial 43, Epoch 21/356, Val RMSE: 0.6517, Time: 5.00s\n",
      "Trial 43, Epoch 22/356, Val RMSE: 0.6541, Time: 5.01s\n",
      "Trial 43, Epoch 23/356, Val RMSE: 0.6502, Time: 5.02s\n",
      "Trial 43, Epoch 24/356, Val RMSE: 0.6562, Time: 5.19s\n",
      "Trial 43, Epoch 25/356, Val RMSE: 0.6509, Time: 5.06s\n",
      "Trial 43, Epoch 26/356, Val RMSE: 0.6584, Time: 5.02s\n",
      "Trial 43, Epoch 27/356, Val RMSE: 0.6568, Time: 5.08s\n",
      "Trial 43, Epoch 28/356, Val RMSE: 0.6510, Time: 5.07s\n",
      "Trial 43, Epoch 29/356, Val RMSE: 0.6515, Time: 5.27s\n",
      "Trial 43, Epoch 30/356, Val RMSE: 0.6529, Time: 5.24s\n",
      "Trial 43, Epoch 31/356, Val RMSE: 0.6685, Time: 5.11s\n",
      "Trial 43, Epoch 32/356, Val RMSE: 0.6583, Time: 5.01s\n",
      "Trial 43, Epoch 33/356, Val RMSE: 0.6591, Time: 4.99s\n",
      "Trial 43, Epoch 34/356, Val RMSE: 0.6590, Time: 5.01s\n",
      "Trial 43, Epoch 35/356, Val RMSE: 0.6609, Time: 4.94s\n",
      "Trial 43, Epoch 36/356, Val RMSE: 0.6594, Time: 5.06s\n",
      "Trial 43, Epoch 37/356, Val RMSE: 0.6580, Time: 5.01s\n",
      "Trial 43, Epoch 38/356, Val RMSE: 0.6589, Time: 5.05s\n",
      "Trial 43, Epoch 39/356, Val RMSE: 0.6667, Time: 5.02s\n",
      "Trial 43, Epoch 40/356, Val RMSE: 0.6608, Time: 5.00s\n",
      "Trial 43, Epoch 41/356, Val RMSE: 0.6608, Time: 4.94s\n",
      "Trial 43, Epoch 42/356, Val RMSE: 0.6566, Time: 5.03s\n",
      "Trial 43, Epoch 43/356, Val RMSE: 0.6606, Time: 4.99s\n",
      "Trial 43, Epoch 44/356, Val RMSE: 0.6666, Time: 5.06s\n",
      "Trial 43, Epoch 45/356, Val RMSE: 0.6632, Time: 5.06s\n",
      "Trial 43, Epoch 46/356, Val RMSE: 0.6620, Time: 5.04s\n",
      "Trial 43, Epoch 47/356, Val RMSE: 0.6655, Time: 5.17s\n",
      "Trial 43, Epoch 48/356, Val RMSE: 0.6674, Time: 5.03s\n",
      "Trial 43, Epoch 49/356, Val RMSE: 0.6653, Time: 5.02s\n",
      "Trial 43, Epoch 50/356, Val RMSE: 0.6607, Time: 5.03s\n",
      "Trial 43, Epoch 51/356, Val RMSE: 0.6642, Time: 4.99s\n",
      "Trial 43, Epoch 52/356, Val RMSE: 0.6578, Time: 4.98s\n",
      "Trial 43, Epoch 53/356, Val RMSE: 0.6662, Time: 5.04s\n",
      "Trial 43, Epoch 54/356, Val RMSE: 0.6655, Time: 5.03s\n",
      "Trial 43, Epoch 55/356, Val RMSE: 0.6609, Time: 5.00s\n",
      "Trial 43, Epoch 56/356, Val RMSE: 0.6615, Time: 5.03s\n",
      "Trial 43, Epoch 57/356, Val RMSE: 0.6659, Time: 4.91s\n",
      "Trial 43, Epoch 58/356, Val RMSE: 0.6645, Time: 5.00s\n",
      "Trial 43, Epoch 59/356, Val RMSE: 0.6671, Time: 5.00s\n",
      "Trial 43, Epoch 60/356, Val RMSE: 0.6714, Time: 4.99s\n",
      "Trial 43, Epoch 61/356, Val RMSE: 0.6670, Time: 4.96s\n",
      "Trial 43, Epoch 62/356, Val RMSE: 0.6762, Time: 4.99s\n",
      "Trial 43, Epoch 63/356, Val RMSE: 0.6653, Time: 4.99s\n",
      "Trial 43, Epoch 64/356, Val RMSE: 0.6663, Time: 4.94s\n",
      "Trial 43, Epoch 65/356, Val RMSE: 0.6686, Time: 5.03s\n",
      "Trial 43, Epoch 66/356, Val RMSE: 0.6732, Time: 6.46s\n",
      "Trial 43, Epoch 67/356, Val RMSE: 0.6685, Time: 3.21s\n",
      "Trial 43, Epoch 68/356, Val RMSE: 0.6696, Time: 6.09s\n",
      "Trial 43, Epoch 69/356, Val RMSE: 0.6699, Time: 5.54s\n",
      "Trial 43, Epoch 70/356, Val RMSE: 0.6711, Time: 5.93s\n",
      "Early stopping at epoch 70 for trial 43\n",
      "[I 2025-07-16 21:25:14,063] Trial 43 finished with value: 0.6711135047183879 and parameters: {'hidden_channels': 280, 'lr': 5.230703526783511e-05, 'batch_size': 64, 'n_epochs': 356, 'num_layers': 1, 'dropout_rate': 0.4510715452241612, 'weight_decay': 2.4208666775484684e-06}. Best is trial 40 with value: 0.663321068578847.\n",
      "Trial 44, Epoch 1/310, Val RMSE: 1.0344, Time: 5.95s\n",
      "Trial 44, Epoch 2/310, Val RMSE: 0.7681, Time: 6.09s\n",
      "Trial 44, Epoch 3/310, Val RMSE: 0.7326, Time: 5.62s\n",
      "Trial 44, Epoch 4/310, Val RMSE: 0.7133, Time: 6.10s\n",
      "Trial 44, Epoch 5/310, Val RMSE: 0.6994, Time: 5.80s\n",
      "Trial 44, Epoch 6/310, Val RMSE: 0.6865, Time: 5.82s\n",
      "Trial 44, Epoch 7/310, Val RMSE: 0.6791, Time: 5.90s\n",
      "Trial 44, Epoch 8/310, Val RMSE: 0.6787, Time: 6.00s\n",
      "Trial 44, Epoch 9/310, Val RMSE: 0.6637, Time: 5.70s\n",
      "Trial 44, Epoch 10/310, Val RMSE: 0.6629, Time: 5.96s\n",
      "Trial 44, Epoch 11/310, Val RMSE: 0.6515, Time: 6.03s\n",
      "Trial 44, Epoch 12/310, Val RMSE: 0.6538, Time: 5.86s\n",
      "Trial 44, Epoch 13/310, Val RMSE: 0.6593, Time: 5.69s\n",
      "Trial 44, Epoch 14/310, Val RMSE: 0.6495, Time: 5.69s\n",
      "Trial 44, Epoch 15/310, Val RMSE: 0.6491, Time: 5.79s\n",
      "Trial 44, Epoch 16/310, Val RMSE: 0.6559, Time: 6.01s\n",
      "Trial 44, Epoch 17/310, Val RMSE: 0.6521, Time: 5.81s\n",
      "Trial 44, Epoch 18/310, Val RMSE: 0.6532, Time: 5.97s\n",
      "Trial 44, Epoch 19/310, Val RMSE: 0.6477, Time: 6.96s\n",
      "Trial 44, Epoch 20/310, Val RMSE: 0.6434, Time: 5.96s\n",
      "Trial 44, Epoch 21/310, Val RMSE: 0.6486, Time: 6.54s\n",
      "Trial 44, Epoch 22/310, Val RMSE: 0.6538, Time: 6.16s\n",
      "Trial 44, Epoch 23/310, Val RMSE: 0.6516, Time: 6.04s\n",
      "Trial 44, Epoch 24/310, Val RMSE: 0.6510, Time: 6.07s\n",
      "Trial 44, Epoch 25/310, Val RMSE: 0.6511, Time: 5.78s\n",
      "Trial 44, Epoch 26/310, Val RMSE: 0.6489, Time: 5.97s\n",
      "Trial 44, Epoch 27/310, Val RMSE: 0.6485, Time: 6.24s\n",
      "Trial 44, Epoch 28/310, Val RMSE: 0.6513, Time: 5.64s\n",
      "Trial 44, Epoch 29/310, Val RMSE: 0.6594, Time: 6.25s\n",
      "Trial 44, Epoch 30/310, Val RMSE: 0.6598, Time: 6.14s\n",
      "Trial 44, Epoch 31/310, Val RMSE: 0.6560, Time: 6.01s\n",
      "Trial 44, Epoch 32/310, Val RMSE: 0.6577, Time: 5.95s\n",
      "Trial 44, Epoch 33/310, Val RMSE: 0.6586, Time: 6.16s\n",
      "Trial 44, Epoch 34/310, Val RMSE: 0.6532, Time: 6.23s\n",
      "Trial 44, Epoch 35/310, Val RMSE: 0.6525, Time: 5.90s\n",
      "Trial 44, Epoch 36/310, Val RMSE: 0.6588, Time: 5.87s\n",
      "Trial 44, Epoch 37/310, Val RMSE: 0.6522, Time: 5.92s\n",
      "Trial 44, Epoch 38/310, Val RMSE: 0.6571, Time: 6.18s\n",
      "Trial 44, Epoch 39/310, Val RMSE: 0.6549, Time: 6.15s\n",
      "Trial 44, Epoch 40/310, Val RMSE: 0.6649, Time: 6.09s\n",
      "Trial 44, Epoch 41/310, Val RMSE: 0.6632, Time: 5.85s\n",
      "Trial 44, Epoch 42/310, Val RMSE: 0.6592, Time: 5.83s\n",
      "Trial 44, Epoch 43/310, Val RMSE: 0.6593, Time: 6.23s\n",
      "Trial 44, Epoch 44/310, Val RMSE: 0.6595, Time: 6.12s\n",
      "Trial 44, Epoch 45/310, Val RMSE: 0.6594, Time: 6.12s\n",
      "Trial 44, Epoch 46/310, Val RMSE: 0.6589, Time: 5.86s\n",
      "Trial 44, Epoch 47/310, Val RMSE: 0.6560, Time: 6.44s\n",
      "Trial 44, Epoch 48/310, Val RMSE: 0.6597, Time: 6.27s\n",
      "Trial 44, Epoch 49/310, Val RMSE: 0.6605, Time: 6.45s\n",
      "Trial 44, Epoch 50/310, Val RMSE: 0.6628, Time: 7.26s\n",
      "Trial 44, Epoch 51/310, Val RMSE: 0.6604, Time: 4.97s\n",
      "Trial 44, Epoch 52/310, Val RMSE: 0.6650, Time: 5.63s\n",
      "Trial 44, Epoch 53/310, Val RMSE: 0.6664, Time: 5.14s\n",
      "Trial 44, Epoch 54/310, Val RMSE: 0.6611, Time: 5.05s\n",
      "Trial 44, Epoch 55/310, Val RMSE: 0.6615, Time: 5.14s\n",
      "Trial 44, Epoch 56/310, Val RMSE: 0.6695, Time: 5.17s\n",
      "Trial 44, Epoch 57/310, Val RMSE: 0.6648, Time: 5.21s\n",
      "Trial 44, Epoch 58/310, Val RMSE: 0.6580, Time: 5.12s\n",
      "Trial 44, Epoch 59/310, Val RMSE: 0.6653, Time: 5.25s\n",
      "Trial 44, Epoch 60/310, Val RMSE: 0.6583, Time: 5.03s\n",
      "Trial 44, Epoch 61/310, Val RMSE: 0.6634, Time: 5.17s\n",
      "Trial 44, Epoch 62/310, Val RMSE: 0.6622, Time: 5.42s\n",
      "Trial 44, Epoch 63/310, Val RMSE: 0.6659, Time: 5.17s\n",
      "Trial 44, Epoch 64/310, Val RMSE: 0.6634, Time: 5.20s\n",
      "Trial 44, Epoch 65/310, Val RMSE: 0.6676, Time: 5.13s\n",
      "Trial 44, Epoch 66/310, Val RMSE: 0.6707, Time: 5.18s\n",
      "Trial 44, Epoch 67/310, Val RMSE: 0.6650, Time: 5.11s\n",
      "Trial 44, Epoch 68/310, Val RMSE: 0.6641, Time: 5.27s\n",
      "Trial 44, Epoch 69/310, Val RMSE: 0.6688, Time: 5.15s\n",
      "Trial 44, Epoch 70/310, Val RMSE: 0.6640, Time: 5.16s\n",
      "Early stopping at epoch 70 for trial 44\n",
      "[I 2025-07-16 21:32:02,517] Trial 44 finished with value: 0.6640377198992452 and parameters: {'hidden_channels': 285, 'lr': 5.748626266447493e-05, 'batch_size': 64, 'n_epochs': 310, 'num_layers': 1, 'dropout_rate': 0.40282058482761723, 'weight_decay': 1.959952513122253e-06}. Best is trial 40 with value: 0.663321068578847.\n",
      "Trial 45, Epoch 1/300, Val RMSE: 0.8334, Time: 5.22s\n",
      "Trial 45, Epoch 2/300, Val RMSE: 0.7544, Time: 5.13s\n",
      "Trial 45, Epoch 3/300, Val RMSE: 0.7289, Time: 5.17s\n",
      "Trial 45, Epoch 4/300, Val RMSE: 0.7030, Time: 5.09s\n",
      "Trial 45, Epoch 5/300, Val RMSE: 0.6852, Time: 5.15s\n",
      "Trial 45, Epoch 6/300, Val RMSE: 0.6822, Time: 5.26s\n",
      "Trial 45, Epoch 7/300, Val RMSE: 0.6712, Time: 5.18s\n",
      "Trial 45, Epoch 8/300, Val RMSE: 0.6697, Time: 5.08s\n",
      "Trial 45, Epoch 9/300, Val RMSE: 0.6584, Time: 5.16s\n",
      "Trial 45, Epoch 10/300, Val RMSE: 0.6569, Time: 5.06s\n",
      "Trial 45, Epoch 11/300, Val RMSE: 0.6583, Time: 5.04s\n",
      "Trial 45, Epoch 12/300, Val RMSE: 0.6517, Time: 5.07s\n",
      "Trial 45, Epoch 13/300, Val RMSE: 0.6492, Time: 5.13s\n",
      "Trial 45, Epoch 14/300, Val RMSE: 0.6545, Time: 5.15s\n",
      "Trial 45, Epoch 15/300, Val RMSE: 0.6532, Time: 5.05s\n",
      "Trial 45, Epoch 16/300, Val RMSE: 0.6444, Time: 5.03s\n",
      "Trial 45, Epoch 17/300, Val RMSE: 0.6475, Time: 5.09s\n",
      "Trial 45, Epoch 18/300, Val RMSE: 0.6501, Time: 4.97s\n",
      "Trial 45, Epoch 19/300, Val RMSE: 0.6528, Time: 5.01s\n",
      "Trial 45, Epoch 20/300, Val RMSE: 0.6515, Time: 5.07s\n",
      "Trial 45, Epoch 21/300, Val RMSE: 0.6585, Time: 5.03s\n",
      "Trial 45, Epoch 22/300, Val RMSE: 0.6448, Time: 5.09s\n",
      "Trial 45, Epoch 23/300, Val RMSE: 0.6512, Time: 5.13s\n",
      "Trial 45, Epoch 24/300, Val RMSE: 0.6511, Time: 5.05s\n",
      "Trial 45, Epoch 25/300, Val RMSE: 0.6438, Time: 5.11s\n",
      "Trial 45, Epoch 26/300, Val RMSE: 0.6535, Time: 5.11s\n",
      "Trial 45, Epoch 27/300, Val RMSE: 0.6504, Time: 5.05s\n",
      "Trial 45, Epoch 28/300, Val RMSE: 0.6544, Time: 5.02s\n",
      "Trial 45, Epoch 29/300, Val RMSE: 0.6500, Time: 5.12s\n",
      "Trial 45, Epoch 30/300, Val RMSE: 0.6619, Time: 5.08s\n",
      "Trial 45, Epoch 31/300, Val RMSE: 0.6559, Time: 5.12s\n",
      "Trial 45, Epoch 32/300, Val RMSE: 0.6509, Time: 5.08s\n",
      "Trial 45, Epoch 33/300, Val RMSE: 0.6601, Time: 5.05s\n",
      "Trial 45, Epoch 34/300, Val RMSE: 0.6530, Time: 5.03s\n",
      "Trial 45, Epoch 35/300, Val RMSE: 0.6564, Time: 5.10s\n",
      "Trial 45, Epoch 36/300, Val RMSE: 0.6559, Time: 5.06s\n",
      "Trial 45, Epoch 37/300, Val RMSE: 0.6535, Time: 5.13s\n",
      "Trial 45, Epoch 38/300, Val RMSE: 0.6476, Time: 5.14s\n",
      "Trial 45, Epoch 39/300, Val RMSE: 0.6542, Time: 5.12s\n",
      "Trial 45, Epoch 40/300, Val RMSE: 0.6591, Time: 5.06s\n",
      "Trial 45, Epoch 41/300, Val RMSE: 0.6584, Time: 5.08s\n",
      "Trial 45, Epoch 42/300, Val RMSE: 0.6540, Time: 5.03s\n",
      "Trial 45, Epoch 43/300, Val RMSE: 0.6526, Time: 5.20s\n",
      "Trial 45, Epoch 44/300, Val RMSE: 0.6548, Time: 5.06s\n",
      "Trial 45, Epoch 45/300, Val RMSE: 0.6476, Time: 5.15s\n",
      "Trial 45, Epoch 46/300, Val RMSE: 0.6610, Time: 5.17s\n",
      "Trial 45, Epoch 47/300, Val RMSE: 0.6541, Time: 5.10s\n",
      "Trial 45, Epoch 48/300, Val RMSE: 0.6581, Time: 5.14s\n",
      "Trial 45, Epoch 49/300, Val RMSE: 0.6597, Time: 5.18s\n",
      "Trial 45, Epoch 50/300, Val RMSE: 0.6570, Time: 5.22s\n",
      "Trial 45, Epoch 51/300, Val RMSE: 0.6590, Time: 5.04s\n",
      "Trial 45, Epoch 52/300, Val RMSE: 0.6544, Time: 4.88s\n",
      "Trial 45, Epoch 53/300, Val RMSE: 0.6584, Time: 4.98s\n",
      "Trial 45, Epoch 54/300, Val RMSE: 0.6583, Time: 5.17s\n",
      "Trial 45, Epoch 55/300, Val RMSE: 0.6599, Time: 5.02s\n",
      "Trial 45, Epoch 56/300, Val RMSE: 0.6509, Time: 5.07s\n",
      "Trial 45, Epoch 57/300, Val RMSE: 0.6610, Time: 4.97s\n",
      "Trial 45, Epoch 58/300, Val RMSE: 0.6560, Time: 4.96s\n",
      "Trial 45, Epoch 59/300, Val RMSE: 0.6606, Time: 5.04s\n",
      "Trial 45, Epoch 60/300, Val RMSE: 0.6581, Time: 5.06s\n",
      "Trial 45, Epoch 61/300, Val RMSE: 0.6601, Time: 4.99s\n",
      "Trial 45, Epoch 62/300, Val RMSE: 0.6525, Time: 5.07s\n",
      "Trial 45, Epoch 63/300, Val RMSE: 0.6600, Time: 5.02s\n",
      "Trial 45, Epoch 64/300, Val RMSE: 0.6591, Time: 4.98s\n",
      "Trial 45, Epoch 65/300, Val RMSE: 0.6603, Time: 5.07s\n",
      "Trial 45, Epoch 66/300, Val RMSE: 0.6562, Time: 5.01s\n",
      "Trial 45, Epoch 67/300, Val RMSE: 0.6597, Time: 5.03s\n",
      "Trial 45, Epoch 68/300, Val RMSE: 0.6604, Time: 5.05s\n",
      "Trial 45, Epoch 69/300, Val RMSE: 0.6652, Time: 4.99s\n",
      "Trial 45, Epoch 70/300, Val RMSE: 0.6622, Time: 4.95s\n",
      "Trial 45, Epoch 71/300, Val RMSE: 0.6576, Time: 4.94s\n",
      "Trial 45, Epoch 72/300, Val RMSE: 0.6591, Time: 4.97s\n",
      "Trial 45, Epoch 73/300, Val RMSE: 0.6603, Time: 4.93s\n",
      "Trial 45, Epoch 74/300, Val RMSE: 0.6588, Time: 4.96s\n",
      "Trial 45, Epoch 75/300, Val RMSE: 0.6535, Time: 5.22s\n",
      "Early stopping at epoch 75 for trial 45\n",
      "[I 2025-07-16 21:38:25,212] Trial 45 finished with value: 0.6534684883358601 and parameters: {'hidden_channels': 411, 'lr': 6.103466706632384e-05, 'batch_size': 64, 'n_epochs': 300, 'num_layers': 1, 'dropout_rate': 0.3788335910208114, 'weight_decay': 7.180077519131929e-07}. Best is trial 45 with value: 0.6534684883358601.\n",
      "Trial 46, Epoch 1/299, Val RMSE: 0.8389, Time: 5.18s\n",
      "Trial 46, Epoch 2/299, Val RMSE: 0.7512, Time: 4.96s\n",
      "Trial 46, Epoch 3/299, Val RMSE: 0.7305, Time: 4.98s\n",
      "Trial 46, Epoch 4/299, Val RMSE: 0.7194, Time: 4.98s\n",
      "Trial 46, Epoch 5/299, Val RMSE: 0.6947, Time: 5.02s\n",
      "Trial 46, Epoch 6/299, Val RMSE: 0.6853, Time: 4.97s\n",
      "Trial 46, Epoch 7/299, Val RMSE: 0.6788, Time: 5.18s\n",
      "Trial 46, Epoch 8/299, Val RMSE: 0.6754, Time: 5.11s\n",
      "Trial 46, Epoch 9/299, Val RMSE: 0.6685, Time: 5.06s\n",
      "Trial 46, Epoch 10/299, Val RMSE: 0.6690, Time: 5.07s\n",
      "Trial 46, Epoch 11/299, Val RMSE: 0.6580, Time: 5.05s\n",
      "Trial 46, Epoch 12/299, Val RMSE: 0.6539, Time: 5.08s\n",
      "Trial 46, Epoch 13/299, Val RMSE: 0.6556, Time: 5.05s\n",
      "Trial 46, Epoch 14/299, Val RMSE: 0.6538, Time: 5.09s\n",
      "Trial 46, Epoch 15/299, Val RMSE: 0.6488, Time: 5.19s\n",
      "Trial 46, Epoch 16/299, Val RMSE: 0.6543, Time: 5.17s\n",
      "Trial 46, Epoch 17/299, Val RMSE: 0.6525, Time: 5.18s\n",
      "Trial 46, Epoch 18/299, Val RMSE: 0.6551, Time: 5.01s\n",
      "Trial 46, Epoch 19/299, Val RMSE: 0.6492, Time: 5.71s\n",
      "Trial 46, Epoch 20/299, Val RMSE: 0.6545, Time: 5.07s\n",
      "Trial 46, Epoch 21/299, Val RMSE: 0.6541, Time: 5.33s\n",
      "Trial 46, Epoch 22/299, Val RMSE: 0.6505, Time: 5.11s\n",
      "Trial 46, Epoch 23/299, Val RMSE: 0.6583, Time: 5.21s\n",
      "Trial 46, Epoch 24/299, Val RMSE: 0.6472, Time: 5.13s\n",
      "Trial 46, Epoch 25/299, Val RMSE: 0.6425, Time: 5.04s\n",
      "Trial 46, Epoch 26/299, Val RMSE: 0.6640, Time: 5.11s\n",
      "Trial 46, Epoch 27/299, Val RMSE: 0.6568, Time: 5.03s\n",
      "Trial 46, Epoch 28/299, Val RMSE: 0.6535, Time: 5.18s\n",
      "Trial 46, Epoch 29/299, Val RMSE: 0.6534, Time: 5.20s\n",
      "Trial 46, Epoch 30/299, Val RMSE: 0.6459, Time: 5.18s\n",
      "Trial 46, Epoch 31/299, Val RMSE: 0.6536, Time: 5.47s\n",
      "Trial 46, Epoch 32/299, Val RMSE: 0.6534, Time: 5.19s\n",
      "Trial 46, Epoch 33/299, Val RMSE: 0.6578, Time: 5.13s\n",
      "Trial 46, Epoch 34/299, Val RMSE: 0.6492, Time: 5.41s\n",
      "Trial 46, Epoch 35/299, Val RMSE: 0.6543, Time: 5.24s\n",
      "Trial 46, Epoch 36/299, Val RMSE: 0.6533, Time: 5.04s\n",
      "Trial 46, Epoch 37/299, Val RMSE: 0.6569, Time: 5.07s\n",
      "Trial 46, Epoch 38/299, Val RMSE: 0.6505, Time: 5.21s\n",
      "Trial 46, Epoch 39/299, Val RMSE: 0.6530, Time: 5.08s\n",
      "Trial 46, Epoch 40/299, Val RMSE: 0.6507, Time: 5.04s\n",
      "Trial 46, Epoch 41/299, Val RMSE: 0.6532, Time: 5.07s\n",
      "Trial 46, Epoch 42/299, Val RMSE: 0.6514, Time: 4.96s\n",
      "Trial 46, Epoch 43/299, Val RMSE: 0.6537, Time: 4.94s\n",
      "Trial 46, Epoch 44/299, Val RMSE: 0.6533, Time: 4.92s\n",
      "Trial 46, Epoch 45/299, Val RMSE: 0.6516, Time: 4.97s\n",
      "Trial 46, Epoch 46/299, Val RMSE: 0.6514, Time: 4.94s\n",
      "Trial 46, Epoch 47/299, Val RMSE: 0.6519, Time: 5.10s\n",
      "Trial 46, Epoch 48/299, Val RMSE: 0.6497, Time: 5.19s\n",
      "Trial 46, Epoch 49/299, Val RMSE: 0.6533, Time: 5.12s\n",
      "Trial 46, Epoch 50/299, Val RMSE: 0.6537, Time: 4.93s\n",
      "Trial 46, Epoch 51/299, Val RMSE: 0.6516, Time: 5.00s\n",
      "Trial 46, Epoch 52/299, Val RMSE: 0.6520, Time: 5.02s\n",
      "Trial 46, Epoch 53/299, Val RMSE: 0.6532, Time: 5.00s\n",
      "Trial 46, Epoch 54/299, Val RMSE: 0.6493, Time: 5.04s\n",
      "Trial 46, Epoch 55/299, Val RMSE: 0.6456, Time: 5.04s\n",
      "Trial 46, Epoch 56/299, Val RMSE: 0.6593, Time: 4.99s\n",
      "Trial 46, Epoch 57/299, Val RMSE: 0.6542, Time: 5.05s\n",
      "Trial 46, Epoch 58/299, Val RMSE: 0.6525, Time: 5.00s\n",
      "Trial 46, Epoch 59/299, Val RMSE: 0.6552, Time: 5.03s\n",
      "Trial 46, Epoch 60/299, Val RMSE: 0.6532, Time: 5.01s\n",
      "Trial 46, Epoch 61/299, Val RMSE: 0.6585, Time: 5.02s\n",
      "Trial 46, Epoch 62/299, Val RMSE: 0.6614, Time: 4.97s\n",
      "Trial 46, Epoch 63/299, Val RMSE: 0.6589, Time: 4.99s\n",
      "Trial 46, Epoch 64/299, Val RMSE: 0.6586, Time: 4.99s\n",
      "Trial 46, Epoch 65/299, Val RMSE: 0.6583, Time: 5.03s\n",
      "Trial 46, Epoch 66/299, Val RMSE: 0.6521, Time: 4.98s\n",
      "Trial 46, Epoch 67/299, Val RMSE: 0.6538, Time: 5.16s\n",
      "Trial 46, Epoch 68/299, Val RMSE: 0.6525, Time: 5.11s\n",
      "Trial 46, Epoch 69/299, Val RMSE: 0.6621, Time: 5.10s\n",
      "Trial 46, Epoch 70/299, Val RMSE: 0.6559, Time: 5.01s\n",
      "Trial 46, Epoch 71/299, Val RMSE: 0.6579, Time: 5.02s\n",
      "Trial 46, Epoch 72/299, Val RMSE: 0.6569, Time: 5.03s\n",
      "Trial 46, Epoch 73/299, Val RMSE: 0.6605, Time: 5.03s\n",
      "Trial 46, Epoch 74/299, Val RMSE: 0.6511, Time: 5.05s\n",
      "Trial 46, Epoch 75/299, Val RMSE: 0.6582, Time: 5.01s\n",
      "Early stopping at epoch 75 for trial 46\n",
      "[I 2025-07-16 21:44:48,832] Trial 46 finished with value: 0.6582321879887565 and parameters: {'hidden_channels': 408, 'lr': 5.9571541535778645e-05, 'batch_size': 64, 'n_epochs': 299, 'num_layers': 1, 'dropout_rate': 0.37614598631127616, 'weight_decay': 8.718761195585528e-07}. Best is trial 45 with value: 0.6534684883358601.\n",
      "Trial 47, Epoch 1/281, Val RMSE: 0.8322, Time: 5.13s\n",
      "Trial 47, Epoch 2/281, Val RMSE: 0.7622, Time: 5.10s\n",
      "Trial 47, Epoch 3/281, Val RMSE: 0.7325, Time: 5.09s\n",
      "Trial 47, Epoch 4/281, Val RMSE: 0.7117, Time: 5.04s\n",
      "Trial 47, Epoch 5/281, Val RMSE: 0.7054, Time: 5.16s\n",
      "Trial 47, Epoch 6/281, Val RMSE: 0.6817, Time: 5.26s\n",
      "Trial 47, Epoch 7/281, Val RMSE: 0.6735, Time: 5.16s\n",
      "Trial 47, Epoch 8/281, Val RMSE: 0.6743, Time: 5.10s\n",
      "Trial 47, Epoch 9/281, Val RMSE: 0.6740, Time: 5.12s\n",
      "Trial 47, Epoch 10/281, Val RMSE: 0.6517, Time: 5.03s\n",
      "Trial 47, Epoch 11/281, Val RMSE: 0.6642, Time: 5.16s\n",
      "Trial 47, Epoch 12/281, Val RMSE: 0.6597, Time: 5.09s\n",
      "Trial 47, Epoch 13/281, Val RMSE: 0.6571, Time: 5.00s\n",
      "Trial 47, Epoch 14/281, Val RMSE: 0.6626, Time: 4.94s\n",
      "Trial 47, Epoch 15/281, Val RMSE: 0.6517, Time: 5.01s\n",
      "Trial 47, Epoch 16/281, Val RMSE: 0.6513, Time: 5.14s\n",
      "Trial 47, Epoch 17/281, Val RMSE: 0.6501, Time: 5.02s\n",
      "Trial 47, Epoch 18/281, Val RMSE: 0.6494, Time: 5.20s\n",
      "Trial 47, Epoch 19/281, Val RMSE: 0.6504, Time: 5.14s\n",
      "Trial 47, Epoch 20/281, Val RMSE: 0.6493, Time: 5.02s\n",
      "Trial 47, Epoch 21/281, Val RMSE: 0.6475, Time: 4.96s\n",
      "Trial 47, Epoch 22/281, Val RMSE: 0.6537, Time: 5.01s\n",
      "Trial 47, Epoch 23/281, Val RMSE: 0.6552, Time: 4.98s\n",
      "Trial 47, Epoch 24/281, Val RMSE: 0.6494, Time: 5.00s\n",
      "Trial 47, Epoch 25/281, Val RMSE: 0.6585, Time: 5.00s\n",
      "Trial 47, Epoch 26/281, Val RMSE: 0.6543, Time: 5.15s\n",
      "Trial 47, Epoch 27/281, Val RMSE: 0.6542, Time: 5.09s\n",
      "Trial 47, Epoch 28/281, Val RMSE: 0.6588, Time: 5.09s\n",
      "Trial 47, Epoch 29/281, Val RMSE: 0.6551, Time: 5.18s\n",
      "Trial 47, Epoch 30/281, Val RMSE: 0.6490, Time: 5.46s\n",
      "Trial 47, Epoch 31/281, Val RMSE: 0.6566, Time: 5.07s\n",
      "Trial 47, Epoch 32/281, Val RMSE: 0.6628, Time: 5.09s\n",
      "Trial 47, Epoch 33/281, Val RMSE: 0.6583, Time: 5.04s\n",
      "Trial 47, Epoch 34/281, Val RMSE: 0.6519, Time: 5.04s\n",
      "Trial 47, Epoch 35/281, Val RMSE: 0.6497, Time: 5.11s\n",
      "Trial 47, Epoch 36/281, Val RMSE: 0.6431, Time: 5.09s\n",
      "Trial 47, Epoch 37/281, Val RMSE: 0.6617, Time: 5.10s\n",
      "Trial 47, Epoch 38/281, Val RMSE: 0.6482, Time: 4.97s\n",
      "Trial 47, Epoch 39/281, Val RMSE: 0.6505, Time: 4.96s\n",
      "Trial 47, Epoch 40/281, Val RMSE: 0.6581, Time: 4.92s\n",
      "Trial 47, Epoch 41/281, Val RMSE: 0.6555, Time: 5.05s\n",
      "Trial 47, Epoch 42/281, Val RMSE: 0.6544, Time: 4.95s\n",
      "Trial 47, Epoch 43/281, Val RMSE: 0.6517, Time: 5.01s\n",
      "Trial 47, Epoch 44/281, Val RMSE: 0.6514, Time: 4.99s\n",
      "Trial 47, Epoch 45/281, Val RMSE: 0.6589, Time: 4.92s\n",
      "Trial 47, Epoch 46/281, Val RMSE: 0.6472, Time: 4.99s\n",
      "Trial 47, Epoch 47/281, Val RMSE: 0.6573, Time: 5.10s\n",
      "Trial 47, Epoch 48/281, Val RMSE: 0.6524, Time: 5.13s\n",
      "Trial 47, Epoch 49/281, Val RMSE: 0.6515, Time: 5.08s\n",
      "Trial 47, Epoch 50/281, Val RMSE: 0.6536, Time: 5.11s\n",
      "Trial 47, Epoch 51/281, Val RMSE: 0.6536, Time: 5.01s\n",
      "Trial 47, Epoch 52/281, Val RMSE: 0.6548, Time: 5.13s\n",
      "Trial 47, Epoch 53/281, Val RMSE: 0.6587, Time: 5.09s\n",
      "Trial 47, Epoch 54/281, Val RMSE: 0.6496, Time: 5.04s\n",
      "Trial 47, Epoch 55/281, Val RMSE: 0.6539, Time: 5.10s\n",
      "Trial 47, Epoch 56/281, Val RMSE: 0.6495, Time: 5.04s\n",
      "Trial 47, Epoch 57/281, Val RMSE: 0.6570, Time: 5.04s\n",
      "Trial 47, Epoch 58/281, Val RMSE: 0.6550, Time: 5.18s\n",
      "Trial 47, Epoch 59/281, Val RMSE: 0.6566, Time: 5.18s\n",
      "Trial 47, Epoch 60/281, Val RMSE: 0.6485, Time: 5.10s\n",
      "Trial 47, Epoch 61/281, Val RMSE: 0.6554, Time: 4.97s\n",
      "Trial 47, Epoch 62/281, Val RMSE: 0.6585, Time: 5.04s\n",
      "Trial 47, Epoch 63/281, Val RMSE: 0.6485, Time: 4.98s\n",
      "Trial 47, Epoch 64/281, Val RMSE: 0.6551, Time: 5.02s\n",
      "Trial 47, Epoch 65/281, Val RMSE: 0.6469, Time: 5.15s\n",
      "Trial 47, Epoch 66/281, Val RMSE: 0.6489, Time: 4.99s\n",
      "Trial 47, Epoch 67/281, Val RMSE: 0.6579, Time: 5.05s\n",
      "Trial 47, Epoch 68/281, Val RMSE: 0.6541, Time: 5.13s\n",
      "Trial 47, Epoch 69/281, Val RMSE: 0.6562, Time: 5.00s\n",
      "Trial 47, Epoch 70/281, Val RMSE: 0.6579, Time: 5.11s\n",
      "Trial 47, Epoch 71/281, Val RMSE: 0.6574, Time: 5.14s\n",
      "Trial 47, Epoch 72/281, Val RMSE: 0.6514, Time: 5.02s\n",
      "Trial 47, Epoch 73/281, Val RMSE: 0.6547, Time: 5.25s\n",
      "Trial 47, Epoch 74/281, Val RMSE: 0.6557, Time: 5.23s\n",
      "Trial 47, Epoch 75/281, Val RMSE: 0.6491, Time: 5.10s\n",
      "Trial 47, Epoch 76/281, Val RMSE: 0.6560, Time: 5.19s\n",
      "Trial 47, Epoch 77/281, Val RMSE: 0.6522, Time: 5.26s\n",
      "Trial 47, Epoch 78/281, Val RMSE: 0.6553, Time: 5.24s\n",
      "Trial 47, Epoch 79/281, Val RMSE: 0.6516, Time: 5.02s\n",
      "Trial 47, Epoch 80/281, Val RMSE: 0.6558, Time: 5.10s\n",
      "Trial 47, Epoch 81/281, Val RMSE: 0.6546, Time: 5.15s\n",
      "Trial 47, Epoch 82/281, Val RMSE: 0.6605, Time: 5.07s\n",
      "Trial 47, Epoch 83/281, Val RMSE: 0.6581, Time: 5.21s\n",
      "Trial 47, Epoch 84/281, Val RMSE: 0.6537, Time: 5.10s\n",
      "Trial 47, Epoch 85/281, Val RMSE: 0.6571, Time: 5.07s\n",
      "Trial 47, Epoch 86/281, Val RMSE: 0.6581, Time: 5.13s\n",
      "Early stopping at epoch 86 for trial 47\n",
      "[I 2025-07-16 21:52:08,407] Trial 47 finished with value: 0.6580933332274295 and parameters: {'hidden_channels': 423, 'lr': 7.329247924628799e-05, 'batch_size': 64, 'n_epochs': 281, 'num_layers': 1, 'dropout_rate': 0.4036032520586506, 'weight_decay': 6.018706394360516e-07}. Best is trial 45 with value: 0.6534684883358601.\n",
      "Trial 48, Epoch 1/225, Val RMSE: 1.3317, Time: 5.20s\n",
      "Trial 48, Epoch 2/225, Val RMSE: 0.8005, Time: 5.05s\n",
      "Trial 48, Epoch 3/225, Val RMSE: 0.7505, Time: 5.24s\n",
      "Trial 48, Epoch 4/225, Val RMSE: 0.7311, Time: 5.21s\n",
      "Trial 48, Epoch 5/225, Val RMSE: 0.7164, Time: 5.19s\n",
      "Trial 48, Epoch 6/225, Val RMSE: 0.7031, Time: 5.32s\n",
      "Trial 48, Epoch 7/225, Val RMSE: 0.6912, Time: 5.06s\n",
      "Trial 48, Epoch 8/225, Val RMSE: 0.6860, Time: 5.22s\n",
      "Trial 48, Epoch 9/225, Val RMSE: 0.6755, Time: 5.11s\n",
      "Trial 48, Epoch 10/225, Val RMSE: 0.6661, Time: 5.14s\n",
      "Trial 48, Epoch 11/225, Val RMSE: 0.6615, Time: 5.12s\n",
      "Trial 48, Epoch 12/225, Val RMSE: 0.6571, Time: 5.02s\n",
      "Trial 48, Epoch 13/225, Val RMSE: 0.6543, Time: 5.12s\n",
      "Trial 48, Epoch 14/225, Val RMSE: 0.6501, Time: 5.14s\n",
      "Trial 48, Epoch 15/225, Val RMSE: 0.6486, Time: 5.15s\n",
      "Trial 48, Epoch 16/225, Val RMSE: 0.6497, Time: 5.25s\n",
      "Trial 48, Epoch 17/225, Val RMSE: 0.6436, Time: 5.11s\n",
      "Trial 48, Epoch 18/225, Val RMSE: 0.6459, Time: 5.18s\n",
      "Trial 48, Epoch 19/225, Val RMSE: 0.6397, Time: 5.10s\n",
      "Trial 48, Epoch 20/225, Val RMSE: 0.6444, Time: 5.21s\n",
      "Trial 48, Epoch 21/225, Val RMSE: 0.6420, Time: 5.10s\n",
      "Trial 48, Epoch 22/225, Val RMSE: 0.6505, Time: 5.06s\n",
      "Trial 48, Epoch 23/225, Val RMSE: 0.6385, Time: 5.03s\n",
      "Trial 48, Epoch 24/225, Val RMSE: 0.6400, Time: 5.11s\n",
      "Trial 48, Epoch 25/225, Val RMSE: 0.6434, Time: 5.10s\n",
      "Trial 48, Epoch 26/225, Val RMSE: 0.6381, Time: 5.08s\n",
      "Trial 48, Epoch 27/225, Val RMSE: 0.6396, Time: 5.21s\n",
      "Trial 48, Epoch 28/225, Val RMSE: 0.6469, Time: 5.12s\n",
      "Trial 48, Epoch 29/225, Val RMSE: 0.6473, Time: 5.16s\n",
      "Trial 48, Epoch 30/225, Val RMSE: 0.6437, Time: 5.04s\n",
      "Trial 48, Epoch 31/225, Val RMSE: 0.6442, Time: 5.00s\n",
      "Trial 48, Epoch 32/225, Val RMSE: 0.6436, Time: 5.06s\n",
      "Trial 48, Epoch 33/225, Val RMSE: 0.6358, Time: 5.07s\n",
      "Trial 48, Epoch 34/225, Val RMSE: 0.6452, Time: 5.08s\n",
      "Trial 48, Epoch 35/225, Val RMSE: 0.6427, Time: 5.14s\n",
      "Trial 48, Epoch 36/225, Val RMSE: 0.6413, Time: 5.10s\n",
      "Trial 48, Epoch 37/225, Val RMSE: 0.6447, Time: 5.09s\n",
      "Trial 48, Epoch 38/225, Val RMSE: 0.6421, Time: 5.49s\n",
      "Trial 48, Epoch 39/225, Val RMSE: 0.6404, Time: 5.19s\n",
      "Trial 48, Epoch 40/225, Val RMSE: 0.6426, Time: 5.21s\n",
      "Trial 48, Epoch 41/225, Val RMSE: 0.6436, Time: 5.13s\n",
      "Trial 48, Epoch 42/225, Val RMSE: 0.6423, Time: 5.17s\n",
      "Trial 48, Epoch 43/225, Val RMSE: 0.6460, Time: 5.16s\n",
      "Trial 48, Epoch 44/225, Val RMSE: 0.6477, Time: 5.20s\n",
      "Trial 48, Epoch 45/225, Val RMSE: 0.6442, Time: 5.09s\n",
      "Trial 48, Epoch 46/225, Val RMSE: 0.6426, Time: 5.01s\n",
      "Trial 48, Epoch 47/225, Val RMSE: 0.6400, Time: 4.99s\n",
      "Trial 48, Epoch 48/225, Val RMSE: 0.6448, Time: 4.94s\n",
      "Trial 48, Epoch 49/225, Val RMSE: 0.6468, Time: 5.27s\n",
      "Trial 48, Epoch 50/225, Val RMSE: 0.6431, Time: 4.98s\n",
      "Trial 48, Epoch 51/225, Val RMSE: 0.6470, Time: 5.22s\n",
      "Trial 48, Epoch 52/225, Val RMSE: 0.6497, Time: 5.03s\n",
      "Trial 48, Epoch 53/225, Val RMSE: 0.6514, Time: 5.09s\n",
      "Trial 48, Epoch 54/225, Val RMSE: 0.6471, Time: 4.99s\n",
      "Trial 48, Epoch 55/225, Val RMSE: 0.6492, Time: 5.03s\n",
      "Trial 48, Epoch 56/225, Val RMSE: 0.6479, Time: 4.99s\n",
      "Trial 48, Epoch 57/225, Val RMSE: 0.6465, Time: 5.03s\n",
      "Trial 48, Epoch 58/225, Val RMSE: 0.6469, Time: 5.06s\n",
      "Trial 48, Epoch 59/225, Val RMSE: 0.6471, Time: 5.04s\n",
      "Trial 48, Epoch 60/225, Val RMSE: 0.6510, Time: 5.06s\n",
      "Trial 48, Epoch 61/225, Val RMSE: 0.6518, Time: 5.11s\n",
      "Trial 48, Epoch 62/225, Val RMSE: 0.6538, Time: 5.08s\n",
      "Trial 48, Epoch 63/225, Val RMSE: 0.6504, Time: 5.08s\n",
      "Trial 48, Epoch 64/225, Val RMSE: 0.6499, Time: 5.19s\n",
      "Trial 48, Epoch 65/225, Val RMSE: 0.6503, Time: 5.10s\n",
      "Trial 48, Epoch 66/225, Val RMSE: 0.6519, Time: 5.08s\n",
      "Trial 48, Epoch 67/225, Val RMSE: 0.6542, Time: 5.10s\n",
      "Trial 48, Epoch 68/225, Val RMSE: 0.6509, Time: 5.09s\n",
      "Trial 48, Epoch 69/225, Val RMSE: 0.6478, Time: 5.06s\n",
      "Trial 48, Epoch 70/225, Val RMSE: 0.6494, Time: 4.95s\n",
      "Trial 48, Epoch 71/225, Val RMSE: 0.6519, Time: 5.01s\n",
      "Trial 48, Epoch 72/225, Val RMSE: 0.6475, Time: 5.09s\n",
      "Trial 48, Epoch 73/225, Val RMSE: 0.6508, Time: 5.09s\n",
      "Trial 48, Epoch 74/225, Val RMSE: 0.6471, Time: 5.16s\n",
      "Trial 48, Epoch 75/225, Val RMSE: 0.6487, Time: 5.13s\n",
      "Trial 48, Epoch 76/225, Val RMSE: 0.6492, Time: 5.04s\n",
      "Trial 48, Epoch 77/225, Val RMSE: 0.6513, Time: 5.10s\n",
      "Trial 48, Epoch 78/225, Val RMSE: 0.6563, Time: 5.02s\n",
      "Trial 48, Epoch 79/225, Val RMSE: 0.6539, Time: 4.98s\n",
      "Trial 48, Epoch 80/225, Val RMSE: 0.6513, Time: 5.06s\n",
      "Trial 48, Epoch 81/225, Val RMSE: 0.6554, Time: 5.05s\n",
      "Trial 48, Epoch 82/225, Val RMSE: 0.6511, Time: 5.05s\n",
      "Trial 48, Epoch 83/225, Val RMSE: 0.6487, Time: 5.05s\n",
      "Early stopping at epoch 83 for trial 48\n",
      "[I 2025-07-16 21:59:14,631] Trial 48 finished with value: 0.648708700008342 and parameters: {'hidden_channels': 401, 'lr': 3.898905727770221e-05, 'batch_size': 64, 'n_epochs': 225, 'num_layers': 1, 'dropout_rate': 0.36019317743177537, 'weight_decay': 3.609741549321615e-07}. Best is trial 48 with value: 0.648708700008342.\n",
      "Trial 49, Epoch 1/218, Val RMSE: 0.7935, Time: 5.11s\n",
      "Trial 49, Epoch 2/218, Val RMSE: 0.7519, Time: 5.03s\n",
      "Trial 49, Epoch 3/218, Val RMSE: 0.7287, Time: 5.07s\n",
      "Trial 49, Epoch 4/218, Val RMSE: 0.6964, Time: 5.05s\n",
      "Trial 49, Epoch 5/218, Val RMSE: 0.7059, Time: 5.10s\n",
      "Trial 49, Epoch 6/218, Val RMSE: 0.6865, Time: 5.30s\n",
      "Trial 49, Epoch 7/218, Val RMSE: 0.6746, Time: 5.26s\n",
      "Trial 49, Epoch 8/218, Val RMSE: 0.6672, Time: 5.19s\n",
      "Trial 49, Epoch 9/218, Val RMSE: 0.6577, Time: 5.02s\n",
      "Trial 49, Epoch 10/218, Val RMSE: 0.6511, Time: 5.10s\n",
      "Trial 49, Epoch 11/218, Val RMSE: 0.6566, Time: 5.07s\n",
      "Trial 49, Epoch 12/218, Val RMSE: 0.6437, Time: 5.08s\n",
      "Trial 49, Epoch 13/218, Val RMSE: 0.6660, Time: 5.11s\n",
      "Trial 49, Epoch 14/218, Val RMSE: 0.6504, Time: 5.00s\n",
      "Trial 49, Epoch 15/218, Val RMSE: 0.6534, Time: 5.06s\n",
      "Trial 49, Epoch 16/218, Val RMSE: 0.6536, Time: 5.13s\n",
      "Trial 49, Epoch 17/218, Val RMSE: 0.6393, Time: 5.01s\n",
      "Trial 49, Epoch 18/218, Val RMSE: 0.6431, Time: 5.06s\n",
      "Trial 49, Epoch 19/218, Val RMSE: 0.6468, Time: 5.09s\n",
      "Trial 49, Epoch 20/218, Val RMSE: 0.6505, Time: 5.09s\n",
      "Trial 49, Epoch 21/218, Val RMSE: 0.6448, Time: 5.12s\n",
      "Trial 49, Epoch 22/218, Val RMSE: 0.6364, Time: 5.15s\n",
      "Trial 49, Epoch 23/218, Val RMSE: 0.6392, Time: 5.26s\n",
      "Trial 49, Epoch 24/218, Val RMSE: 0.6480, Time: 5.46s\n",
      "Trial 49, Epoch 25/218, Val RMSE: 0.6466, Time: 5.26s\n",
      "Trial 49, Epoch 26/218, Val RMSE: 0.6420, Time: 5.09s\n",
      "Trial 49, Epoch 27/218, Val RMSE: 0.6418, Time: 5.19s\n",
      "Trial 49, Epoch 28/218, Val RMSE: 0.6449, Time: 5.09s\n",
      "Trial 49, Epoch 29/218, Val RMSE: 0.6548, Time: 5.22s\n",
      "Trial 49, Epoch 30/218, Val RMSE: 0.6396, Time: 5.18s\n",
      "Trial 49, Epoch 31/218, Val RMSE: 0.6407, Time: 5.10s\n",
      "Trial 49, Epoch 32/218, Val RMSE: 0.6477, Time: 5.02s\n",
      "Trial 49, Epoch 33/218, Val RMSE: 0.6421, Time: 5.03s\n",
      "Trial 49, Epoch 34/218, Val RMSE: 0.6341, Time: 5.00s\n",
      "Trial 49, Epoch 35/218, Val RMSE: 0.6425, Time: 5.10s\n",
      "Trial 49, Epoch 36/218, Val RMSE: 0.6370, Time: 5.00s\n",
      "Trial 49, Epoch 37/218, Val RMSE: 0.6460, Time: 5.11s\n",
      "Trial 49, Epoch 38/218, Val RMSE: 0.6443, Time: 5.03s\n",
      "Trial 49, Epoch 39/218, Val RMSE: 0.6391, Time: 5.05s\n",
      "Trial 49, Epoch 40/218, Val RMSE: 0.6436, Time: 5.11s\n",
      "Trial 49, Epoch 41/218, Val RMSE: 0.6396, Time: 5.08s\n",
      "Trial 49, Epoch 42/218, Val RMSE: 0.6389, Time: 5.17s\n",
      "Trial 49, Epoch 43/218, Val RMSE: 0.6425, Time: 5.07s\n",
      "Trial 49, Epoch 44/218, Val RMSE: 0.6405, Time: 5.00s\n",
      "Trial 49, Epoch 45/218, Val RMSE: 0.6393, Time: 5.10s\n",
      "Trial 49, Epoch 46/218, Val RMSE: 0.6463, Time: 5.09s\n",
      "Trial 49, Epoch 47/218, Val RMSE: 0.6388, Time: 5.06s\n",
      "Trial 49, Epoch 48/218, Val RMSE: 0.6483, Time: 5.10s\n",
      "Trial 49, Epoch 49/218, Val RMSE: 0.6376, Time: 5.27s\n",
      "Trial 49, Epoch 50/218, Val RMSE: 0.6385, Time: 5.13s\n",
      "Trial 49, Epoch 51/218, Val RMSE: 0.6374, Time: 5.11s\n",
      "Trial 49, Epoch 52/218, Val RMSE: 0.6418, Time: 5.01s\n",
      "Trial 49, Epoch 53/218, Val RMSE: 0.6408, Time: 5.06s\n",
      "Trial 49, Epoch 54/218, Val RMSE: 0.6427, Time: 5.17s\n",
      "Trial 49, Epoch 55/218, Val RMSE: 0.6416, Time: 5.01s\n",
      "Trial 49, Epoch 56/218, Val RMSE: 0.6378, Time: 5.04s\n",
      "Trial 49, Epoch 57/218, Val RMSE: 0.6436, Time: 5.03s\n",
      "Trial 49, Epoch 58/218, Val RMSE: 0.6409, Time: 4.99s\n",
      "Trial 49, Epoch 59/218, Val RMSE: 0.6329, Time: 5.00s\n",
      "Trial 49, Epoch 60/218, Val RMSE: 0.6367, Time: 5.04s\n",
      "Trial 49, Epoch 61/218, Val RMSE: 0.6466, Time: 5.10s\n",
      "Trial 49, Epoch 62/218, Val RMSE: 0.6418, Time: 5.13s\n",
      "Trial 49, Epoch 63/218, Val RMSE: 0.6450, Time: 5.12s\n",
      "Trial 49, Epoch 64/218, Val RMSE: 0.6373, Time: 5.04s\n",
      "Trial 49, Epoch 65/218, Val RMSE: 0.6422, Time: 5.16s\n",
      "Trial 49, Epoch 66/218, Val RMSE: 0.6397, Time: 5.17s\n",
      "Trial 49, Epoch 67/218, Val RMSE: 0.6395, Time: 5.14s\n",
      "Trial 49, Epoch 68/218, Val RMSE: 0.6385, Time: 5.23s\n",
      "Trial 49, Epoch 69/218, Val RMSE: 0.6415, Time: 5.05s\n",
      "Trial 49, Epoch 70/218, Val RMSE: 0.6382, Time: 5.07s\n",
      "Trial 49, Epoch 71/218, Val RMSE: 0.6415, Time: 5.05s\n",
      "Trial 49, Epoch 72/218, Val RMSE: 0.6421, Time: 5.10s\n",
      "Trial 49, Epoch 73/218, Val RMSE: 0.6361, Time: 5.11s\n",
      "Trial 49, Epoch 74/218, Val RMSE: 0.6448, Time: 5.00s\n",
      "Trial 49, Epoch 75/218, Val RMSE: 0.6411, Time: 5.10s\n",
      "Trial 49, Epoch 76/218, Val RMSE: 0.6389, Time: 5.16s\n",
      "Trial 49, Epoch 77/218, Val RMSE: 0.6476, Time: 5.15s\n",
      "Trial 49, Epoch 78/218, Val RMSE: 0.6411, Time: 5.04s\n",
      "Trial 49, Epoch 79/218, Val RMSE: 0.6386, Time: 5.03s\n",
      "Trial 49, Epoch 80/218, Val RMSE: 0.6411, Time: 5.02s\n",
      "Trial 49, Epoch 81/218, Val RMSE: 0.6484, Time: 4.98s\n",
      "Trial 49, Epoch 82/218, Val RMSE: 0.6416, Time: 5.01s\n",
      "Trial 49, Epoch 83/218, Val RMSE: 0.6408, Time: 5.00s\n",
      "Trial 49, Epoch 84/218, Val RMSE: 0.6419, Time: 5.18s\n",
      "Trial 49, Epoch 85/218, Val RMSE: 0.6362, Time: 5.13s\n",
      "Trial 49, Epoch 86/218, Val RMSE: 0.6420, Time: 5.09s\n",
      "Trial 49, Epoch 87/218, Val RMSE: 0.6449, Time: 5.01s\n",
      "Trial 49, Epoch 88/218, Val RMSE: 0.6428, Time: 5.06s\n",
      "Trial 49, Epoch 89/218, Val RMSE: 0.6422, Time: 5.05s\n",
      "Trial 49, Epoch 90/218, Val RMSE: 0.6356, Time: 5.18s\n",
      "Trial 49, Epoch 91/218, Val RMSE: 0.6401, Time: 5.26s\n",
      "Trial 49, Epoch 92/218, Val RMSE: 0.6416, Time: 5.06s\n",
      "Trial 49, Epoch 93/218, Val RMSE: 0.6390, Time: 5.04s\n",
      "Trial 49, Epoch 94/218, Val RMSE: 0.6391, Time: 5.10s\n",
      "Trial 49, Epoch 95/218, Val RMSE: 0.6458, Time: 5.04s\n",
      "Trial 49, Epoch 96/218, Val RMSE: 0.6415, Time: 5.09s\n",
      "Trial 49, Epoch 97/218, Val RMSE: 0.6393, Time: 5.08s\n",
      "Trial 49, Epoch 98/218, Val RMSE: 0.6407, Time: 5.05s\n",
      "Trial 49, Epoch 99/218, Val RMSE: 0.6371, Time: 5.05s\n",
      "Trial 49, Epoch 100/218, Val RMSE: 0.6406, Time: 4.98s\n",
      "Trial 49, Epoch 101/218, Val RMSE: 0.6445, Time: 5.09s\n",
      "Trial 49, Epoch 102/218, Val RMSE: 0.6447, Time: 5.01s\n",
      "Trial 49, Epoch 103/218, Val RMSE: 0.6491, Time: 5.10s\n",
      "Trial 49, Epoch 104/218, Val RMSE: 0.6425, Time: 5.11s\n",
      "Trial 49, Epoch 105/218, Val RMSE: 0.6401, Time: 5.11s\n",
      "Trial 49, Epoch 106/218, Val RMSE: 0.6414, Time: 5.08s\n",
      "Trial 49, Epoch 107/218, Val RMSE: 0.6409, Time: 5.10s\n",
      "Trial 49, Epoch 108/218, Val RMSE: 0.6399, Time: 5.13s\n",
      "Trial 49, Epoch 109/218, Val RMSE: 0.6426, Time: 5.05s\n",
      "Early stopping at epoch 109 for trial 49\n",
      "[I 2025-07-16 22:08:32,925] Trial 49 finished with value: 0.642565308993761 and parameters: {'hidden_channels': 566, 'lr': 8.418276978344444e-05, 'batch_size': 64, 'n_epochs': 218, 'num_layers': 1, 'dropout_rate': 0.36922770289428564, 'weight_decay': 2.0318431744447552e-07}. Best is trial 49 with value: 0.642565308993761.\n",
      "Trial 50, Epoch 1/208, Val RMSE: 0.8010, Time: 5.08s\n",
      "Trial 50, Epoch 2/208, Val RMSE: 0.7697, Time: 5.15s\n",
      "Trial 50, Epoch 3/208, Val RMSE: 0.7220, Time: 5.05s\n",
      "Trial 50, Epoch 4/208, Val RMSE: 0.7260, Time: 5.11s\n",
      "Trial 50, Epoch 5/208, Val RMSE: 0.6902, Time: 5.06s\n",
      "Trial 50, Epoch 6/208, Val RMSE: 0.6901, Time: 5.00s\n",
      "Trial 50, Epoch 7/208, Val RMSE: 0.6701, Time: 5.09s\n",
      "Trial 50, Epoch 8/208, Val RMSE: 0.6681, Time: 5.00s\n",
      "Trial 50, Epoch 9/208, Val RMSE: 0.6886, Time: 5.21s\n",
      "Trial 50, Epoch 10/208, Val RMSE: 0.6550, Time: 5.23s\n",
      "Trial 50, Epoch 11/208, Val RMSE: 0.6566, Time: 5.20s\n",
      "Trial 50, Epoch 12/208, Val RMSE: 0.6573, Time: 5.13s\n",
      "Trial 50, Epoch 13/208, Val RMSE: 0.6570, Time: 5.06s\n",
      "Trial 50, Epoch 14/208, Val RMSE: 0.6458, Time: 5.14s\n",
      "Trial 50, Epoch 15/208, Val RMSE: 0.6501, Time: 5.13s\n",
      "Trial 50, Epoch 16/208, Val RMSE: 0.6471, Time: 5.06s\n",
      "Trial 50, Epoch 17/208, Val RMSE: 0.6498, Time: 5.08s\n",
      "Trial 50, Epoch 18/208, Val RMSE: 0.6646, Time: 4.99s\n",
      "Trial 50, Epoch 19/208, Val RMSE: 0.6500, Time: 5.15s\n",
      "Trial 50, Epoch 20/208, Val RMSE: 0.6511, Time: 5.12s\n",
      "Trial 50, Epoch 21/208, Val RMSE: 0.6557, Time: 5.10s\n",
      "Trial 50, Epoch 22/208, Val RMSE: 0.6494, Time: 5.11s\n",
      "Trial 50, Epoch 23/208, Val RMSE: 0.6550, Time: 5.06s\n",
      "Trial 50, Epoch 24/208, Val RMSE: 0.6491, Time: 5.10s\n",
      "Trial 50, Epoch 25/208, Val RMSE: 0.6569, Time: 5.13s\n",
      "Trial 50, Epoch 26/208, Val RMSE: 0.6554, Time: 5.23s\n",
      "Trial 50, Epoch 27/208, Val RMSE: 0.6499, Time: 5.29s\n",
      "Trial 50, Epoch 28/208, Val RMSE: 0.6413, Time: 5.14s\n",
      "Trial 50, Epoch 29/208, Val RMSE: 0.6474, Time: 5.13s\n",
      "Trial 50, Epoch 30/208, Val RMSE: 0.6545, Time: 5.09s\n",
      "Trial 50, Epoch 31/208, Val RMSE: 0.6469, Time: 5.15s\n",
      "Trial 50, Epoch 32/208, Val RMSE: 0.6434, Time: 5.20s\n",
      "Trial 50, Epoch 33/208, Val RMSE: 0.6425, Time: 5.29s\n",
      "Trial 50, Epoch 34/208, Val RMSE: 0.6427, Time: 5.13s\n",
      "Trial 50, Epoch 35/208, Val RMSE: 0.6442, Time: 5.06s\n",
      "Trial 50, Epoch 36/208, Val RMSE: 0.6524, Time: 5.17s\n",
      "Trial 50, Epoch 37/208, Val RMSE: 0.6431, Time: 5.11s\n",
      "Trial 50, Epoch 38/208, Val RMSE: 0.6443, Time: 5.14s\n",
      "Trial 50, Epoch 39/208, Val RMSE: 0.6447, Time: 5.11s\n",
      "Trial 50, Epoch 40/208, Val RMSE: 0.6501, Time: 5.00s\n",
      "Trial 50, Epoch 41/208, Val RMSE: 0.6442, Time: 5.06s\n",
      "Trial 50, Epoch 42/208, Val RMSE: 0.6433, Time: 5.03s\n",
      "Trial 50, Epoch 43/208, Val RMSE: 0.6509, Time: 5.08s\n",
      "Trial 50, Epoch 44/208, Val RMSE: 0.6457, Time: 5.00s\n",
      "Trial 50, Epoch 45/208, Val RMSE: 0.6412, Time: 4.97s\n",
      "Trial 50, Epoch 46/208, Val RMSE: 0.6415, Time: 5.09s\n",
      "Trial 50, Epoch 47/208, Val RMSE: 0.6421, Time: 5.02s\n",
      "Trial 50, Epoch 48/208, Val RMSE: 0.6403, Time: 5.07s\n",
      "Trial 50, Epoch 49/208, Val RMSE: 0.6370, Time: 5.27s\n",
      "Trial 50, Epoch 50/208, Val RMSE: 0.6524, Time: 5.12s\n",
      "Trial 50, Epoch 51/208, Val RMSE: 0.6444, Time: 5.04s\n",
      "Trial 50, Epoch 52/208, Val RMSE: 0.6398, Time: 5.14s\n",
      "Trial 50, Epoch 53/208, Val RMSE: 0.6408, Time: 5.11s\n",
      "Trial 50, Epoch 54/208, Val RMSE: 0.6447, Time: 5.09s\n",
      "Trial 50, Epoch 55/208, Val RMSE: 0.6481, Time: 5.10s\n",
      "Trial 50, Epoch 56/208, Val RMSE: 0.6397, Time: 5.13s\n",
      "Trial 50, Epoch 57/208, Val RMSE: 0.6468, Time: 5.19s\n",
      "Trial 50, Epoch 58/208, Val RMSE: 0.6361, Time: 5.07s\n",
      "Trial 50, Epoch 59/208, Val RMSE: 0.6439, Time: 5.05s\n",
      "Trial 50, Epoch 60/208, Val RMSE: 0.6363, Time: 5.08s\n",
      "Trial 50, Epoch 61/208, Val RMSE: 0.6477, Time: 5.14s\n",
      "Trial 50, Epoch 62/208, Val RMSE: 0.6481, Time: 5.14s\n",
      "Trial 50, Epoch 63/208, Val RMSE: 0.6400, Time: 5.11s\n",
      "Trial 50, Epoch 64/208, Val RMSE: 0.6434, Time: 5.00s\n",
      "Trial 50, Epoch 65/208, Val RMSE: 0.6384, Time: 5.10s\n",
      "Trial 50, Epoch 66/208, Val RMSE: 0.6407, Time: 5.02s\n",
      "Trial 50, Epoch 67/208, Val RMSE: 0.6447, Time: 5.03s\n",
      "Trial 50, Epoch 68/208, Val RMSE: 0.6397, Time: 5.12s\n",
      "Trial 50, Epoch 69/208, Val RMSE: 0.6453, Time: 5.06s\n",
      "Trial 50, Epoch 70/208, Val RMSE: 0.6461, Time: 5.18s\n",
      "Trial 50, Epoch 71/208, Val RMSE: 0.6406, Time: 5.16s\n",
      "Trial 50, Epoch 72/208, Val RMSE: 0.6456, Time: 5.05s\n",
      "Trial 50, Epoch 73/208, Val RMSE: 0.6451, Time: 5.21s\n",
      "Trial 50, Epoch 74/208, Val RMSE: 0.6444, Time: 5.06s\n",
      "Trial 50, Epoch 75/208, Val RMSE: 0.6424, Time: 5.02s\n",
      "Trial 50, Epoch 76/208, Val RMSE: 0.6430, Time: 5.11s\n",
      "Trial 50, Epoch 77/208, Val RMSE: 0.6410, Time: 5.06s\n",
      "Trial 50, Epoch 78/208, Val RMSE: 0.6392, Time: 5.14s\n",
      "Trial 50, Epoch 79/208, Val RMSE: 0.6396, Time: 5.15s\n",
      "Trial 50, Epoch 80/208, Val RMSE: 0.6460, Time: 5.17s\n",
      "Trial 50, Epoch 81/208, Val RMSE: 0.6391, Time: 5.04s\n",
      "Trial 50, Epoch 82/208, Val RMSE: 0.6367, Time: 5.13s\n",
      "Trial 50, Epoch 83/208, Val RMSE: 0.6444, Time: 5.15s\n",
      "Trial 50, Epoch 84/208, Val RMSE: 0.6381, Time: 5.27s\n",
      "Trial 50, Epoch 85/208, Val RMSE: 0.6387, Time: 5.16s\n",
      "Trial 50, Epoch 86/208, Val RMSE: 0.6449, Time: 5.00s\n",
      "Trial 50, Epoch 87/208, Val RMSE: 0.6437, Time: 5.04s\n",
      "Trial 50, Epoch 88/208, Val RMSE: 0.6437, Time: 5.13s\n",
      "Trial 50, Epoch 89/208, Val RMSE: 0.6407, Time: 5.06s\n",
      "Trial 50, Epoch 90/208, Val RMSE: 0.6376, Time: 5.25s\n",
      "[I 2025-07-16 22:16:14,811] Trial 50 pruned. \n",
      "Trial 51, Epoch 1/223, Val RMSE: 0.7932, Time: 5.29s\n",
      "Trial 51, Epoch 2/223, Val RMSE: 0.7387, Time: 5.24s\n",
      "Trial 51, Epoch 3/223, Val RMSE: 0.7192, Time: 5.00s\n",
      "Trial 51, Epoch 4/223, Val RMSE: 0.7072, Time: 5.12s\n",
      "Trial 51, Epoch 5/223, Val RMSE: 0.6951, Time: 5.18s\n",
      "Trial 51, Epoch 6/223, Val RMSE: 0.6812, Time: 5.13s\n",
      "Trial 51, Epoch 7/223, Val RMSE: 0.6634, Time: 5.10s\n",
      "Trial 51, Epoch 8/223, Val RMSE: 0.6751, Time: 5.18s\n",
      "Trial 51, Epoch 9/223, Val RMSE: 0.6683, Time: 5.20s\n",
      "Trial 51, Epoch 10/223, Val RMSE: 0.6666, Time: 5.09s\n",
      "Trial 51, Epoch 11/223, Val RMSE: 0.6568, Time: 5.01s\n",
      "Trial 51, Epoch 12/223, Val RMSE: 0.6604, Time: 5.13s\n",
      "Trial 51, Epoch 13/223, Val RMSE: 0.6523, Time: 5.14s\n",
      "Trial 51, Epoch 14/223, Val RMSE: 0.6633, Time: 5.04s\n",
      "Trial 51, Epoch 15/223, Val RMSE: 0.6500, Time: 5.25s\n",
      "Trial 51, Epoch 16/223, Val RMSE: 0.6598, Time: 5.16s\n",
      "Trial 51, Epoch 17/223, Val RMSE: 0.6526, Time: 5.17s\n",
      "Trial 51, Epoch 18/223, Val RMSE: 0.6630, Time: 5.18s\n",
      "Trial 51, Epoch 19/223, Val RMSE: 0.6524, Time: 5.06s\n",
      "Trial 51, Epoch 20/223, Val RMSE: 0.6501, Time: 4.93s\n",
      "Trial 51, Epoch 21/223, Val RMSE: 0.6444, Time: 5.09s\n",
      "Trial 51, Epoch 22/223, Val RMSE: 0.6515, Time: 5.06s\n",
      "Trial 51, Epoch 23/223, Val RMSE: 0.6465, Time: 5.04s\n",
      "Trial 51, Epoch 24/223, Val RMSE: 0.6440, Time: 5.13s\n",
      "Trial 51, Epoch 25/223, Val RMSE: 0.6472, Time: 5.09s\n",
      "Trial 51, Epoch 26/223, Val RMSE: 0.6537, Time: 5.04s\n",
      "Trial 51, Epoch 27/223, Val RMSE: 0.6410, Time: 5.11s\n",
      "Trial 51, Epoch 28/223, Val RMSE: 0.6487, Time: 5.09s\n",
      "Trial 51, Epoch 29/223, Val RMSE: 0.6508, Time: 5.10s\n",
      "Trial 51, Epoch 30/223, Val RMSE: 0.6498, Time: 5.06s\n",
      "Trial 51, Epoch 31/223, Val RMSE: 0.6502, Time: 5.07s\n",
      "Trial 51, Epoch 32/223, Val RMSE: 0.6450, Time: 5.15s\n",
      "Trial 51, Epoch 33/223, Val RMSE: 0.6452, Time: 5.03s\n",
      "Trial 51, Epoch 34/223, Val RMSE: 0.6470, Time: 5.08s\n",
      "Trial 51, Epoch 35/223, Val RMSE: 0.6510, Time: 5.07s\n",
      "Trial 51, Epoch 36/223, Val RMSE: 0.6475, Time: 5.10s\n",
      "Trial 51, Epoch 37/223, Val RMSE: 0.6517, Time: 5.07s\n",
      "Trial 51, Epoch 38/223, Val RMSE: 0.6430, Time: 5.08s\n",
      "Trial 51, Epoch 39/223, Val RMSE: 0.6517, Time: 5.13s\n",
      "Trial 51, Epoch 40/223, Val RMSE: 0.6530, Time: 5.10s\n",
      "Trial 51, Epoch 41/223, Val RMSE: 0.6492, Time: 5.14s\n",
      "Trial 51, Epoch 42/223, Val RMSE: 0.6474, Time: 5.08s\n",
      "Trial 51, Epoch 43/223, Val RMSE: 0.6554, Time: 5.11s\n",
      "Trial 51, Epoch 44/223, Val RMSE: 0.6495, Time: 5.09s\n",
      "Trial 51, Epoch 45/223, Val RMSE: 0.6530, Time: 5.11s\n",
      "Trial 51, Epoch 46/223, Val RMSE: 0.6486, Time: 5.02s\n",
      "Trial 51, Epoch 47/223, Val RMSE: 0.6475, Time: 5.18s\n",
      "Trial 51, Epoch 48/223, Val RMSE: 0.6466, Time: 5.18s\n",
      "Trial 51, Epoch 49/223, Val RMSE: 0.6512, Time: 5.10s\n",
      "Trial 51, Epoch 50/223, Val RMSE: 0.6487, Time: 5.14s\n",
      "Trial 51, Epoch 51/223, Val RMSE: 0.6491, Time: 5.10s\n",
      "Trial 51, Epoch 52/223, Val RMSE: 0.6467, Time: 5.07s\n",
      "Trial 51, Epoch 53/223, Val RMSE: 0.6467, Time: 5.15s\n",
      "Trial 51, Epoch 54/223, Val RMSE: 0.6522, Time: 5.09s\n",
      "Trial 51, Epoch 55/223, Val RMSE: 0.6478, Time: 5.10s\n",
      "Trial 51, Epoch 56/223, Val RMSE: 0.6572, Time: 5.16s\n",
      "Trial 51, Epoch 57/223, Val RMSE: 0.6446, Time: 5.23s\n",
      "Trial 51, Epoch 58/223, Val RMSE: 0.6498, Time: 5.11s\n",
      "Trial 51, Epoch 59/223, Val RMSE: 0.6458, Time: 5.09s\n",
      "Trial 51, Epoch 60/223, Val RMSE: 0.6481, Time: 5.28s\n",
      "Trial 51, Epoch 61/223, Val RMSE: 0.6448, Time: 5.07s\n",
      "Trial 51, Epoch 62/223, Val RMSE: 0.6528, Time: 5.04s\n",
      "Trial 51, Epoch 63/223, Val RMSE: 0.6421, Time: 5.20s\n",
      "Trial 51, Epoch 64/223, Val RMSE: 0.6462, Time: 5.09s\n",
      "Trial 51, Epoch 65/223, Val RMSE: 0.6459, Time: 5.11s\n",
      "Trial 51, Epoch 66/223, Val RMSE: 0.6476, Time: 5.20s\n",
      "Trial 51, Epoch 67/223, Val RMSE: 0.6518, Time: 5.10s\n",
      "Trial 51, Epoch 68/223, Val RMSE: 0.6450, Time: 5.25s\n",
      "Trial 51, Epoch 69/223, Val RMSE: 0.6508, Time: 5.06s\n",
      "Trial 51, Epoch 70/223, Val RMSE: 0.6460, Time: 5.04s\n",
      "Trial 51, Epoch 71/223, Val RMSE: 0.6474, Time: 5.19s\n",
      "Trial 51, Epoch 72/223, Val RMSE: 0.6542, Time: 5.09s\n",
      "Trial 51, Epoch 73/223, Val RMSE: 0.6493, Time: 5.20s\n",
      "Trial 51, Epoch 74/223, Val RMSE: 0.6434, Time: 5.13s\n",
      "Trial 51, Epoch 75/223, Val RMSE: 0.6456, Time: 5.11s\n",
      "Trial 51, Epoch 76/223, Val RMSE: 0.6482, Time: 5.37s\n",
      "Trial 51, Epoch 77/223, Val RMSE: 0.6521, Time: 5.05s\n",
      "Early stopping at epoch 77 for trial 51\n",
      "[I 2025-07-16 22:22:51,333] Trial 51 finished with value: 0.6521096903193745 and parameters: {'hidden_channels': 549, 'lr': 7.849123110586537e-05, 'batch_size': 64, 'n_epochs': 223, 'num_layers': 1, 'dropout_rate': 0.3154385220463447, 'weight_decay': 1.861749140237751e-07}. Best is trial 49 with value: 0.642565308993761.\n",
      "Trial 52, Epoch 1/216, Val RMSE: 0.8478, Time: 5.12s\n",
      "Trial 52, Epoch 2/216, Val RMSE: 0.7476, Time: 5.31s\n",
      "Trial 52, Epoch 3/216, Val RMSE: 0.7288, Time: 5.20s\n",
      "Trial 52, Epoch 4/216, Val RMSE: 0.7077, Time: 5.15s\n",
      "Trial 52, Epoch 5/216, Val RMSE: 0.6972, Time: 5.18s\n",
      "Trial 52, Epoch 6/216, Val RMSE: 0.6826, Time: 5.20s\n",
      "Trial 52, Epoch 7/216, Val RMSE: 0.6729, Time: 5.22s\n",
      "Trial 52, Epoch 8/216, Val RMSE: 0.6802, Time: 5.23s\n",
      "Trial 52, Epoch 9/216, Val RMSE: 0.6653, Time: 5.14s\n",
      "Trial 52, Epoch 10/216, Val RMSE: 0.6535, Time: 5.17s\n",
      "Trial 52, Epoch 11/216, Val RMSE: 0.6525, Time: 5.18s\n",
      "Trial 52, Epoch 12/216, Val RMSE: 0.6576, Time: 5.25s\n",
      "Trial 52, Epoch 13/216, Val RMSE: 0.6484, Time: 5.16s\n",
      "Trial 52, Epoch 14/216, Val RMSE: 0.6537, Time: 5.12s\n",
      "Trial 52, Epoch 15/216, Val RMSE: 0.6409, Time: 5.19s\n",
      "Trial 52, Epoch 16/216, Val RMSE: 0.6546, Time: 5.08s\n",
      "Trial 52, Epoch 17/216, Val RMSE: 0.6469, Time: 5.25s\n",
      "Trial 52, Epoch 18/216, Val RMSE: 0.6413, Time: 5.24s\n",
      "Trial 52, Epoch 19/216, Val RMSE: 0.6408, Time: 5.23s\n",
      "Trial 52, Epoch 20/216, Val RMSE: 0.6481, Time: 5.11s\n",
      "Trial 52, Epoch 21/216, Val RMSE: 0.6413, Time: 5.07s\n",
      "Trial 52, Epoch 22/216, Val RMSE: 0.6425, Time: 5.23s\n",
      "Trial 52, Epoch 23/216, Val RMSE: 0.6553, Time: 5.21s\n",
      "Trial 52, Epoch 24/216, Val RMSE: 0.6410, Time: 5.19s\n",
      "Trial 52, Epoch 25/216, Val RMSE: 0.6431, Time: 5.27s\n",
      "Trial 52, Epoch 26/216, Val RMSE: 0.6476, Time: 5.15s\n",
      "Trial 52, Epoch 27/216, Val RMSE: 0.6395, Time: 5.12s\n",
      "Trial 52, Epoch 28/216, Val RMSE: 0.6452, Time: 5.12s\n",
      "Trial 52, Epoch 29/216, Val RMSE: 0.6485, Time: 5.17s\n",
      "Trial 52, Epoch 30/216, Val RMSE: 0.6463, Time: 5.01s\n",
      "Trial 52, Epoch 31/216, Val RMSE: 0.6440, Time: 5.06s\n",
      "Trial 52, Epoch 32/216, Val RMSE: 0.6445, Time: 5.11s\n",
      "Trial 52, Epoch 33/216, Val RMSE: 0.6407, Time: 5.10s\n",
      "Trial 52, Epoch 34/216, Val RMSE: 0.6480, Time: 5.07s\n",
      "Trial 52, Epoch 35/216, Val RMSE: 0.6492, Time: 5.08s\n",
      "Trial 52, Epoch 36/216, Val RMSE: 0.6458, Time: 5.09s\n",
      "Trial 52, Epoch 37/216, Val RMSE: 0.6476, Time: 5.02s\n",
      "Trial 52, Epoch 38/216, Val RMSE: 0.6472, Time: 5.19s\n",
      "Trial 52, Epoch 39/216, Val RMSE: 0.6462, Time: 5.26s\n",
      "Trial 52, Epoch 40/216, Val RMSE: 0.6418, Time: 5.11s\n",
      "Trial 52, Epoch 41/216, Val RMSE: 0.6451, Time: 5.16s\n",
      "Trial 52, Epoch 42/216, Val RMSE: 0.6424, Time: 5.04s\n",
      "Trial 52, Epoch 43/216, Val RMSE: 0.6378, Time: 5.11s\n",
      "Trial 52, Epoch 44/216, Val RMSE: 0.6488, Time: 5.00s\n",
      "Trial 52, Epoch 45/216, Val RMSE: 0.6493, Time: 5.18s\n",
      "Trial 52, Epoch 46/216, Val RMSE: 0.6444, Time: 5.10s\n",
      "Trial 52, Epoch 47/216, Val RMSE: 0.6429, Time: 5.01s\n",
      "Trial 52, Epoch 48/216, Val RMSE: 0.6454, Time: 5.07s\n",
      "Trial 52, Epoch 49/216, Val RMSE: 0.6495, Time: 5.15s\n",
      "Trial 52, Epoch 50/216, Val RMSE: 0.6478, Time: 5.40s\n",
      "Trial 52, Epoch 51/216, Val RMSE: 0.6377, Time: 5.17s\n",
      "Trial 52, Epoch 52/216, Val RMSE: 0.6414, Time: 5.23s\n",
      "Trial 52, Epoch 53/216, Val RMSE: 0.6474, Time: 5.14s\n",
      "Trial 52, Epoch 54/216, Val RMSE: 0.6399, Time: 5.18s\n",
      "Trial 52, Epoch 55/216, Val RMSE: 0.6399, Time: 5.22s\n",
      "Trial 52, Epoch 56/216, Val RMSE: 0.6406, Time: 5.13s\n",
      "Trial 52, Epoch 57/216, Val RMSE: 0.6426, Time: 5.11s\n",
      "Trial 52, Epoch 58/216, Val RMSE: 0.6467, Time: 5.10s\n",
      "Trial 52, Epoch 59/216, Val RMSE: 0.6491, Time: 5.08s\n",
      "Trial 52, Epoch 60/216, Val RMSE: 0.6440, Time: 5.11s\n",
      "Trial 52, Epoch 61/216, Val RMSE: 0.6436, Time: 5.18s\n",
      "Trial 52, Epoch 62/216, Val RMSE: 0.6483, Time: 5.16s\n",
      "Trial 52, Epoch 63/216, Val RMSE: 0.6451, Time: 5.13s\n",
      "Trial 52, Epoch 64/216, Val RMSE: 0.6447, Time: 5.12s\n",
      "Trial 52, Epoch 65/216, Val RMSE: 0.6440, Time: 5.08s\n",
      "Trial 52, Epoch 66/216, Val RMSE: 0.6380, Time: 5.09s\n",
      "Trial 52, Epoch 67/216, Val RMSE: 0.6474, Time: 5.08s\n",
      "Trial 52, Epoch 68/216, Val RMSE: 0.6422, Time: 5.08s\n",
      "Trial 52, Epoch 69/216, Val RMSE: 0.6423, Time: 5.04s\n",
      "Trial 52, Epoch 70/216, Val RMSE: 0.6464, Time: 5.04s\n",
      "Trial 52, Epoch 71/216, Val RMSE: 0.6445, Time: 5.06s\n",
      "Trial 52, Epoch 72/216, Val RMSE: 0.6515, Time: 5.10s\n",
      "Trial 52, Epoch 73/216, Val RMSE: 0.6456, Time: 5.02s\n",
      "Trial 52, Epoch 74/216, Val RMSE: 0.6478, Time: 5.21s\n",
      "Trial 52, Epoch 75/216, Val RMSE: 0.6501, Time: 5.06s\n",
      "Trial 52, Epoch 76/216, Val RMSE: 0.6455, Time: 5.12s\n",
      "Trial 52, Epoch 77/216, Val RMSE: 0.6410, Time: 5.08s\n",
      "Trial 52, Epoch 78/216, Val RMSE: 0.6425, Time: 5.10s\n",
      "Trial 52, Epoch 79/216, Val RMSE: 0.6424, Time: 5.10s\n",
      "Trial 52, Epoch 80/216, Val RMSE: 0.6440, Time: 5.11s\n",
      "Trial 52, Epoch 81/216, Val RMSE: 0.6476, Time: 5.10s\n",
      "Trial 52, Epoch 82/216, Val RMSE: 0.6470, Time: 5.18s\n",
      "Trial 52, Epoch 83/216, Val RMSE: 0.6410, Time: 5.09s\n",
      "Trial 52, Epoch 84/216, Val RMSE: 0.6447, Time: 5.15s\n",
      "Trial 52, Epoch 85/216, Val RMSE: 0.6453, Time: 5.15s\n",
      "Trial 52, Epoch 86/216, Val RMSE: 0.6472, Time: 5.05s\n",
      "Trial 52, Epoch 87/216, Val RMSE: 0.6454, Time: 5.16s\n",
      "Trial 52, Epoch 88/216, Val RMSE: 0.6418, Time: 5.08s\n",
      "Trial 52, Epoch 89/216, Val RMSE: 0.6494, Time: 5.13s\n",
      "Trial 52, Epoch 90/216, Val RMSE: 0.6504, Time: 5.24s\n",
      "[I 2025-07-16 22:30:35,803] Trial 52 pruned. \n",
      "Trial 53, Epoch 1/195, Val RMSE: 0.8347, Time: 3.32s\n",
      "Trial 53, Epoch 2/195, Val RMSE: 0.7500, Time: 3.33s\n",
      "Trial 53, Epoch 3/195, Val RMSE: 0.7195, Time: 3.28s\n",
      "Trial 53, Epoch 4/195, Val RMSE: 0.6980, Time: 3.28s\n",
      "Trial 53, Epoch 5/195, Val RMSE: 0.6869, Time: 3.31s\n",
      "Trial 53, Epoch 6/195, Val RMSE: 0.6748, Time: 3.32s\n",
      "Trial 53, Epoch 7/195, Val RMSE: 0.6597, Time: 3.29s\n",
      "Trial 53, Epoch 8/195, Val RMSE: 0.6605, Time: 3.36s\n",
      "Trial 53, Epoch 9/195, Val RMSE: 0.6610, Time: 3.28s\n",
      "Trial 53, Epoch 10/195, Val RMSE: 0.6523, Time: 3.27s\n",
      "Trial 53, Epoch 11/195, Val RMSE: 0.6492, Time: 3.37s\n",
      "Trial 53, Epoch 12/195, Val RMSE: 0.6564, Time: 3.33s\n",
      "Trial 53, Epoch 13/195, Val RMSE: 0.6489, Time: 3.57s\n",
      "Trial 53, Epoch 14/195, Val RMSE: 0.6402, Time: 3.25s\n",
      "Trial 53, Epoch 15/195, Val RMSE: 0.6476, Time: 3.38s\n",
      "Trial 53, Epoch 16/195, Val RMSE: 0.6507, Time: 3.30s\n",
      "Trial 53, Epoch 17/195, Val RMSE: 0.6379, Time: 3.28s\n",
      "Trial 53, Epoch 18/195, Val RMSE: 0.6434, Time: 3.33s\n",
      "Trial 53, Epoch 19/195, Val RMSE: 0.6446, Time: 3.34s\n",
      "Trial 53, Epoch 20/195, Val RMSE: 0.6485, Time: 3.42s\n",
      "Trial 53, Epoch 21/195, Val RMSE: 0.6405, Time: 3.32s\n",
      "Trial 53, Epoch 22/195, Val RMSE: 0.6400, Time: 3.26s\n",
      "Trial 53, Epoch 23/195, Val RMSE: 0.6459, Time: 3.27s\n",
      "Trial 53, Epoch 24/195, Val RMSE: 0.6430, Time: 3.35s\n",
      "Trial 53, Epoch 25/195, Val RMSE: 0.6363, Time: 3.42s\n",
      "Trial 53, Epoch 26/195, Val RMSE: 0.6486, Time: 3.38s\n",
      "Trial 53, Epoch 27/195, Val RMSE: 0.6444, Time: 3.35s\n",
      "Trial 53, Epoch 28/195, Val RMSE: 0.6442, Time: 3.34s\n",
      "Trial 53, Epoch 29/195, Val RMSE: 0.6616, Time: 3.25s\n",
      "Trial 53, Epoch 30/195, Val RMSE: 0.6388, Time: 3.29s\n",
      "Trial 53, Epoch 31/195, Val RMSE: 0.6422, Time: 3.39s\n",
      "Trial 53, Epoch 32/195, Val RMSE: 0.6312, Time: 3.37s\n",
      "Trial 53, Epoch 33/195, Val RMSE: 0.6420, Time: 3.28s\n",
      "Trial 53, Epoch 34/195, Val RMSE: 0.6485, Time: 3.24s\n",
      "Trial 53, Epoch 35/195, Val RMSE: 0.6467, Time: 3.29s\n",
      "Trial 53, Epoch 36/195, Val RMSE: 0.6470, Time: 3.30s\n",
      "Trial 53, Epoch 37/195, Val RMSE: 0.6472, Time: 3.36s\n",
      "Trial 53, Epoch 38/195, Val RMSE: 0.6418, Time: 3.30s\n",
      "Trial 53, Epoch 39/195, Val RMSE: 0.6491, Time: 3.39s\n",
      "Trial 53, Epoch 40/195, Val RMSE: 0.6408, Time: 3.32s\n",
      "Trial 53, Epoch 41/195, Val RMSE: 0.6470, Time: 3.25s\n",
      "Trial 53, Epoch 42/195, Val RMSE: 0.6510, Time: 3.35s\n",
      "Trial 53, Epoch 43/195, Val RMSE: 0.6452, Time: 3.35s\n",
      "Trial 53, Epoch 44/195, Val RMSE: 0.6512, Time: 3.38s\n",
      "Trial 53, Epoch 45/195, Val RMSE: 0.6434, Time: 3.39s\n",
      "Trial 53, Epoch 46/195, Val RMSE: 0.6479, Time: 3.37s\n",
      "Trial 53, Epoch 47/195, Val RMSE: 0.6407, Time: 3.38s\n",
      "Trial 53, Epoch 48/195, Val RMSE: 0.6501, Time: 3.29s\n",
      "Trial 53, Epoch 49/195, Val RMSE: 0.6454, Time: 3.39s\n",
      "Trial 53, Epoch 50/195, Val RMSE: 0.6441, Time: 3.33s\n",
      "Trial 53, Epoch 51/195, Val RMSE: 0.6442, Time: 3.39s\n",
      "Trial 53, Epoch 52/195, Val RMSE: 0.6455, Time: 3.40s\n",
      "Trial 53, Epoch 53/195, Val RMSE: 0.6462, Time: 3.36s\n",
      "Trial 53, Epoch 54/195, Val RMSE: 0.6536, Time: 3.29s\n",
      "Trial 53, Epoch 55/195, Val RMSE: 0.6422, Time: 3.34s\n",
      "Trial 53, Epoch 56/195, Val RMSE: 0.6538, Time: 3.39s\n",
      "Trial 53, Epoch 57/195, Val RMSE: 0.6474, Time: 3.34s\n",
      "Trial 53, Epoch 58/195, Val RMSE: 0.6490, Time: 3.27s\n",
      "Trial 53, Epoch 59/195, Val RMSE: 0.6424, Time: 3.36s\n",
      "Trial 53, Epoch 60/195, Val RMSE: 0.6437, Time: 3.24s\n",
      "Trial 53, Epoch 61/195, Val RMSE: 0.6502, Time: 3.32s\n",
      "Trial 53, Epoch 62/195, Val RMSE: 0.6452, Time: 3.33s\n",
      "Trial 53, Epoch 63/195, Val RMSE: 0.6539, Time: 3.32s\n",
      "Trial 53, Epoch 64/195, Val RMSE: 0.6519, Time: 3.36s\n",
      "Trial 53, Epoch 65/195, Val RMSE: 0.6403, Time: 3.35s\n",
      "Trial 53, Epoch 66/195, Val RMSE: 0.6496, Time: 3.30s\n",
      "Trial 53, Epoch 67/195, Val RMSE: 0.6427, Time: 3.28s\n",
      "Trial 53, Epoch 68/195, Val RMSE: 0.6437, Time: 3.33s\n",
      "Trial 53, Epoch 69/195, Val RMSE: 0.6419, Time: 3.29s\n",
      "Trial 53, Epoch 70/195, Val RMSE: 0.6503, Time: 3.31s\n",
      "Trial 53, Epoch 71/195, Val RMSE: 0.6466, Time: 3.27s\n",
      "Trial 53, Epoch 72/195, Val RMSE: 0.6484, Time: 3.31s\n",
      "Trial 53, Epoch 73/195, Val RMSE: 0.6495, Time: 3.36s\n",
      "Trial 53, Epoch 74/195, Val RMSE: 0.6400, Time: 3.33s\n",
      "Trial 53, Epoch 75/195, Val RMSE: 0.6482, Time: 3.35s\n",
      "Trial 53, Epoch 76/195, Val RMSE: 0.6446, Time: 3.31s\n",
      "Trial 53, Epoch 77/195, Val RMSE: 0.6420, Time: 3.38s\n",
      "Trial 53, Epoch 78/195, Val RMSE: 0.6423, Time: 3.39s\n",
      "Trial 53, Epoch 79/195, Val RMSE: 0.6481, Time: 3.42s\n",
      "Trial 53, Epoch 80/195, Val RMSE: 0.6483, Time: 3.33s\n",
      "Trial 53, Epoch 81/195, Val RMSE: 0.6415, Time: 3.37s\n",
      "Trial 53, Epoch 82/195, Val RMSE: 0.6451, Time: 3.32s\n",
      "Early stopping at epoch 82 for trial 53\n",
      "[I 2025-07-16 22:35:11,348] Trial 53 finished with value: 0.6451018253076669 and parameters: {'hidden_channels': 611, 'lr': 7.836229743576408e-05, 'batch_size': 128, 'n_epochs': 195, 'num_layers': 1, 'dropout_rate': 0.21682926142198297, 'weight_decay': 4.411265239751489e-08}. Best is trial 49 with value: 0.642565308993761.\n",
      "Trial 54, Epoch 1/186, Val RMSE: 0.8474, Time: 3.39s\n",
      "Trial 54, Epoch 2/186, Val RMSE: 0.7452, Time: 3.38s\n",
      "Trial 54, Epoch 3/186, Val RMSE: 0.7257, Time: 3.23s\n",
      "Trial 54, Epoch 4/186, Val RMSE: 0.7062, Time: 3.43s\n",
      "Trial 54, Epoch 5/186, Val RMSE: 0.6901, Time: 3.50s\n",
      "Trial 54, Epoch 6/186, Val RMSE: 0.6715, Time: 3.31s\n",
      "Trial 54, Epoch 7/186, Val RMSE: 0.6751, Time: 3.33s\n",
      "Trial 54, Epoch 8/186, Val RMSE: 0.6595, Time: 3.31s\n",
      "Trial 54, Epoch 9/186, Val RMSE: 0.6783, Time: 3.35s\n",
      "Trial 54, Epoch 10/186, Val RMSE: 0.6621, Time: 3.26s\n",
      "Trial 54, Epoch 11/186, Val RMSE: 0.6541, Time: 3.26s\n",
      "Trial 54, Epoch 12/186, Val RMSE: 0.6636, Time: 3.34s\n",
      "Trial 54, Epoch 13/186, Val RMSE: 0.6482, Time: 3.26s\n",
      "Trial 54, Epoch 14/186, Val RMSE: 0.6530, Time: 3.31s\n",
      "Trial 54, Epoch 15/186, Val RMSE: 0.6594, Time: 3.24s\n",
      "Trial 54, Epoch 16/186, Val RMSE: 0.6420, Time: 3.23s\n",
      "Trial 54, Epoch 17/186, Val RMSE: 0.6446, Time: 3.34s\n",
      "Trial 54, Epoch 18/186, Val RMSE: 0.6436, Time: 3.32s\n",
      "Trial 54, Epoch 19/186, Val RMSE: 0.6489, Time: 3.32s\n",
      "Trial 54, Epoch 20/186, Val RMSE: 0.6484, Time: 3.31s\n",
      "Trial 54, Epoch 21/186, Val RMSE: 0.6692, Time: 3.48s\n",
      "Trial 54, Epoch 22/186, Val RMSE: 0.6449, Time: 3.36s\n",
      "Trial 54, Epoch 23/186, Val RMSE: 0.6467, Time: 3.33s\n",
      "Trial 54, Epoch 24/186, Val RMSE: 0.6410, Time: 3.32s\n",
      "Trial 54, Epoch 25/186, Val RMSE: 0.6427, Time: 3.30s\n",
      "Trial 54, Epoch 26/186, Val RMSE: 0.6511, Time: 3.33s\n",
      "Trial 54, Epoch 27/186, Val RMSE: 0.6450, Time: 3.32s\n",
      "Trial 54, Epoch 28/186, Val RMSE: 0.6474, Time: 3.33s\n",
      "Trial 54, Epoch 29/186, Val RMSE: 0.6414, Time: 3.35s\n",
      "Trial 54, Epoch 30/186, Val RMSE: 0.6524, Time: 3.32s\n",
      "Trial 54, Epoch 31/186, Val RMSE: 0.6516, Time: 3.25s\n",
      "Trial 54, Epoch 32/186, Val RMSE: 0.6427, Time: 3.37s\n",
      "Trial 54, Epoch 33/186, Val RMSE: 0.6562, Time: 3.38s\n",
      "Trial 54, Epoch 34/186, Val RMSE: 0.6472, Time: 3.33s\n",
      "Trial 54, Epoch 35/186, Val RMSE: 0.6430, Time: 3.34s\n",
      "Trial 54, Epoch 36/186, Val RMSE: 0.6487, Time: 3.36s\n",
      "Trial 54, Epoch 37/186, Val RMSE: 0.6502, Time: 3.35s\n",
      "Trial 54, Epoch 38/186, Val RMSE: 0.6445, Time: 3.34s\n",
      "Trial 54, Epoch 39/186, Val RMSE: 0.6485, Time: 3.33s\n",
      "Trial 54, Epoch 40/186, Val RMSE: 0.6485, Time: 3.32s\n",
      "Trial 54, Epoch 41/186, Val RMSE: 0.6422, Time: 3.34s\n",
      "Trial 54, Epoch 42/186, Val RMSE: 0.6445, Time: 3.31s\n",
      "Trial 54, Epoch 43/186, Val RMSE: 0.6427, Time: 3.30s\n",
      "Trial 54, Epoch 44/186, Val RMSE: 0.6495, Time: 3.27s\n",
      "Trial 54, Epoch 45/186, Val RMSE: 0.6447, Time: 3.32s\n",
      "Trial 54, Epoch 46/186, Val RMSE: 0.6404, Time: 3.28s\n",
      "Trial 54, Epoch 47/186, Val RMSE: 0.6400, Time: 3.30s\n",
      "Trial 54, Epoch 48/186, Val RMSE: 0.6421, Time: 3.32s\n",
      "Trial 54, Epoch 49/186, Val RMSE: 0.6470, Time: 3.34s\n",
      "Trial 54, Epoch 50/186, Val RMSE: 0.6438, Time: 3.24s\n",
      "Trial 54, Epoch 51/186, Val RMSE: 0.6401, Time: 3.28s\n",
      "Trial 54, Epoch 52/186, Val RMSE: 0.6445, Time: 3.32s\n",
      "Trial 54, Epoch 53/186, Val RMSE: 0.6430, Time: 3.25s\n",
      "Trial 54, Epoch 54/186, Val RMSE: 0.6406, Time: 3.22s\n",
      "Trial 54, Epoch 55/186, Val RMSE: 0.6506, Time: 3.26s\n",
      "Trial 54, Epoch 56/186, Val RMSE: 0.6492, Time: 3.32s\n",
      "Trial 54, Epoch 57/186, Val RMSE: 0.6412, Time: 3.28s\n",
      "Trial 54, Epoch 58/186, Val RMSE: 0.6478, Time: 3.28s\n",
      "Trial 54, Epoch 59/186, Val RMSE: 0.6459, Time: 3.30s\n",
      "Trial 54, Epoch 60/186, Val RMSE: 0.6503, Time: 3.30s\n",
      "Trial 54, Epoch 61/186, Val RMSE: 0.6495, Time: 3.32s\n",
      "Trial 54, Epoch 62/186, Val RMSE: 0.6472, Time: 3.36s\n",
      "Trial 54, Epoch 63/186, Val RMSE: 0.6387, Time: 3.26s\n",
      "Trial 54, Epoch 64/186, Val RMSE: 0.6369, Time: 3.25s\n",
      "Trial 54, Epoch 65/186, Val RMSE: 0.6494, Time: 3.30s\n",
      "Trial 54, Epoch 66/186, Val RMSE: 0.6405, Time: 3.30s\n",
      "Trial 54, Epoch 67/186, Val RMSE: 0.6446, Time: 3.32s\n",
      "Trial 54, Epoch 68/186, Val RMSE: 0.6497, Time: 3.38s\n",
      "Trial 54, Epoch 69/186, Val RMSE: 0.6394, Time: 3.32s\n",
      "Trial 54, Epoch 70/186, Val RMSE: 0.6392, Time: 3.28s\n",
      "Trial 54, Epoch 71/186, Val RMSE: 0.6427, Time: 3.23s\n",
      "Trial 54, Epoch 72/186, Val RMSE: 0.6422, Time: 3.30s\n",
      "Trial 54, Epoch 73/186, Val RMSE: 0.6485, Time: 3.29s\n",
      "Trial 54, Epoch 74/186, Val RMSE: 0.6427, Time: 3.34s\n",
      "Trial 54, Epoch 75/186, Val RMSE: 0.6373, Time: 3.29s\n",
      "Trial 54, Epoch 76/186, Val RMSE: 0.6396, Time: 3.29s\n",
      "Trial 54, Epoch 77/186, Val RMSE: 0.6500, Time: 3.31s\n",
      "Trial 54, Epoch 78/186, Val RMSE: 0.6370, Time: 3.32s\n",
      "Trial 54, Epoch 79/186, Val RMSE: 0.6446, Time: 3.27s\n",
      "Trial 54, Epoch 80/186, Val RMSE: 0.6468, Time: 3.36s\n",
      "Trial 54, Epoch 81/186, Val RMSE: 0.6407, Time: 3.28s\n",
      "Trial 54, Epoch 82/186, Val RMSE: 0.6414, Time: 3.31s\n",
      "Trial 54, Epoch 83/186, Val RMSE: 0.6396, Time: 3.24s\n",
      "Trial 54, Epoch 84/186, Val RMSE: 0.6462, Time: 3.26s\n",
      "Trial 54, Epoch 85/186, Val RMSE: 0.6459, Time: 3.32s\n",
      "Trial 54, Epoch 86/186, Val RMSE: 0.6447, Time: 3.30s\n",
      "Trial 54, Epoch 87/186, Val RMSE: 0.6446, Time: 3.27s\n",
      "Trial 54, Epoch 88/186, Val RMSE: 0.6415, Time: 3.34s\n",
      "Trial 54, Epoch 89/186, Val RMSE: 0.6434, Time: 3.24s\n",
      "Trial 54, Epoch 90/186, Val RMSE: 0.6477, Time: 3.26s\n",
      "[I 2025-07-16 22:40:11,389] Trial 54 pruned. \n",
      "Trial 55, Epoch 1/239, Val RMSE: 0.7982, Time: 3.31s\n",
      "Trial 55, Epoch 2/239, Val RMSE: 0.7958, Time: 3.39s\n",
      "Trial 55, Epoch 3/239, Val RMSE: 0.7225, Time: 3.25s\n",
      "Trial 55, Epoch 4/239, Val RMSE: 0.7088, Time: 3.39s\n",
      "Trial 55, Epoch 5/239, Val RMSE: 0.6869, Time: 3.37s\n",
      "Trial 55, Epoch 6/239, Val RMSE: 0.6834, Time: 3.32s\n",
      "Trial 55, Epoch 7/239, Val RMSE: 0.6710, Time: 3.32s\n",
      "Trial 55, Epoch 8/239, Val RMSE: 0.6612, Time: 3.34s\n",
      "Trial 55, Epoch 9/239, Val RMSE: 0.6571, Time: 3.21s\n",
      "Trial 55, Epoch 10/239, Val RMSE: 0.6574, Time: 3.26s\n",
      "Trial 55, Epoch 11/239, Val RMSE: 0.6626, Time: 3.31s\n",
      "Trial 55, Epoch 12/239, Val RMSE: 0.6563, Time: 3.37s\n",
      "Trial 55, Epoch 13/239, Val RMSE: 0.6639, Time: 3.31s\n",
      "Trial 55, Epoch 14/239, Val RMSE: 0.6476, Time: 3.27s\n",
      "Trial 55, Epoch 15/239, Val RMSE: 0.6382, Time: 3.37s\n",
      "Trial 55, Epoch 16/239, Val RMSE: 0.6336, Time: 3.35s\n",
      "Trial 55, Epoch 17/239, Val RMSE: 0.6607, Time: 3.32s\n",
      "Trial 55, Epoch 18/239, Val RMSE: 0.6562, Time: 3.38s\n",
      "Trial 55, Epoch 19/239, Val RMSE: 0.6490, Time: 3.35s\n",
      "Trial 55, Epoch 20/239, Val RMSE: 0.6455, Time: 3.36s\n",
      "Trial 55, Epoch 21/239, Val RMSE: 0.6471, Time: 3.33s\n",
      "Trial 55, Epoch 22/239, Val RMSE: 0.6484, Time: 3.52s\n",
      "Trial 55, Epoch 23/239, Val RMSE: 0.6561, Time: 3.32s\n",
      "Trial 55, Epoch 24/239, Val RMSE: 0.6375, Time: 3.38s\n",
      "Trial 55, Epoch 25/239, Val RMSE: 0.6360, Time: 3.33s\n",
      "Trial 55, Epoch 26/239, Val RMSE: 0.6434, Time: 3.39s\n",
      "Trial 55, Epoch 27/239, Val RMSE: 0.6551, Time: 3.32s\n",
      "Trial 55, Epoch 28/239, Val RMSE: 0.6293, Time: 3.30s\n",
      "Trial 55, Epoch 29/239, Val RMSE: 0.6413, Time: 3.44s\n",
      "Trial 55, Epoch 30/239, Val RMSE: 0.6342, Time: 3.36s\n",
      "Trial 55, Epoch 31/239, Val RMSE: 0.6402, Time: 3.38s\n",
      "Trial 55, Epoch 32/239, Val RMSE: 0.6429, Time: 3.28s\n",
      "Trial 55, Epoch 33/239, Val RMSE: 0.6362, Time: 3.50s\n",
      "Trial 55, Epoch 34/239, Val RMSE: 0.6336, Time: 3.35s\n",
      "Trial 55, Epoch 35/239, Val RMSE: 0.6441, Time: 3.31s\n",
      "Trial 55, Epoch 36/239, Val RMSE: 0.6376, Time: 3.30s\n",
      "Trial 55, Epoch 37/239, Val RMSE: 0.6399, Time: 3.31s\n",
      "Trial 55, Epoch 38/239, Val RMSE: 0.6311, Time: 3.33s\n",
      "Trial 55, Epoch 39/239, Val RMSE: 0.6446, Time: 3.31s\n",
      "Trial 55, Epoch 40/239, Val RMSE: 0.6387, Time: 3.29s\n",
      "Trial 55, Epoch 41/239, Val RMSE: 0.6445, Time: 3.32s\n",
      "Trial 55, Epoch 42/239, Val RMSE: 0.6335, Time: 3.31s\n",
      "Trial 55, Epoch 43/239, Val RMSE: 0.6325, Time: 3.33s\n",
      "Trial 55, Epoch 44/239, Val RMSE: 0.6356, Time: 3.35s\n",
      "Trial 55, Epoch 45/239, Val RMSE: 0.6380, Time: 3.28s\n",
      "Trial 55, Epoch 46/239, Val RMSE: 0.6360, Time: 3.25s\n",
      "Trial 55, Epoch 47/239, Val RMSE: 0.6326, Time: 3.31s\n",
      "Trial 55, Epoch 48/239, Val RMSE: 0.6316, Time: 3.36s\n",
      "Trial 55, Epoch 49/239, Val RMSE: 0.6326, Time: 3.31s\n",
      "Trial 55, Epoch 50/239, Val RMSE: 0.6274, Time: 3.38s\n",
      "Trial 55, Epoch 51/239, Val RMSE: 0.6313, Time: 3.33s\n",
      "Trial 55, Epoch 52/239, Val RMSE: 0.6370, Time: 3.36s\n",
      "Trial 55, Epoch 53/239, Val RMSE: 0.6375, Time: 3.32s\n",
      "Trial 55, Epoch 54/239, Val RMSE: 0.6363, Time: 3.36s\n",
      "Trial 55, Epoch 55/239, Val RMSE: 0.6408, Time: 3.31s\n",
      "Trial 55, Epoch 56/239, Val RMSE: 0.6344, Time: 3.36s\n",
      "Trial 55, Epoch 57/239, Val RMSE: 0.6435, Time: 3.34s\n",
      "Trial 55, Epoch 58/239, Val RMSE: 0.6363, Time: 3.37s\n",
      "Trial 55, Epoch 59/239, Val RMSE: 0.6311, Time: 3.34s\n",
      "Trial 55, Epoch 60/239, Val RMSE: 0.6316, Time: 3.33s\n",
      "Trial 55, Epoch 61/239, Val RMSE: 0.6338, Time: 3.37s\n",
      "Trial 55, Epoch 62/239, Val RMSE: 0.6332, Time: 3.34s\n",
      "Trial 55, Epoch 63/239, Val RMSE: 0.6374, Time: 3.36s\n",
      "Trial 55, Epoch 64/239, Val RMSE: 0.6314, Time: 3.31s\n",
      "Trial 55, Epoch 65/239, Val RMSE: 0.6266, Time: 3.34s\n",
      "Trial 55, Epoch 66/239, Val RMSE: 0.6398, Time: 3.41s\n",
      "Trial 55, Epoch 67/239, Val RMSE: 0.6490, Time: 3.39s\n",
      "Trial 55, Epoch 68/239, Val RMSE: 0.6341, Time: 3.32s\n",
      "Trial 55, Epoch 69/239, Val RMSE: 0.6301, Time: 3.38s\n",
      "Trial 55, Epoch 70/239, Val RMSE: 0.6360, Time: 3.38s\n",
      "Trial 55, Epoch 71/239, Val RMSE: 0.6362, Time: 3.22s\n",
      "Trial 55, Epoch 72/239, Val RMSE: 0.6375, Time: 3.31s\n",
      "Trial 55, Epoch 73/239, Val RMSE: 0.6410, Time: 3.34s\n",
      "Trial 55, Epoch 74/239, Val RMSE: 0.6295, Time: 3.35s\n",
      "Trial 55, Epoch 75/239, Val RMSE: 0.6348, Time: 3.32s\n",
      "Trial 55, Epoch 76/239, Val RMSE: 0.6381, Time: 3.23s\n",
      "Trial 55, Epoch 77/239, Val RMSE: 0.6286, Time: 3.30s\n",
      "Trial 55, Epoch 78/239, Val RMSE: 0.6343, Time: 3.30s\n",
      "Trial 55, Epoch 79/239, Val RMSE: 0.6280, Time: 3.39s\n",
      "Trial 55, Epoch 80/239, Val RMSE: 0.6374, Time: 3.46s\n",
      "Trial 55, Epoch 81/239, Val RMSE: 0.6317, Time: 3.34s\n",
      "Trial 55, Epoch 82/239, Val RMSE: 0.6285, Time: 3.28s\n",
      "Trial 55, Epoch 83/239, Val RMSE: 0.6325, Time: 3.29s\n",
      "Trial 55, Epoch 84/239, Val RMSE: 0.6286, Time: 3.37s\n",
      "Trial 55, Epoch 85/239, Val RMSE: 0.6300, Time: 3.39s\n",
      "Trial 55, Epoch 86/239, Val RMSE: 0.6362, Time: 3.37s\n",
      "Trial 55, Epoch 87/239, Val RMSE: 0.6312, Time: 3.41s\n",
      "Trial 55, Epoch 88/239, Val RMSE: 0.6326, Time: 3.34s\n",
      "Trial 55, Epoch 89/239, Val RMSE: 0.6314, Time: 3.35s\n",
      "Trial 55, Epoch 90/239, Val RMSE: 0.6337, Time: 3.33s\n",
      "Trial 55, Epoch 91/239, Val RMSE: 0.6338, Time: 3.39s\n",
      "Trial 55, Epoch 92/239, Val RMSE: 0.6326, Time: 3.35s\n",
      "Trial 55, Epoch 93/239, Val RMSE: 0.6421, Time: 3.36s\n",
      "Trial 55, Epoch 94/239, Val RMSE: 0.6341, Time: 3.31s\n",
      "Trial 55, Epoch 95/239, Val RMSE: 0.6282, Time: 3.33s\n",
      "Trial 55, Epoch 96/239, Val RMSE: 0.6320, Time: 3.35s\n",
      "Trial 55, Epoch 97/239, Val RMSE: 0.6306, Time: 3.33s\n",
      "Trial 55, Epoch 98/239, Val RMSE: 0.6343, Time: 3.37s\n",
      "Trial 55, Epoch 99/239, Val RMSE: 0.6255, Time: 3.33s\n",
      "Trial 55, Epoch 100/239, Val RMSE: 0.6306, Time: 3.31s\n",
      "Trial 55, Epoch 101/239, Val RMSE: 0.6358, Time: 3.27s\n",
      "Trial 55, Epoch 102/239, Val RMSE: 0.6285, Time: 3.34s\n",
      "Trial 55, Epoch 103/239, Val RMSE: 0.6289, Time: 3.34s\n",
      "Trial 55, Epoch 104/239, Val RMSE: 0.6328, Time: 3.31s\n",
      "Trial 55, Epoch 105/239, Val RMSE: 0.6367, Time: 3.36s\n",
      "Trial 55, Epoch 106/239, Val RMSE: 0.6325, Time: 3.28s\n",
      "Trial 55, Epoch 107/239, Val RMSE: 0.6315, Time: 3.26s\n",
      "Trial 55, Epoch 108/239, Val RMSE: 0.6316, Time: 3.32s\n",
      "Trial 55, Epoch 109/239, Val RMSE: 0.6302, Time: 3.36s\n",
      "Trial 55, Epoch 110/239, Val RMSE: 0.6293, Time: 3.29s\n",
      "Trial 55, Epoch 111/239, Val RMSE: 0.6343, Time: 3.58s\n",
      "Trial 55, Epoch 112/239, Val RMSE: 0.6353, Time: 3.40s\n",
      "Trial 55, Epoch 113/239, Val RMSE: 0.6321, Time: 3.34s\n",
      "Trial 55, Epoch 114/239, Val RMSE: 0.6319, Time: 3.31s\n",
      "Trial 55, Epoch 115/239, Val RMSE: 0.6340, Time: 3.31s\n",
      "Trial 55, Epoch 116/239, Val RMSE: 0.6341, Time: 3.30s\n",
      "Trial 55, Epoch 117/239, Val RMSE: 0.6321, Time: 3.33s\n",
      "Trial 55, Epoch 118/239, Val RMSE: 0.6305, Time: 3.31s\n",
      "Trial 55, Epoch 119/239, Val RMSE: 0.6346, Time: 3.38s\n",
      "Trial 55, Epoch 120/239, Val RMSE: 0.6323, Time: 3.40s\n",
      "Trial 55, Epoch 121/239, Val RMSE: 0.6304, Time: 3.34s\n",
      "Trial 55, Epoch 122/239, Val RMSE: 0.6335, Time: 3.29s\n",
      "Trial 55, Epoch 123/239, Val RMSE: 0.6280, Time: 3.29s\n",
      "Trial 55, Epoch 124/239, Val RMSE: 0.6383, Time: 3.35s\n",
      "Trial 55, Epoch 125/239, Val RMSE: 0.6401, Time: 3.34s\n",
      "Trial 55, Epoch 126/239, Val RMSE: 0.6319, Time: 3.32s\n",
      "Trial 55, Epoch 127/239, Val RMSE: 0.6311, Time: 3.30s\n",
      "Trial 55, Epoch 128/239, Val RMSE: 0.6348, Time: 3.33s\n",
      "Trial 55, Epoch 129/239, Val RMSE: 0.6344, Time: 3.27s\n",
      "Trial 55, Epoch 130/239, Val RMSE: 0.6306, Time: 3.37s\n",
      "Trial 55, Epoch 131/239, Val RMSE: 0.6322, Time: 3.38s\n",
      "Trial 55, Epoch 132/239, Val RMSE: 0.6324, Time: 3.38s\n",
      "Trial 55, Epoch 133/239, Val RMSE: 0.6321, Time: 3.36s\n",
      "Trial 55, Epoch 134/239, Val RMSE: 0.6310, Time: 3.35s\n",
      "Trial 55, Epoch 135/239, Val RMSE: 0.6353, Time: 3.34s\n",
      "Trial 55, Epoch 136/239, Val RMSE: 0.6394, Time: 3.36s\n",
      "Trial 55, Epoch 137/239, Val RMSE: 0.6344, Time: 3.30s\n",
      "Trial 55, Epoch 138/239, Val RMSE: 0.6370, Time: 3.36s\n",
      "Trial 55, Epoch 139/239, Val RMSE: 0.6342, Time: 3.31s\n",
      "Trial 55, Epoch 140/239, Val RMSE: 0.6334, Time: 3.30s\n",
      "Trial 55, Epoch 141/239, Val RMSE: 0.6326, Time: 3.30s\n",
      "Trial 55, Epoch 142/239, Val RMSE: 0.6367, Time: 3.25s\n",
      "Trial 55, Epoch 143/239, Val RMSE: 0.6443, Time: 3.29s\n",
      "Trial 55, Epoch 144/239, Val RMSE: 0.6318, Time: 3.37s\n",
      "Trial 55, Epoch 145/239, Val RMSE: 0.6344, Time: 3.31s\n",
      "Trial 55, Epoch 146/239, Val RMSE: 0.6339, Time: 3.32s\n",
      "Trial 55, Epoch 147/239, Val RMSE: 0.6331, Time: 3.33s\n",
      "Trial 55, Epoch 148/239, Val RMSE: 0.6361, Time: 3.35s\n",
      "Trial 55, Epoch 149/239, Val RMSE: 0.6308, Time: 3.27s\n",
      "Early stopping at epoch 149 for trial 55\n",
      "[I 2025-07-16 22:48:32,585] Trial 55 finished with value: 0.6307869586175866 and parameters: {'hidden_channels': 844, 'lr': 0.0001359156773333439, 'batch_size': 128, 'n_epochs': 239, 'num_layers': 1, 'dropout_rate': 0.23041135656934808, 'weight_decay': 5.83494253814369e-08}. Best is trial 55 with value: 0.6307869586175866.\n",
      "Trial 56, Epoch 1/245, Val RMSE: 0.8201, Time: 3.35s\n",
      "Trial 56, Epoch 2/245, Val RMSE: 0.7587, Time: 3.33s\n",
      "Trial 56, Epoch 3/245, Val RMSE: 0.7274, Time: 3.30s\n",
      "Trial 56, Epoch 4/245, Val RMSE: 0.7053, Time: 3.29s\n",
      "Trial 56, Epoch 5/245, Val RMSE: 0.6919, Time: 3.33s\n",
      "Trial 56, Epoch 6/245, Val RMSE: 0.6996, Time: 3.28s\n",
      "Trial 56, Epoch 7/245, Val RMSE: 0.6789, Time: 3.36s\n",
      "Trial 56, Epoch 8/245, Val RMSE: 0.6592, Time: 3.36s\n",
      "Trial 56, Epoch 9/245, Val RMSE: 0.6580, Time: 3.41s\n",
      "Trial 56, Epoch 10/245, Val RMSE: 0.6697, Time: 3.40s\n",
      "Trial 56, Epoch 11/245, Val RMSE: 0.6546, Time: 3.36s\n",
      "Trial 56, Epoch 12/245, Val RMSE: 0.6424, Time: 3.35s\n",
      "Trial 56, Epoch 13/245, Val RMSE: 0.6500, Time: 3.36s\n",
      "Trial 56, Epoch 14/245, Val RMSE: 0.6433, Time: 3.38s\n",
      "Trial 56, Epoch 15/245, Val RMSE: 0.6544, Time: 3.44s\n",
      "Trial 56, Epoch 16/245, Val RMSE: 0.6625, Time: 3.34s\n",
      "Trial 56, Epoch 17/245, Val RMSE: 0.6384, Time: 3.28s\n",
      "Trial 56, Epoch 18/245, Val RMSE: 0.6468, Time: 3.35s\n",
      "Trial 56, Epoch 19/245, Val RMSE: 0.6437, Time: 3.38s\n",
      "Trial 56, Epoch 20/245, Val RMSE: 0.6450, Time: 3.27s\n",
      "Trial 56, Epoch 21/245, Val RMSE: 0.6495, Time: 3.38s\n",
      "Trial 56, Epoch 22/245, Val RMSE: 0.6325, Time: 3.35s\n",
      "Trial 56, Epoch 23/245, Val RMSE: 0.6385, Time: 3.37s\n",
      "Trial 56, Epoch 24/245, Val RMSE: 0.6374, Time: 3.38s\n",
      "Trial 56, Epoch 25/245, Val RMSE: 0.6465, Time: 3.38s\n",
      "Trial 56, Epoch 26/245, Val RMSE: 0.6354, Time: 3.35s\n",
      "Trial 56, Epoch 27/245, Val RMSE: 0.6494, Time: 3.28s\n",
      "Trial 56, Epoch 28/245, Val RMSE: 0.6385, Time: 3.27s\n",
      "Trial 56, Epoch 29/245, Val RMSE: 0.6383, Time: 3.41s\n",
      "Trial 56, Epoch 30/245, Val RMSE: 0.6344, Time: 3.33s\n",
      "Trial 56, Epoch 31/245, Val RMSE: 0.6316, Time: 3.34s\n",
      "Trial 56, Epoch 32/245, Val RMSE: 0.6380, Time: 3.32s\n",
      "Trial 56, Epoch 33/245, Val RMSE: 0.6321, Time: 3.30s\n",
      "Trial 56, Epoch 34/245, Val RMSE: 0.6388, Time: 3.28s\n",
      "Trial 56, Epoch 35/245, Val RMSE: 0.6382, Time: 3.27s\n",
      "Trial 56, Epoch 36/245, Val RMSE: 0.6442, Time: 3.29s\n",
      "Trial 56, Epoch 37/245, Val RMSE: 0.6414, Time: 3.31s\n",
      "Trial 56, Epoch 38/245, Val RMSE: 0.6384, Time: 3.31s\n",
      "Trial 56, Epoch 39/245, Val RMSE: 0.6466, Time: 3.30s\n",
      "Trial 56, Epoch 40/245, Val RMSE: 0.6422, Time: 3.33s\n",
      "Trial 56, Epoch 41/245, Val RMSE: 0.6365, Time: 3.30s\n",
      "Trial 56, Epoch 42/245, Val RMSE: 0.6328, Time: 3.29s\n",
      "Trial 56, Epoch 43/245, Val RMSE: 0.6468, Time: 3.27s\n",
      "Trial 56, Epoch 44/245, Val RMSE: 0.6337, Time: 3.30s\n",
      "Trial 56, Epoch 45/245, Val RMSE: 0.6325, Time: 3.40s\n",
      "Trial 56, Epoch 46/245, Val RMSE: 0.6347, Time: 3.32s\n",
      "Trial 56, Epoch 47/245, Val RMSE: 0.6403, Time: 3.28s\n",
      "Trial 56, Epoch 48/245, Val RMSE: 0.6327, Time: 3.34s\n",
      "Trial 56, Epoch 49/245, Val RMSE: 0.6345, Time: 3.33s\n",
      "Trial 56, Epoch 50/245, Val RMSE: 0.6309, Time: 3.51s\n",
      "Trial 56, Epoch 51/245, Val RMSE: 0.6473, Time: 3.33s\n",
      "Trial 56, Epoch 52/245, Val RMSE: 0.6327, Time: 3.30s\n",
      "Trial 56, Epoch 53/245, Val RMSE: 0.6298, Time: 3.36s\n",
      "Trial 56, Epoch 54/245, Val RMSE: 0.6335, Time: 3.32s\n",
      "Trial 56, Epoch 55/245, Val RMSE: 0.6299, Time: 3.26s\n",
      "Trial 56, Epoch 56/245, Val RMSE: 0.6319, Time: 3.36s\n",
      "Trial 56, Epoch 57/245, Val RMSE: 0.6275, Time: 3.51s\n",
      "Trial 56, Epoch 58/245, Val RMSE: 0.6352, Time: 3.48s\n",
      "Trial 56, Epoch 59/245, Val RMSE: 0.6272, Time: 3.43s\n",
      "Trial 56, Epoch 60/245, Val RMSE: 0.6426, Time: 3.40s\n",
      "Trial 56, Epoch 61/245, Val RMSE: 0.6298, Time: 3.35s\n",
      "Trial 56, Epoch 62/245, Val RMSE: 0.6260, Time: 3.32s\n",
      "Trial 56, Epoch 63/245, Val RMSE: 0.6294, Time: 3.31s\n",
      "Trial 56, Epoch 64/245, Val RMSE: 0.6255, Time: 3.30s\n",
      "Trial 56, Epoch 65/245, Val RMSE: 0.6359, Time: 3.38s\n",
      "Trial 56, Epoch 66/245, Val RMSE: 0.6328, Time: 3.30s\n",
      "Trial 56, Epoch 67/245, Val RMSE: 0.6314, Time: 3.39s\n",
      "Trial 56, Epoch 68/245, Val RMSE: 0.6300, Time: 3.36s\n",
      "Trial 56, Epoch 69/245, Val RMSE: 0.6432, Time: 3.34s\n",
      "Trial 56, Epoch 70/245, Val RMSE: 0.6288, Time: 3.40s\n",
      "Trial 56, Epoch 71/245, Val RMSE: 0.6304, Time: 3.29s\n",
      "Trial 56, Epoch 72/245, Val RMSE: 0.6272, Time: 3.42s\n",
      "Trial 56, Epoch 73/245, Val RMSE: 0.6350, Time: 3.41s\n",
      "Trial 56, Epoch 74/245, Val RMSE: 0.6292, Time: 3.39s\n",
      "Trial 56, Epoch 75/245, Val RMSE: 0.6312, Time: 3.37s\n",
      "Trial 56, Epoch 76/245, Val RMSE: 0.6324, Time: 3.32s\n",
      "Trial 56, Epoch 77/245, Val RMSE: 0.6229, Time: 3.41s\n",
      "Trial 56, Epoch 78/245, Val RMSE: 0.6313, Time: 3.33s\n",
      "Trial 56, Epoch 79/245, Val RMSE: 0.6326, Time: 3.35s\n",
      "Trial 56, Epoch 80/245, Val RMSE: 0.6305, Time: 3.34s\n",
      "Trial 56, Epoch 81/245, Val RMSE: 0.6330, Time: 3.29s\n",
      "Trial 56, Epoch 82/245, Val RMSE: 0.6318, Time: 3.28s\n",
      "Trial 56, Epoch 83/245, Val RMSE: 0.6263, Time: 3.36s\n",
      "Trial 56, Epoch 84/245, Val RMSE: 0.6334, Time: 3.35s\n",
      "Trial 56, Epoch 85/245, Val RMSE: 0.6369, Time: 3.34s\n",
      "Trial 56, Epoch 86/245, Val RMSE: 0.6350, Time: 3.27s\n",
      "Trial 56, Epoch 87/245, Val RMSE: 0.6287, Time: 3.32s\n",
      "Trial 56, Epoch 88/245, Val RMSE: 0.6300, Time: 3.29s\n",
      "Trial 56, Epoch 89/245, Val RMSE: 0.6368, Time: 3.29s\n",
      "Trial 56, Epoch 90/245, Val RMSE: 0.6271, Time: 3.33s\n",
      "Trial 56, Epoch 91/245, Val RMSE: 0.6281, Time: 3.29s\n",
      "Trial 56, Epoch 92/245, Val RMSE: 0.6351, Time: 3.27s\n",
      "Trial 56, Epoch 93/245, Val RMSE: 0.6267, Time: 3.34s\n",
      "Trial 56, Epoch 94/245, Val RMSE: 0.6280, Time: 3.31s\n",
      "Trial 56, Epoch 95/245, Val RMSE: 0.6283, Time: 3.27s\n",
      "Trial 56, Epoch 96/245, Val RMSE: 0.6284, Time: 3.29s\n",
      "Trial 56, Epoch 97/245, Val RMSE: 0.6262, Time: 3.26s\n",
      "Trial 56, Epoch 98/245, Val RMSE: 0.6369, Time: 3.24s\n",
      "Trial 56, Epoch 99/245, Val RMSE: 0.6307, Time: 3.42s\n",
      "Trial 56, Epoch 100/245, Val RMSE: 0.6251, Time: 3.35s\n",
      "Trial 56, Epoch 101/245, Val RMSE: 0.6260, Time: 3.35s\n",
      "Trial 56, Epoch 102/245, Val RMSE: 0.6297, Time: 3.37s\n",
      "Trial 56, Epoch 103/245, Val RMSE: 0.6278, Time: 3.33s\n",
      "Trial 56, Epoch 104/245, Val RMSE: 0.6287, Time: 3.34s\n",
      "Trial 56, Epoch 105/245, Val RMSE: 0.6256, Time: 3.27s\n",
      "Trial 56, Epoch 106/245, Val RMSE: 0.6352, Time: 3.33s\n",
      "Trial 56, Epoch 107/245, Val RMSE: 0.6342, Time: 3.37s\n",
      "Trial 56, Epoch 108/245, Val RMSE: 0.6302, Time: 3.40s\n",
      "Trial 56, Epoch 109/245, Val RMSE: 0.6283, Time: 3.31s\n",
      "Trial 56, Epoch 110/245, Val RMSE: 0.6347, Time: 3.33s\n",
      "Trial 56, Epoch 111/245, Val RMSE: 0.6190, Time: 3.32s\n",
      "Trial 56, Epoch 112/245, Val RMSE: 0.6302, Time: 3.35s\n",
      "Trial 56, Epoch 113/245, Val RMSE: 0.6285, Time: 3.28s\n",
      "Trial 56, Epoch 114/245, Val RMSE: 0.6285, Time: 3.27s\n",
      "Trial 56, Epoch 115/245, Val RMSE: 0.6298, Time: 3.41s\n",
      "Trial 56, Epoch 116/245, Val RMSE: 0.6249, Time: 3.37s\n",
      "Trial 56, Epoch 117/245, Val RMSE: 0.6279, Time: 3.33s\n",
      "Trial 56, Epoch 118/245, Val RMSE: 0.6237, Time: 3.46s\n",
      "Trial 56, Epoch 119/245, Val RMSE: 0.6286, Time: 3.31s\n",
      "Trial 56, Epoch 120/245, Val RMSE: 0.6277, Time: 3.39s\n",
      "Trial 56, Epoch 121/245, Val RMSE: 0.6332, Time: 3.33s\n",
      "Trial 56, Epoch 122/245, Val RMSE: 0.6236, Time: 3.42s\n",
      "Trial 56, Epoch 123/245, Val RMSE: 0.6342, Time: 3.62s\n",
      "Trial 56, Epoch 124/245, Val RMSE: 0.6242, Time: 3.39s\n",
      "Trial 56, Epoch 125/245, Val RMSE: 0.6303, Time: 3.32s\n",
      "Trial 56, Epoch 126/245, Val RMSE: 0.6323, Time: 3.32s\n",
      "Trial 56, Epoch 127/245, Val RMSE: 0.6346, Time: 3.34s\n",
      "Trial 56, Epoch 128/245, Val RMSE: 0.6260, Time: 3.36s\n",
      "Trial 56, Epoch 129/245, Val RMSE: 0.6276, Time: 3.28s\n",
      "Trial 56, Epoch 130/245, Val RMSE: 0.6296, Time: 3.32s\n",
      "Trial 56, Epoch 131/245, Val RMSE: 0.6264, Time: 3.39s\n",
      "Trial 56, Epoch 132/245, Val RMSE: 0.6303, Time: 3.30s\n",
      "Trial 56, Epoch 133/245, Val RMSE: 0.6291, Time: 3.30s\n",
      "Trial 56, Epoch 134/245, Val RMSE: 0.6291, Time: 3.30s\n",
      "Trial 56, Epoch 135/245, Val RMSE: 0.6254, Time: 3.28s\n",
      "Trial 56, Epoch 136/245, Val RMSE: 0.6324, Time: 3.34s\n",
      "Trial 56, Epoch 137/245, Val RMSE: 0.6257, Time: 3.29s\n",
      "Trial 56, Epoch 138/245, Val RMSE: 0.6270, Time: 3.31s\n",
      "Trial 56, Epoch 139/245, Val RMSE: 0.6265, Time: 3.47s\n",
      "Trial 56, Epoch 140/245, Val RMSE: 0.6246, Time: 3.28s\n",
      "Trial 56, Epoch 141/245, Val RMSE: 0.6256, Time: 3.32s\n",
      "Trial 56, Epoch 142/245, Val RMSE: 0.6291, Time: 3.35s\n",
      "Trial 56, Epoch 143/245, Val RMSE: 0.6235, Time: 3.30s\n",
      "Trial 56, Epoch 144/245, Val RMSE: 0.6382, Time: 3.34s\n",
      "Trial 56, Epoch 145/245, Val RMSE: 0.6263, Time: 3.34s\n",
      "Trial 56, Epoch 146/245, Val RMSE: 0.6276, Time: 3.35s\n",
      "Trial 56, Epoch 147/245, Val RMSE: 0.6262, Time: 3.39s\n",
      "Trial 56, Epoch 148/245, Val RMSE: 0.6324, Time: 3.32s\n",
      "Trial 56, Epoch 149/245, Val RMSE: 0.6276, Time: 3.45s\n",
      "Trial 56, Epoch 150/245, Val RMSE: 0.6276, Time: 3.38s\n",
      "Trial 56, Epoch 151/245, Val RMSE: 0.6314, Time: 3.42s\n",
      "Trial 56, Epoch 152/245, Val RMSE: 0.6306, Time: 3.36s\n",
      "Trial 56, Epoch 153/245, Val RMSE: 0.6319, Time: 3.35s\n",
      "Trial 56, Epoch 154/245, Val RMSE: 0.6288, Time: 3.39s\n",
      "Trial 56, Epoch 155/245, Val RMSE: 0.6277, Time: 3.36s\n",
      "Trial 56, Epoch 156/245, Val RMSE: 0.6267, Time: 3.39s\n",
      "Trial 56, Epoch 157/245, Val RMSE: 0.6267, Time: 3.34s\n",
      "Trial 56, Epoch 158/245, Val RMSE: 0.6257, Time: 3.41s\n",
      "Trial 56, Epoch 159/245, Val RMSE: 0.6318, Time: 3.34s\n",
      "Trial 56, Epoch 160/245, Val RMSE: 0.6336, Time: 3.34s\n",
      "Trial 56, Epoch 161/245, Val RMSE: 0.6275, Time: 3.37s\n",
      "Early stopping at epoch 161 for trial 56\n",
      "[I 2025-07-16 22:57:35,014] Trial 56 finished with value: 0.6275020888006481 and parameters: {'hidden_channels': 846, 'lr': 0.00015519357728471518, 'batch_size': 128, 'n_epochs': 245, 'num_layers': 1, 'dropout_rate': 0.2599101323786843, 'weight_decay': 6.434840041056903e-08}. Best is trial 56 with value: 0.6275020888006481.\n",
      "Trial 57, Epoch 1/255, Val RMSE: 0.7965, Time: 3.37s\n",
      "Trial 57, Epoch 2/255, Val RMSE: 0.7424, Time: 3.32s\n",
      "Trial 57, Epoch 3/255, Val RMSE: 0.7230, Time: 3.34s\n",
      "Trial 57, Epoch 4/255, Val RMSE: 0.7036, Time: 3.34s\n",
      "Trial 57, Epoch 5/255, Val RMSE: 0.6990, Time: 3.39s\n",
      "Trial 57, Epoch 6/255, Val RMSE: 0.6818, Time: 3.39s\n",
      "Trial 57, Epoch 7/255, Val RMSE: 0.6890, Time: 3.31s\n",
      "Trial 57, Epoch 8/255, Val RMSE: 0.6921, Time: 3.37s\n",
      "[I 2025-07-16 22:58:02,234] Trial 57 pruned. \n",
      "Trial 58, Epoch 1/189, Val RMSE: 0.7955, Time: 3.38s\n",
      "Trial 58, Epoch 2/189, Val RMSE: 0.7515, Time: 3.42s\n",
      "Trial 58, Epoch 3/189, Val RMSE: 0.7232, Time: 3.35s\n",
      "Trial 58, Epoch 4/189, Val RMSE: 0.6987, Time: 3.36s\n",
      "Trial 58, Epoch 5/189, Val RMSE: 0.6823, Time: 3.39s\n",
      "Trial 58, Epoch 6/189, Val RMSE: 0.6727, Time: 3.43s\n",
      "Trial 58, Epoch 7/189, Val RMSE: 0.6634, Time: 3.42s\n",
      "Trial 58, Epoch 8/189, Val RMSE: 0.6662, Time: 3.41s\n",
      "Trial 58, Epoch 9/189, Val RMSE: 0.6567, Time: 3.33s\n",
      "Trial 58, Epoch 10/189, Val RMSE: 0.6643, Time: 3.35s\n",
      "Trial 58, Epoch 11/189, Val RMSE: 0.6466, Time: 3.35s\n",
      "Trial 58, Epoch 12/189, Val RMSE: 0.6475, Time: 3.40s\n",
      "Trial 58, Epoch 13/189, Val RMSE: 0.6622, Time: 3.42s\n",
      "Trial 58, Epoch 14/189, Val RMSE: 0.6460, Time: 3.24s\n",
      "Trial 58, Epoch 15/189, Val RMSE: 0.6439, Time: 3.36s\n",
      "Trial 58, Epoch 16/189, Val RMSE: 0.6440, Time: 3.28s\n",
      "Trial 58, Epoch 17/189, Val RMSE: 0.6448, Time: 3.33s\n",
      "Trial 58, Epoch 18/189, Val RMSE: 0.6390, Time: 3.34s\n",
      "Trial 58, Epoch 19/189, Val RMSE: 0.6507, Time: 3.31s\n",
      "Trial 58, Epoch 20/189, Val RMSE: 0.6499, Time: 3.33s\n",
      "Trial 58, Epoch 21/189, Val RMSE: 0.6434, Time: 3.33s\n",
      "Trial 58, Epoch 22/189, Val RMSE: 0.6387, Time: 3.35s\n",
      "Trial 58, Epoch 23/189, Val RMSE: 0.6403, Time: 3.29s\n",
      "Trial 58, Epoch 24/189, Val RMSE: 0.6452, Time: 3.34s\n",
      "Trial 58, Epoch 25/189, Val RMSE: 0.6400, Time: 3.32s\n",
      "Trial 58, Epoch 26/189, Val RMSE: 0.6432, Time: 3.32s\n",
      "Trial 58, Epoch 27/189, Val RMSE: 0.6356, Time: 3.28s\n",
      "Trial 58, Epoch 28/189, Val RMSE: 0.6393, Time: 3.34s\n",
      "Trial 58, Epoch 29/189, Val RMSE: 0.6412, Time: 3.32s\n",
      "Trial 58, Epoch 30/189, Val RMSE: 0.6375, Time: 3.32s\n",
      "Trial 58, Epoch 31/189, Val RMSE: 0.6442, Time: 3.29s\n",
      "Trial 58, Epoch 32/189, Val RMSE: 0.6366, Time: 3.28s\n",
      "Trial 58, Epoch 33/189, Val RMSE: 0.6305, Time: 3.31s\n",
      "Trial 58, Epoch 34/189, Val RMSE: 0.6290, Time: 3.25s\n",
      "Trial 58, Epoch 35/189, Val RMSE: 0.6309, Time: 3.28s\n",
      "Trial 58, Epoch 36/189, Val RMSE: 0.6287, Time: 3.27s\n",
      "Trial 58, Epoch 37/189, Val RMSE: 0.6415, Time: 3.31s\n",
      "Trial 58, Epoch 38/189, Val RMSE: 0.6338, Time: 3.35s\n",
      "Trial 58, Epoch 39/189, Val RMSE: 0.6448, Time: 3.23s\n",
      "Trial 58, Epoch 40/189, Val RMSE: 0.6310, Time: 3.25s\n",
      "Trial 58, Epoch 41/189, Val RMSE: 0.6347, Time: 3.35s\n",
      "Trial 58, Epoch 42/189, Val RMSE: 0.6489, Time: 3.36s\n",
      "Trial 58, Epoch 43/189, Val RMSE: 0.6288, Time: 3.35s\n",
      "Trial 58, Epoch 44/189, Val RMSE: 0.6384, Time: 3.39s\n",
      "Trial 58, Epoch 45/189, Val RMSE: 0.6378, Time: 3.31s\n",
      "Trial 58, Epoch 46/189, Val RMSE: 0.6270, Time: 3.32s\n",
      "Trial 58, Epoch 47/189, Val RMSE: 0.6369, Time: 3.30s\n",
      "Trial 58, Epoch 48/189, Val RMSE: 0.6341, Time: 3.29s\n",
      "Trial 58, Epoch 49/189, Val RMSE: 0.6358, Time: 3.32s\n",
      "Trial 58, Epoch 50/189, Val RMSE: 0.6324, Time: 3.35s\n",
      "Trial 58, Epoch 51/189, Val RMSE: 0.6363, Time: 3.33s\n",
      "Trial 58, Epoch 52/189, Val RMSE: 0.6387, Time: 3.29s\n",
      "Trial 58, Epoch 53/189, Val RMSE: 0.6330, Time: 3.36s\n",
      "Trial 58, Epoch 54/189, Val RMSE: 0.6522, Time: 3.37s\n",
      "Trial 58, Epoch 55/189, Val RMSE: 0.6299, Time: 3.31s\n",
      "Trial 58, Epoch 56/189, Val RMSE: 0.6283, Time: 3.40s\n",
      "Trial 58, Epoch 57/189, Val RMSE: 0.6535, Time: 3.36s\n",
      "Trial 58, Epoch 58/189, Val RMSE: 0.6291, Time: 3.37s\n",
      "Trial 58, Epoch 59/189, Val RMSE: 0.6339, Time: 3.54s\n",
      "Trial 58, Epoch 60/189, Val RMSE: 0.6275, Time: 3.32s\n",
      "Trial 58, Epoch 61/189, Val RMSE: 0.6317, Time: 3.29s\n",
      "Trial 58, Epoch 62/189, Val RMSE: 0.6264, Time: 3.38s\n",
      "Trial 58, Epoch 63/189, Val RMSE: 0.6356, Time: 3.33s\n",
      "Trial 58, Epoch 64/189, Val RMSE: 0.6240, Time: 3.33s\n",
      "Trial 58, Epoch 65/189, Val RMSE: 0.6285, Time: 3.34s\n",
      "Trial 58, Epoch 66/189, Val RMSE: 0.6247, Time: 3.41s\n",
      "Trial 58, Epoch 67/189, Val RMSE: 0.6331, Time: 3.35s\n",
      "Trial 58, Epoch 68/189, Val RMSE: 0.6263, Time: 3.39s\n",
      "Trial 58, Epoch 69/189, Val RMSE: 0.6383, Time: 3.32s\n",
      "Trial 58, Epoch 70/189, Val RMSE: 0.6351, Time: 3.36s\n",
      "Trial 58, Epoch 71/189, Val RMSE: 0.6275, Time: 3.31s\n",
      "Trial 58, Epoch 72/189, Val RMSE: 0.6306, Time: 3.26s\n",
      "Trial 58, Epoch 73/189, Val RMSE: 0.6275, Time: 3.28s\n",
      "Trial 58, Epoch 74/189, Val RMSE: 0.6309, Time: 3.33s\n",
      "Trial 58, Epoch 75/189, Val RMSE: 0.6289, Time: 3.35s\n",
      "Trial 58, Epoch 76/189, Val RMSE: 0.6279, Time: 3.36s\n",
      "Trial 58, Epoch 77/189, Val RMSE: 0.6317, Time: 3.32s\n",
      "Trial 58, Epoch 78/189, Val RMSE: 0.6284, Time: 3.35s\n",
      "Trial 58, Epoch 79/189, Val RMSE: 0.6306, Time: 3.28s\n",
      "Trial 58, Epoch 80/189, Val RMSE: 0.6288, Time: 3.33s\n",
      "Trial 58, Epoch 81/189, Val RMSE: 0.6333, Time: 3.31s\n",
      "Trial 58, Epoch 82/189, Val RMSE: 0.6329, Time: 3.32s\n",
      "Trial 58, Epoch 83/189, Val RMSE: 0.6298, Time: 3.30s\n",
      "Trial 58, Epoch 84/189, Val RMSE: 0.6317, Time: 3.39s\n",
      "Trial 58, Epoch 85/189, Val RMSE: 0.6312, Time: 3.31s\n",
      "Trial 58, Epoch 86/189, Val RMSE: 0.6233, Time: 3.29s\n",
      "Trial 58, Epoch 87/189, Val RMSE: 0.6283, Time: 3.39s\n",
      "Trial 58, Epoch 88/189, Val RMSE: 0.6259, Time: 3.43s\n",
      "Trial 58, Epoch 89/189, Val RMSE: 0.6327, Time: 3.40s\n",
      "Trial 58, Epoch 90/189, Val RMSE: 0.6287, Time: 3.44s\n",
      "Trial 58, Epoch 91/189, Val RMSE: 0.6238, Time: 3.36s\n",
      "Trial 58, Epoch 92/189, Val RMSE: 0.6249, Time: 3.36s\n",
      "Trial 58, Epoch 93/189, Val RMSE: 0.6256, Time: 3.43s\n",
      "Trial 58, Epoch 94/189, Val RMSE: 0.6299, Time: 3.42s\n",
      "Trial 58, Epoch 95/189, Val RMSE: 0.6325, Time: 3.37s\n",
      "Trial 58, Epoch 96/189, Val RMSE: 0.6263, Time: 3.39s\n",
      "Trial 58, Epoch 97/189, Val RMSE: 0.6312, Time: 3.44s\n",
      "Trial 58, Epoch 98/189, Val RMSE: 0.6238, Time: 3.37s\n",
      "Trial 58, Epoch 99/189, Val RMSE: 0.6314, Time: 3.30s\n",
      "Trial 58, Epoch 100/189, Val RMSE: 0.6275, Time: 3.34s\n",
      "Trial 58, Epoch 101/189, Val RMSE: 0.6250, Time: 3.38s\n",
      "Trial 58, Epoch 102/189, Val RMSE: 0.6307, Time: 3.39s\n",
      "Trial 58, Epoch 103/189, Val RMSE: 0.6264, Time: 3.42s\n",
      "Trial 58, Epoch 104/189, Val RMSE: 0.6331, Time: 3.39s\n",
      "Trial 58, Epoch 105/189, Val RMSE: 0.6278, Time: 3.34s\n",
      "Trial 58, Epoch 106/189, Val RMSE: 0.6324, Time: 3.35s\n",
      "Trial 58, Epoch 107/189, Val RMSE: 0.6325, Time: 3.29s\n",
      "Trial 58, Epoch 108/189, Val RMSE: 0.6306, Time: 3.32s\n",
      "Trial 58, Epoch 109/189, Val RMSE: 0.6241, Time: 3.35s\n",
      "Trial 58, Epoch 110/189, Val RMSE: 0.6283, Time: 3.30s\n",
      "Trial 58, Epoch 111/189, Val RMSE: 0.6324, Time: 3.36s\n",
      "Trial 58, Epoch 112/189, Val RMSE: 0.6298, Time: 3.35s\n",
      "Trial 58, Epoch 113/189, Val RMSE: 0.6262, Time: 3.26s\n",
      "Trial 58, Epoch 114/189, Val RMSE: 0.6324, Time: 3.30s\n",
      "Trial 58, Epoch 115/189, Val RMSE: 0.6266, Time: 3.30s\n",
      "Trial 58, Epoch 116/189, Val RMSE: 0.6294, Time: 3.37s\n",
      "Trial 58, Epoch 117/189, Val RMSE: 0.6267, Time: 3.35s\n",
      "Trial 58, Epoch 118/189, Val RMSE: 0.6359, Time: 3.29s\n",
      "Trial 58, Epoch 119/189, Val RMSE: 0.6344, Time: 3.27s\n",
      "Trial 58, Epoch 120/189, Val RMSE: 0.6267, Time: 3.33s\n",
      "Trial 58, Epoch 121/189, Val RMSE: 0.6266, Time: 3.35s\n",
      "Trial 58, Epoch 122/189, Val RMSE: 0.6252, Time: 3.33s\n",
      "Trial 58, Epoch 123/189, Val RMSE: 0.6263, Time: 3.33s\n",
      "Trial 58, Epoch 124/189, Val RMSE: 0.6261, Time: 3.38s\n",
      "Trial 58, Epoch 125/189, Val RMSE: 0.6299, Time: 3.47s\n",
      "Trial 58, Epoch 126/189, Val RMSE: 0.6288, Time: 3.37s\n",
      "Trial 58, Epoch 127/189, Val RMSE: 0.6257, Time: 3.36s\n",
      "Trial 58, Epoch 128/189, Val RMSE: 0.6315, Time: 3.30s\n",
      "Trial 58, Epoch 129/189, Val RMSE: 0.6306, Time: 3.38s\n",
      "Trial 58, Epoch 130/189, Val RMSE: 0.6235, Time: 3.34s\n",
      "Trial 58, Epoch 131/189, Val RMSE: 0.6265, Time: 3.37s\n",
      "Trial 58, Epoch 132/189, Val RMSE: 0.6272, Time: 3.40s\n",
      "Trial 58, Epoch 133/189, Val RMSE: 0.6282, Time: 3.36s\n",
      "Trial 58, Epoch 134/189, Val RMSE: 0.6250, Time: 3.39s\n",
      "Trial 58, Epoch 135/189, Val RMSE: 0.6285, Time: 3.39s\n",
      "Trial 58, Epoch 136/189, Val RMSE: 0.6247, Time: 3.32s\n",
      "Early stopping at epoch 136 for trial 58\n",
      "[I 2025-07-16 23:05:40,753] Trial 58 finished with value: 0.624701555411358 and parameters: {'hidden_channels': 981, 'lr': 0.00012757691978285753, 'batch_size': 128, 'n_epochs': 189, 'num_layers': 1, 'dropout_rate': 0.2705233408648206, 'weight_decay': 4.481539044710921e-08}. Best is trial 58 with value: 0.624701555411358.\n",
      "\n",
      "Optuna optimization finished for GNN.\n",
      "\n",
      "--- Best Trial Results for GNN ---\n",
      "Best trial number: 58\n",
      "Best RMSE (Validation): 0.6247\n",
      "Best hyperparameters:\n",
      "  hidden_channels: 981\n",
      "  lr: 0.00012757691978285753\n",
      "  batch_size: 128\n",
      "  n_epochs: 189\n",
      "  num_layers: 1\n",
      "  dropout_rate: 0.2705233408648206\n",
      "  weight_decay: 4.481539044710921e-08\n",
      "Best R2 Score (Validation): 0.6018\n"
     ]
    }
   ],
   "source": [
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "print(f\"Optuna study for GNN will be stored at: {study_db_path}\")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=10,  # Run at least these many trials completely before starting to prune\n",
    "    n_warmup_steps=20,    # Don't prune trials until they've completed these many epochs\n",
    "    interval_steps=10     # Check for pruning every these many epochs\n",
    ")\n",
    "# pruner = None\n",
    "\n",
    "# Check if a study with the same name already exists in the database\n",
    "# If it does, load it to resume the optimization.\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(f\"Loaded existing study '{study_name}' from {study_db_path}. Resuming optimization.\")\n",
    "except KeyError:\n",
    "    # If the study does not exist, create a new one\n",
    "    print(f\"Creating new study '{study_name}' at {study_db_path}.\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        storage=study_db_path,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "print(\"\\nStarting Optuna optimization for GNN...\")\n",
    "# Run for 'n_trial' trials or 'timeout' seconds, whichever completes first\n",
    "study.optimize(objective, n_trials=None, timeout=14400, show_progress_bar=True)\n",
    "print(\"\\nOptuna optimization finished for GNN.\")\n",
    "\n",
    "# Print best trial results\n",
    "print(\"\\n--- Best Trial Results for GNN ---\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best RMSE (Validation): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if \"final_r2_score\" in study.best_trial.user_attrs:\n",
    "    print(f\"Best R2 Score (Validation): {study.best_trial.user_attrs['final_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773cb25-a8fb-4d90-96d7-8745c16b01be",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19444a-e710-425d-ac80-0f42e0fd3ef6",
   "metadata": {},
   "source": [
    "### Reinitialize Everything with Best Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31078f14-daa7-4943-8423-dba53f579e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the study to ensure the latest best parameters\n",
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(\"Best trial parameters (GNN):\", study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "except KeyError:\n",
    "    print(f\"Study '{study_name}' does not exist at {study_db_path}. Please make sure the GNN Optuna study cell has been run.\")\n",
    "\n",
    "best_hidden_channels = best_params[\"hidden_channels\"]\n",
    "best_learning_rate = best_params[\"lr\"]\n",
    "best_batch_size = best_params[\"batch_size\"]\n",
    "best_n_epochs = best_params[\"n_epochs\"]\n",
    "best_num_layers = best_params[\"num_layers\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "best_weight_decay = best_params[\"weight_decay\"]\n",
    "\n",
    "print(f\"Best hyperparameters from Optuna: {best_params}\")\n",
    "\n",
    "# Re-initialize the model with best hyperparameters\n",
    "if not train_data_list:\n",
    "    raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions for GNN.\")\n",
    "\n",
    "node_feature_dim = train_data_list[0].x.shape[1]\n",
    "global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "final_gnn_model = GNN(\n",
    "    node_feature_dim=node_feature_dim,\n",
    "    global_feature_dim=global_feature_dim,\n",
    "    hidden_channels=best_hidden_channels,\n",
    "    num_layers=best_num_layers,\n",
    "    dropout_rate=best_dropout_rate # Ensure your GNN class takes dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Re-initialize criterion and optimizer\n",
    "final_criterion = nn.MSELoss()\n",
    "final_optimizer = optim.Adam(final_mlp_model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Re-create DataLoaders with the best batch size (Training + Validation data COMBINED)\n",
    "X_train_val_tensor = torch.cat((X_train_tensor, X_val_tensor), dim=0)\n",
    "y_train_val_tensor = torch.cat((y_train_tensor, y_val_tensor), dim=0)\n",
    "\n",
    "final_train_val_dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
    "final_train_val_loader = DataLoader(final_train_val_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "# Create the FINAL TEST DataLoader\n",
    "final_test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "final_test_loader = DataLoader(final_test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Final model, criterion, optimizer, and DataLoaders initialized with best parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e421910-603a-469c-b7fe-133619e5eedc",
   "metadata": {},
   "source": [
    "### Get Current Commit ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c9398-d629-407f-a118-575c9d400651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa70146-1e6a-44ce-84a8-dca9bcb04dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e9ed59c-cb77-4821-8773-4936603ac335",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc661f62-1bc3-47d7-9d83-41d64ff14e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4f489f4-d72a-4453-a196-7d2635b2d8d6",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f551353f-28f6-4118-b459-651f371428d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in train_data_list: 13119\n",
      "\n",
      "--- Inspecting Graph 1 ---\n",
      "Node features (data.x) shape: torch.Size([25, 21])\n",
      "Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "Node features (data.x) min: -0.4928\n",
      "Node features (data.x) max: 8.0000\n",
      "Node features (data.x) mean: 1.2363\n",
      "Node features (data.x) std: 1.7344\n",
      "Contains NaN in data.x: False\n",
      "Contains Inf in data.x: False\n",
      "Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "Global features (data.global_features) sample: [6.0, 6.0, 6.033141613006592, 6.033141613006592, 6.033141613006592]\n",
      "Global features (data.global_features) min: -3.1400\n",
      "Global features (data.global_features) max: 1072410.2500\n",
      "Global features (data.global_features) mean: 474.7916\n",
      "Global features (data.global_features) std: 22525.9316\n",
      "Contains NaN in global_features: False\n",
      "Contains Inf in global_features: False\n",
      "Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "Edge index (data.edge_index) first 5 columns:\n",
      "[[0, 1, 1, 2, 2], [1, 0, 2, 1, 3]]\n",
      "Target (data.y) value: 5.7347\n",
      "\n",
      "--- Inspecting Graph 2 ---\n",
      "Node features (data.x) shape: torch.Size([26, 21])\n",
      "Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "Node features (data.x) min: -0.4927\n",
      "Node features (data.x) max: 8.0000\n",
      "Node features (data.x) mean: 1.1826\n",
      "Node features (data.x) std: 1.7676\n",
      "Contains NaN in data.x: False\n",
      "Contains Inf in data.x: False\n",
      "Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "Global features (data.global_features) sample: [9.0, 9.0, 9.645791053771973, 9.645791053771973, 9.645791053771973]\n",
      "Global features (data.global_features) min: -3.3300\n",
      "Global features (data.global_features) max: 962281.2500\n",
      "Global features (data.global_features) mean: 426.1296\n",
      "Global features (data.global_features) std: 20212.6719\n",
      "Contains NaN in global_features: False\n",
      "Contains Inf in global_features: False\n",
      "Edge index (data.edge_index) shape: torch.Size([2, 56])\n",
      "Edge index (data.edge_index) first 5 columns:\n",
      "[[0, 1, 1, 2, 2], [1, 0, 2, 1, 3]]\n",
      "Target (data.y) value: 7.1647\n",
      "\n",
      "--- Inspecting Graph 3 ---\n",
      "Node features (data.x) shape: torch.Size([33, 21])\n",
      "Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "Node features (data.x) min: -0.4895\n",
      "Node features (data.x) max: 8.0000\n",
      "Node features (data.x) mean: 1.2302\n",
      "Node features (data.x) std: 1.7587\n",
      "Contains NaN in data.x: False\n",
      "Contains Inf in data.x: False\n",
      "Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "Global features (data.global_features) sample: [6.0, 6.0, 11.953178405761719, 11.953178405761719, 11.953178405761719]\n",
      "Global features (data.global_features) min: -3.4500\n",
      "Global features (data.global_features) max: 45006212.0000\n",
      "Global features (data.global_features) mean: 19863.3887\n",
      "Global features (data.global_features) std: 945354.1875\n",
      "Contains NaN in global_features: False\n",
      "Contains Inf in global_features: False\n",
      "Edge index (data.edge_index) shape: torch.Size([2, 72])\n",
      "Edge index (data.edge_index) first 5 columns:\n",
      "[[0, 1, 1, 2, 2], [1, 0, 2, 1, 3]]\n",
      "Target (data.y) value: 4.9284\n",
      "\n",
      "--- Inspecting Graph 4 ---\n",
      "Node features (data.x) shape: torch.Size([23, 21])\n",
      "Node features (data.x) sample (first 5 values): [8.0, 1.0, 1.0, 0.0, 0.0]\n",
      "Node features (data.x) min: -0.3112\n",
      "Node features (data.x) max: 35.0000\n",
      "Node features (data.x) mean: 1.2106\n",
      "Node features (data.x) std: 2.2950\n",
      "Contains NaN in data.x: False\n",
      "Contains Inf in data.x: False\n",
      "Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "Global features (data.global_features) sample: [4.0, 4.0, 12.253458023071289, 12.253458023071289, 12.253458023071289]\n",
      "Global features (data.global_features) min: -2.4400\n",
      "Global features (data.global_features) max: 155409.9531\n",
      "Global features (data.global_features) mean: 70.0496\n",
      "Global features (data.global_features) std: 3264.4250\n",
      "Contains NaN in global_features: False\n",
      "Contains Inf in global_features: False\n",
      "Edge index (data.edge_index) shape: torch.Size([2, 48])\n",
      "Edge index (data.edge_index) first 5 columns:\n",
      "[[0, 1, 1, 2, 2], [1, 0, 2, 1, 3]]\n",
      "Target (data.y) value: 6.8827\n",
      "\n",
      "--- Inspecting Graph 5 ---\n",
      "Node features (data.x) shape: torch.Size([33, 21])\n",
      "Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "Node features (data.x) min: -0.4071\n",
      "Node features (data.x) max: 17.0000\n",
      "Node features (data.x) mean: 1.1990\n",
      "Node features (data.x) std: 1.8750\n",
      "Contains NaN in data.x: False\n",
      "Contains Inf in data.x: False\n",
      "Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "Global features (data.global_features) sample: [2.0, 2.0, 14.12848949432373, 14.12848949432373, 14.12848949432373]\n",
      "Global features (data.global_features) min: -4.3275\n",
      "Global features (data.global_features) max: 28593970.0000\n",
      "Global features (data.global_features) mean: 12620.7656\n",
      "Global features (data.global_features) std: 600615.5000\n",
      "Contains NaN in global_features: False\n",
      "Contains Inf in global_features: False\n",
      "Edge index (data.edge_index) shape: torch.Size([2, 74])\n",
      "Edge index (data.edge_index) first 5 columns:\n",
      "[[0, 1, 1, 2, 2], [1, 0, 2, 1, 3]]\n",
      "Target (data.y) value: 6.0942\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_data_list is loaded and accessible from your dataset setup\n",
    "# If you are loading data in a specific way, ensure 'train_data_list' is populated.\n",
    "\n",
    "if 'train_data_list' in locals() and train_data_list:\n",
    "    print(f\"Number of graphs in train_data_list: {len(train_data_list)}\")\n",
    "    \n",
    "    # Check the first 5 graphs in the list\n",
    "    for i, data in enumerate(train_data_list[:5]): \n",
    "        print(f\"\\n--- Inspecting Graph {i+1} ---\")\n",
    "        \n",
    "        # Check node features (data.x)\n",
    "        if hasattr(data, 'x') and data.x is not None:\n",
    "            print(f\"Node features (data.x) shape: {data.x.shape}\")\n",
    "            if data.x.numel() > 0: # Check if tensor is not empty\n",
    "                print(f\"Node features (data.x) sample (first 5 values): {data.x.flatten()[:5].tolist()}\")\n",
    "                print(f\"Node features (data.x) min: {data.x.min().item():.4f}\")\n",
    "                print(f\"Node features (data.x) max: {data.x.max().item():.4f}\")\n",
    "                print(f\"Node features (data.x) mean: {data.x.float().mean().item():.4f}\")\n",
    "                print(f\"Node features (data.x) std: {data.x.float().std().item():.4f}\")\n",
    "                print(f\"Contains NaN in data.x: {torch.isnan(data.x).any().item()}\")\n",
    "                print(f\"Contains Inf in data.x: {torch.isinf(data.x).any().item()}\")\n",
    "            else:\n",
    "                print(\"Node features (data.x) is an empty tensor.\")\n",
    "        else:\n",
    "            print(\"Graph does not have 'x' (node features) attribute.\")\n",
    "\n",
    "        # Check global features (data.global_features)\n",
    "        if hasattr(data, 'global_features') and data.global_features is not None:\n",
    "            print(f\"Global features (data.global_features) shape: {data.global_features.shape}\")\n",
    "            if data.global_features.numel() > 0: # Check if tensor is not empty\n",
    "                print(f\"Global features (data.global_features) sample: {data.global_features.flatten()[:5].tolist()}\")\n",
    "                print(f\"Global features (data.global_features) min: {data.global_features.min().item():.4f}\")\n",
    "                print(f\"Global features (data.global_features) max: {data.global_features.max().item():.4f}\")\n",
    "                print(f\"Global features (data.global_features) mean: {data.global_features.float().mean().item():.4f}\")\n",
    "                print(f\"Global features (data.global_features) std: {data.global_features.float().std().item():.4f}\")\n",
    "                print(f\"Contains NaN in global_features: {torch.isnan(data.global_features).any().item()}\")\n",
    "                print(f\"Contains Inf in global_features: {torch.isinf(data.global_features).any().item()}\")\n",
    "            else:\n",
    "                print(\"Global features (data.global_features) is an empty tensor.\")\n",
    "        else:\n",
    "            print(\"Graph does not have 'global_features' attribute.\")\n",
    "\n",
    "        # Check edge_index\n",
    "        if hasattr(data, 'edge_index') and data.edge_index is not None:\n",
    "            print(f\"Edge index (data.edge_index) shape: {data.edge_index.shape}\")\n",
    "            print(f\"Edge index (data.edge_index) first 5 columns:\\n{data.edge_index[:, :5].tolist()}\")\n",
    "        else:\n",
    "            print(\"Graph does not have 'edge_index' attribute.\")\n",
    "\n",
    "        # Check y\n",
    "        if hasattr(data, 'y') and data.y is not None:\n",
    "            print(f\"Target (data.y) value: {data.y.item():.4f}\")\n",
    "        else:\n",
    "            print(\"Graph does not have 'y' (target) attribute.\")\n",
    "\n",
    "else:\n",
    "    print(\"train_data_list is not available or is empty. Please ensure your dataset loading is complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
